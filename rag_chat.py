from __future__ import annotations
import os, logging, time, hashlib, pickle
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List
from pathlib import Path
from dotenv import load_dotenv

# LLM & ì„ë² ë”©
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# ë‚´ë¶€ ê²€ìƒ‰ ê³µí†µ
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker

# Perplexity ì›¹ ê²€ìƒ‰
from openai import OpenAI

from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document

load_dotenv()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
GEMINI_API_KEY       = os.getenv("GEMINI_API_KEY")
PERPLEXITY_API_KEY   = os.getenv("PERPLEXITY_API_KEY")
if not GEMINI_API_KEY:
    raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ RAG ì´ˆê¸°í™”
print("ğŸš€ DF-RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
init_start_time = time.time()

CHROMA_DIR   = "vector_db/chroma"
EMBED_MODEL  = "text-embedding-3-large"
embed_fn     = OpenAIEmbeddings(model=EMBED_MODEL)

print("ğŸ”„ ë²¡í„° DB ë¡œë”©...")
vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embed_fn)
vector_retriever = vectordb.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 8, "fetch_k": 15, "lambda_mult": 0.8},
)
print("âœ… ë²¡í„° DB ë¡œë”© ì™„ë£Œ")

# BM25
def build_bm25_index():
    """BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìºì‹±"""
    print("ğŸ”„ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    store_data = vectordb.get(include=["documents", "metadatas"])
    docs_for_bm25 = []
    for txt, meta in zip(store_data["documents"], store_data["metadatas"]):
        # ë©”íƒ€ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ì— í¬í•¨ì‹œì¼œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
        enhanced_content = txt
        if meta:
            # ì œëª©ì´ ìˆìœ¼ë©´ ê°•ì¡°
            if meta.get("title"):
                enhanced_content = f"ì œëª©: {meta['title']}\n{txt}"
            # í´ë˜ìŠ¤ëª…ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if meta.get("class_name"):
                enhanced_content += f"\nì§ì—…: {meta['class_name']}"
        
        docs_for_bm25.append(Document(page_content=enhanced_content, metadata=meta))
    
    bm25_retriever = BM25Retriever.from_documents(docs_for_bm25)
    bm25_retriever.k = 8
    
    # ìºì‹± ì €ì¥
    cache_file = CACHE_DIR / "bm25_index.pkl"
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(bm25_retriever, f)
        print(f"âœ… BM25 ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥: {cache_file}")
    except Exception as e:
        print(f"âš ï¸ BM25 ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return bm25_retriever

def load_bm25_index():
    """ìºì‹œì—ì„œ BM25 ì¸ë±ìŠ¤ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ êµ¬ì¶•"""
    cache_file = CACHE_DIR / "bm25_index.pkl"
    
    # ìºì‹œ ë§Œë£Œ ì‹œê°„ (12ì‹œê°„)
    cache_expiry = 60 * 60 * 12
    
    if cache_file.exists():
        file_age = time.time() - cache_file.stat().st_mtime
        if file_age < cache_expiry:
            try:
                print("ğŸ”„ ìºì‹œëœ BM25 ì¸ë±ìŠ¤ ë¡œë”©...")
                with open(cache_file, 'rb') as f:
                    bm25_retriever = pickle.load(f)
                print("âœ… BM25 ì¸ë±ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ")
                return bm25_retriever
            except Exception as e:
                print(f"âš ï¸ BM25 ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ìºì‹œê°€ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ìƒˆë¡œ êµ¬ì¶•
    return build_bm25_index()

bm25_retriever = load_bm25_index()

# RRF â†’ Ensemble
rrf_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5],
)

# Custom metadata-aware retriever wrapper
class MetadataAwareRetriever:
    def __init__(self, base_retriever):
        self.base_retriever = base_retriever
    
    def get_relevant_documents(self, query):
        docs = self.base_retriever.get_relevant_documents(query)
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        scored_docs = []
        for doc in docs:
            score = 1.0  # ê¸°ë³¸ ì ìˆ˜
            meta = doc.metadata or {}
            
            # í’ˆì§ˆ ì ìˆ˜ (priority_score, content_score ê¸°ì¤€)
            if meta.get("priority_score"):
                try:
                    priority = float(meta["priority_score"])
                    score += priority * 0.1  # priority_scoreë¥¼ ì ìˆ˜ì— ë°˜ì˜
                except:
                    pass
            
            if meta.get("content_score"):
                try:
                    content_score = float(meta["content_score"])
                    score += content_score * 0.01  # content_scoreë¥¼ ì ìˆ˜ì— ë°˜ì˜
                except:
                    pass
            
            scored_docs.append((doc, score))
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored_docs[:8]]  # ìƒìœ„ 8ê°œë§Œ ë°˜í™˜

# Cross-Encoder ì¬ë­í‚¹ with caching
def load_cross_encoder():
    """í¬ë¡œìŠ¤ ì¸ì½”ë” ëª¨ë¸ ë¡œë“œ (ìºì‹œ í™œìš©)"""
    cache_file = CACHE_DIR / "cross_encoder.pkl"
    
    # ìºì‹œ ë§Œë£Œ ì‹œê°„ (24ì‹œê°„)
    cache_expiry = 60 * 60 * 24
    
    if cache_file.exists():
        file_age = time.time() - cache_file.stat().st_mtime
        if file_age < cache_expiry:
            try:
                print("ğŸ”„ ìºì‹œëœ Cross-Encoder ë¡œë”©...")
                with open(cache_file, 'rb') as f:
                    cross_encoder = pickle.load(f)
                print("âœ… Cross-Encoder ìºì‹œ ë¡œë“œ ì™„ë£Œ")
                return cross_encoder
            except Exception as e:
                print(f"âš ï¸ Cross-Encoder ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ìºì‹œê°€ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ìƒˆë¡œ ë¡œë“œ
    print("ğŸ”„ Cross-Encoder ëª¨ë¸ ë¡œë”©...")
    cross_encoder = HuggingFaceCrossEncoder(
        model_name="cross-encoder/ms-marco-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"}     
    )
    
    # ìºì‹± ì €ì¥
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(cross_encoder, f)
        print(f"âœ… Cross-Encoder ìºì‹œ ì €ì¥: {cache_file}")
    except Exception as e:
        print(f"âš ï¸ Cross-Encoder ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return cross_encoder

cross_encoder = load_cross_encoder()
compressor = CrossEncoderReranker(
    model=cross_encoder,
    top_n=6                             
)
base_retriever = ContextualCompressionRetriever(
    base_retriever=rrf_retriever,
    base_compressor=compressor,
)
internal_retriever = MetadataAwareRetriever(base_retriever)

init_elapsed_time = time.time() - init_start_time
print(f"ğŸ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {init_elapsed_time:.2f}ì´ˆ)")
print("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìºì‹œ ê´€ë ¨ í•¨ìˆ˜
def generate_cache_key(query):
    return hashlib.md5(query.encode('utf-8')).hexdigest()

def get_from_cache(query, cache_type='search'):
    cache_key = generate_cache_key(query)
    cache_file = CACHE_DIR / f"{cache_type}_{cache_key}.pkl"
    
    cache_expiry = 60 * 60 * 12  # 12ì‹œê°„
    
    if cache_file.exists():
        file_age = time.time() - cache_file.stat().st_mtime
        if file_age < cache_expiry:
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception:
                pass
    return None

def save_to_cache(query, result, cache_type='search'):
    cache_key = generate_cache_key(query)
    cache_file = CACHE_DIR / f"{cache_type}_{cache_key}.pkl"
    
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Perplexity ì›¹ ê²€ìƒ‰
def perplexity_web_search(query: str, max_results=3) -> List[Document]:
    cached_result = get_from_cache(query, 'web_search')
    if cached_result:
        print("ğŸ”„ ìºì‹œëœ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
        return cached_result
    
    if not PERPLEXITY_API_KEY:
        return []
    
    try:
        start_time = time.time()
        client = OpenAI(
            api_key=PERPLEXITY_API_KEY,
            base_url="https://api.perplexity.ai"
        )
        
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a helpful assistant that provides information about "
                    "Dungeon & Fighter (DNF) game. Focus exclusively on providing the most "
                    "recent and accurate information about character progression, equipment, "
                    "and optimization guides. Extract only the key facts and essential details "
                    "from your search results. Prioritize information from 2025 sources. "
                    "Be concise and direct, omitting any unnecessary context or introduction. "
                    "Format your response as clear, actionable points whenever possible."
                )
            },
            {"role": "user", "content": f"2025 ìµœì‹  ë˜ì „ì•¤íŒŒì´í„° {query} ëª…ì„±ë³„ ìŠ¤í™ì—… ê°€ì´ë“œ"}
        ]
        
        response = client.chat.completions.create(
            model="sonar",
            messages=messages,
            max_tokens=1000,
            temperature=0
        )
        
        docs = []
        
        content = response.choices[0].message.content
        docs.append(Document(
            page_content=content,
            metadata={"title": "Perplexity ê²€ìƒ‰ ê²°ê³¼", "url": "", "source": "web_search"}
        ))
        
        citations = []
        if hasattr(response, "citations"):
            citations = response.citations
        elif hasattr(response.choices[0].message, "context") and hasattr(response.choices[0].message.context, "citations"):
            citations = response.choices[0].message.context.citations
            
        if citations:
            for citation in citations[:max_results]:
                if isinstance(citation, dict):
                    url = citation.get("url", "")
                    text = citation.get("text", "")
                else:
                    url = str(citation)
                    text = ""
                    
                # URLì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if url and url != "ë§í¬ ì—†ìŒ":
                    docs.append(Document(
                        page_content=text,
                        metadata={"title": url, "url": url, "source": "web_search"}
                    ))
        
        save_to_cache(query, docs, 'web_search')
        
        elapsed_time = time.time() - start_time
        print(f"â±ï¸ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        return docs
            
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
def smart_hybrid_search(query):
    start_time = time.time()
    
    cached_result = get_from_cache(query, 'hybrid_search')
    if cached_result:
        print("[CACHE] ìºì‹œëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
        return cached_result
    
    def get_internal_results():
        try:
            docs = internal_retriever.get_relevant_documents(query)
            print(f"[DEBUG] ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
            return docs
        except Exception as e:
            print(f"[ERROR] ë‚´ë¶€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_web_results():
        try:
            results = perplexity_web_search(query)
            print(f"[DEBUG] ì›¹ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
            return results
        except Exception as e:
            print(f"[ERROR] ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    # ë³‘ë ¬ ì‹¤í–‰ (í•­ìƒ ì›¹ê²€ìƒ‰ í¬í•¨)
    with ThreadPoolExecutor(max_workers=2) as executor:
        internal_future = executor.submit(get_internal_results)
        web_future = executor.submit(get_web_results)
        
        internal_docs = internal_future.result()
        web_docs = web_future.result()
    
    # ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (URL ì •ë³´ í¬í•¨)
    internal_context_parts = []
    for i, doc in enumerate(internal_docs):
        content = f"[ë‚´ë¶€ ë¬¸ì„œ {i+1}] {doc.page_content}"
        # URL ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if doc.metadata and doc.metadata.get("url"):
            content += f"\nì°¸ê³  ë§í¬: {doc.metadata['url']}"
        internal_context_parts.append(content)
    
    internal_context = "\n\n".join(internal_context_parts)
    
    web_context = "\n\n".join([
        f"[ì›¹ ë¬¸ì„œ {i+1} - 2025ë…„ ìµœì‹  ì •ë³´] {doc.page_content}"
        for i, doc in enumerate(web_docs)
    ]) if web_docs else ""
    
    if not internal_docs and not web_docs:
        internal_context = "[ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ] ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    result = {
        "all_docs": internal_docs + web_docs,
        "internal_docs": internal_docs,
        "web_docs": web_docs,
        "internal_context": internal_context,
        "web_context": web_context,
        "used_web_search": len(web_docs) > 0
    }
    
    save_to_cache(query, result, 'hybrid_search')
    
    elapsed_time = time.time() - start_time
    print(f"[TIME] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM & í”„ë¡¬í”„íŠ¸ (Gemini 2.5 Flash)
llm = ChatGoogleGenerativeAI(
    google_api_key=GEMINI_API_KEY,
    model="models/gemini-2.5-flash-preview-05-20",
    temperature=0,
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
hybrid_prompt = PromptTemplate(
    input_variables=["internal_context", "web_context", "question"],
    template="""
ë‹¹ì‹ ì€ ë˜ì „ì•¤íŒŒì´í„° ì „ë¬¸ ìŠ¤í™ì—… ê°€ì´ë“œ ì±—ë´‡ì…ë‹ˆë‹¤.
ì¤‘ìš”: ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ë‹¤ìŒ ë‘ ê°€ì§€ ì •ë³´ ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

1. ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ (ì£¼ìš” ì •ë³´ì› - ê¸°ì¡´ ê°€ì´ë“œ ë° ì»¤ë®¤ë‹ˆí‹° ì •ë³´):
{internal_context}

2. ì›¹ ê²€ìƒ‰ ì •ë³´ (ë³´ì¡° ì •ë³´ì› - ìµœì‹  ì—…ë°ì´íŠ¸ ë° ì¶”ê°€ ì •ë³´):
{web_context}

ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™:
1. ì œê³µëœ ì •ë³´ ì†ŒìŠ¤ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
2. ìì²´ ì§€ì‹ì´ë‚˜ ê³¼ê±° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
3. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° "ì œê³µëœ ì •ë³´ì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì¤‘ìš”í•œ ì •ë³´ëŠ” ì–´ë–¤ ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ í‘œì‹œí•˜ì„¸ìš”(ì˜ˆ: "ë‚´ë¶€ ë¬¸ì„œì— ë”°ë¥´ë©´..." ë˜ëŠ” "ì›¹ ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥´ë©´...").

ì •ë³´ ì²˜ë¦¬ ì§€ì¹¨:
- ìš°ì„ ìˆœìœ„: ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ ì£¼ìš” ì •ë³´ì›ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì›¹ ê²€ìƒ‰ ì •ë³´ëŠ” ë³´ì¡°ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
- ì¼ê´€ì„±: ì •ë³´ ì†ŒìŠ¤ ê°„ì— ì¶©ëŒì´ ìˆìœ¼ë©´ ìµœì‹  ì •ë³´(2025ë…„)ë¥¼ ìš°ì„ ì‹œí•˜ì„¸ìš”.
- ëª…í™•ì„±: í™•ì‹¤í•œ ì •ë³´ì™€ ë¶ˆí™•ì‹¤í•œ ì •ë³´ë¥¼ êµ¬ë¶„í•˜ì—¬ ì „ë‹¬í•˜ì„¸ìš”.
- ê°„ê²°ì„±: ë¶ˆí•„ìš”í•œ ì„œë¡  ì—†ì´ í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½í•´ì„œ ê°„ëµí•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
- êµ¬ì²´ì„±: ìŠ¤í™ì—… ì¶”ì²œì€ êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ ì´ìœ ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ ì§€ì¹¨:
- ì‚¬ìš©ìê°€ ë¬»ì§€ ì•Šì€ ë‚´ìš©ì€ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
- ì§ì—…ì´ ì–¸ê¸‰ë˜ë©´ í•´ë‹¹ ì§ì—…ì— ë§ëŠ” ì •ë³´ë¥¼ ìš°ì„  ì œê³µí•˜ì„¸ìš”.
- 2025ë…„ ìµœì‹  íŒ¨ì¹˜ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ë°˜ì˜í•˜ì„¸ìš”.
- ë˜íŒŒ ê´€ë ¨ APIë‚˜ ê³µì‹ ì •ë³´ ì†ŒìŠ¤ê°€ ìˆë‹¤ë©´ ì ì ˆíˆ ì¶”ì²œí•˜ì„¸ìš”.

ì´ë²¤íŠ¸ ê´€ë ¨ ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
1. ì´ë²¤íŠ¸ ì¢…ë£Œì¼ì´ í˜„ì¬ ë‚ ì§œ(2025-05-22) ì´í›„ì¸ ê²½ìš°ì—ë§Œ ì´ë²¤íŠ¸ ì°¸ì—¬ë¥¼ ê¶Œì¥í•˜ì„¸ìš”
2. ì´ë²¤íŠ¸ê°€ ì´ë¯¸ ì¢…ë£Œëœ ê²½ìš° "í•´ë‹¹ ì´ë²¤íŠ¸ëŠ” ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ì´ë²¤íŠ¸ ì¢…ë£Œì¼ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° "ì´ë²¤íŠ¸ ì¢…ë£Œì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€:
"""
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µí•© ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def get_answer(query):
    total_start_time = time.time()
    
    search_results = smart_hybrid_search(query)
    
    llm_start_time = time.time()
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
    formatted_prompt = hybrid_prompt.format(
        internal_context=search_results["internal_context"],
        web_context=search_results["web_context"],
        question=query
    )
    
    response = llm.invoke(formatted_prompt).content
    
    llm_elapsed_time = time.time() - llm_start_time
    total_elapsed_time = time.time() - total_start_time
    
    print(f"â±ï¸ LLM ì‘ë‹µ ìƒì„± ì‹œê°„: {llm_elapsed_time:.2f}ì´ˆ")
    print(f"â±ï¸ ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
    
    return {
        "result": response,
        "source_documents": search_results["all_docs"],
        "used_web_search": search_results["used_web_search"],
        "internal_docs": search_results["internal_docs"], 
        "web_docs": search_results["web_docs"],
        "execution_times": {
            "total": total_elapsed_time,
            "llm": llm_elapsed_time
        }
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8ï¸âƒ£ ì½˜ì†” ì±„íŒ… í•¨ìˆ˜
def ask_once(q: str):
    result = get_answer(q)
    return result["result"]

if __name__ == "__main__":
    import argparse, sys

    parser = argparse.ArgumentParser(description="DF Console RAG")
    parser.add_argument("-q", "--query", type=str, help="í•œ ë²ˆë§Œ ì§ˆë¬¸í•˜ê³  ì¢…ë£Œ")
    args = parser.parse_args()

    if args.query:
        print(ask_once(args.query))
        sys.exit()

    # ëŒ€í™”í˜• ë£¨í”„
    print("ğŸ’¬ DF-RAG ì½˜ì†” ì±— (exit ì…ë ¥ ì‹œ ì¢…ë£Œ)")
    while True:
        try:
            user_in = input("\nâ–¶ï¸ ì§ˆë¬¸: ").strip()
            if not user_in or user_in.lower().startswith("exit"):
                break
            print("ğŸ§  thinking â€¦")
            
            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
            result = get_answer(user_in)
            
            # ê²€ìƒ‰ ì†ŒìŠ¤ ì •ë³´ ì¶œë ¥ (í•­ìƒ ì›¹ê²€ìƒ‰ í¬í•¨)
            print("\nâœ… ë‚´ë¶€ DB + ì›¹ ê²€ìƒ‰ ì‚¬ìš©")
            
            print("\në‹µë³€:")
            print(result["result"])
            
            print(f"\nì†Œìš” ì‹œê°„: {result['execution_times']['total']:.2f}ì´ˆ (LLM: {result['execution_times']['llm']:.2f}ì´ˆ)")
            
            # ì¶œì²˜ ì •ë³´ ì¶œë ¥
            print("\nì¶œì²˜:")
            
            for doc in result["web_docs"]:
                title = doc.metadata.get('title', '')
                if title == "Perplexity ê²€ìƒ‰ ê²°ê³¼":
                    print(f"ğŸŒ {title}")
                elif title.startswith('http'):  # URLì¸ ê²½ìš°
                    print(f"ğŸŒ {title}")
                else:
                    print(f"ğŸŒ {title}")
                
            for doc in result["internal_docs"][:3]:
                title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
                url = doc.metadata.get('url', '') or doc.metadata.get('source', '')
                if url:
                    print(f"ğŸ—„ï¸ {title} - {url}")
                else:
                    print(f"ğŸ—„ï¸ {title}")
                
        except (KeyboardInterrupt, EOFError):
            break
