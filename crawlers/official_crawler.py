import time
import requests
import re
import sys
from pathlib import Path
from bs4 import BeautifulSoup

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ config ë° crawler_utils import
sys.path.append(str(Path(__file__).resolve().parent.parent))
from config import config
from crawler_utils import (
    build_item, calculate_content_score, 
    should_process_url, filter_by_keywords
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
HEADERS = config.get_crawler_headers()
BASE_URL = config.OFFICIAL_BASE_URL
SAVE_PATH = config.OFFICIAL_RAW_PATH
FILTER_KEYWORDS = config.get_filter_keywords()
EXCLUDE_KEYWORDS = config.get_exclude_keywords()
QUALITY_THRESHOLD = config.OFFICIAL_QUALITY_THRESHOLD

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GUIDE_BASE   = f"{BASE_URL}/guide?no="
GUIDE_IDS    = [1512, 1508, 1515, 1479, 1478, 1475, 1483, 1480, 1484, 1516, 1510, 1486, 1487, 1490, 1485, 1489, 1488]          # â† í•„ìš”í•˜ë©´ ì—¬ê¸°ë§Œ ëŠ˜ë ¤ ì£¼ì„¸ìš”
GUIDE_QTHOLD = config.GUIDE_QUALITY_THRESHOLD   # guideë„ ì €ì¥í•  ìµœì†Œ í’ˆì§ˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ë‚ ì§œ í™•ì¸ í•¨ìˆ˜
def is_valid_date(date_text):
    """ë‚ ì§œê°€ ìœ íš¨í•œì§€ í™•ì¸ (2025ë…„ ì´í›„ë§Œ ìœ íš¨)"""
    # "[ë‚ ì§œ ì—†ìŒ]"ì¸ ê²½ìš° ìœ íš¨í•˜ì§€ ì•ŠìŒ
    if date_text == "[ë‚ ì§œ ì—†ìŒ]":
        return False
    
    # 2025ë…„ í™•ì¸ (í¬ë§·: "2025-05-12")
    return date_text.startswith("2025")

# ğŸ“Œ 1. ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (í•œ í˜ì´ì§€)
def get_post_list(page_num, session):
    """ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{BASE_URL}/community/dnfboard/list?category=99&page={page_num}"
    try:
        resp = session.get(url, timeout=config.CRAWLER_TIMEOUT)
        resp.raise_for_status()  # HTTP ì˜¤ë¥˜ ì²´í¬
        soup = BeautifulSoup(resp.text, "html.parser")
        posts = soup.select("article.board_list > ul")
        return posts
    except requests.exceptions.RequestException as e:
        return []

# ğŸ“Œ 2. ê²Œì‹œê¸€ URL ë° ì œëª© ì¶”ì¶œ
def parse_post_info(post):
    """ê²Œì‹œê¸€ì—ì„œ URLê³¼ ì œëª© ì¶”ì¶œ"""
    title_li = post.select_one("li.title")
    if not title_li:
        return None, None

    link_tag = title_li.find_all("a")[-1]
    href = link_tag.get("href", "").strip()
    if href.startswith("/community/dnfboard/article/"):
        post_url = BASE_URL + href
    else:
        return None, None
        
    # ì œëª© ì¶”ì¶œ
    title_text = link_tag.get_text(strip=True)
    
    return post_url, title_text

# ğŸ“Œ 3. ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§ (ë³¸ë¬¸ ë‚´ URLë„ ì¬ê·€ í¬ë¡¤ë§)
def crawl_post_content(post_url, session, visited_urls, depth=0, max_depth=2):
    """ê²Œì‹œê¸€ ë‚´ìš© í¬ë¡¤ë§ ë° ì¬ê·€ì ìœ¼ë¡œ ë§í¬ íƒìƒ‰"""
    # ì¦ë¶„ í¬ë¡¤ë§: ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ê±´ë„ˆëœ€
    if not should_process_url(post_url, visited_urls):
        return []

    # ë°©ë¬¸ ê¸°ë¡ì— ì¶”ê°€
    visited_urls.add(post_url)
    results = []

    try:
        # ê²Œì‹œê¸€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        resp = session.get(post_url, timeout=config.CRAWLER_TIMEOUT)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ì œëª© ì¶”ì¶œ
        title_tag = soup.select_one("p.commu1st span")
        title_text = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"
        title_text = re.sub(r"\s+", " ", title_text)

        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.select_one("ul.commu2nd span.date")
        if date_tag:
            raw = date_tag.get_text(strip=True)
            
            # ìˆ˜ì •ì¼ ìš°ì„ , ì—†ìœ¼ë©´ ë“±ë¡ì¼ ì‚¬ìš©
            if "ìˆ˜ì • :" in raw:
                date_part = raw.split("ìˆ˜ì • :")[1].strip()
            else:
                date_match = re.search(r"ë“±ë¡ : (\d{4}\.\d{2}\.\d{2})", raw)
                date_part = date_match.group(1) if date_match else "[ë‚ ì§œ ì—†ìŒ]"
            
            # ë‚ ì§œë§Œ ì¶”ì¶œí•˜ê³  í˜•ì‹ ë³€í™˜ (YYYY-MM-DD)
            if date_part != "[ë‚ ì§œ ì—†ìŒ]":
                date_text = date_part.split(" ")[0].replace(".", "-")
            else:
                date_text = "[ë‚ ì§œ ì—†ìŒ]"
        else:
            date_text = "[ë‚ ì§œ ì—†ìŒ]"

        # 2025ë…„ ê²Œì‹œê¸€ë§Œ í—ˆìš©
        if not is_valid_date(date_text):
            return []
        
        # ì¡°íšŒìˆ˜ ì¶”ì¶œ
        hit_count = 0
        hit_tag = soup.select_one("ul.commu2nd li span.hits")
        if hit_tag:
            try:
                hit_text = hit_tag.get_text(strip=True)
                hit_count = int(hit_text.replace(',', ''))
            except ValueError:
                hit_count = 0
        
        # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
        like_count = 0
        like_tag = soup.select_one("ul.commu2nd li span.like")
        if like_tag:
            try:
                like_count = int(like_tag.get_text(strip=True).replace(',', '') or 0)
            except ValueError:
                like_count = 0

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_div = soup.select_one("div.bd_viewcont")
        content_text = content_div.get_text("\n", strip=True) if content_div else "[ë³¸ë¬¸ ì—†ìŒ]"
        
        # ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (utils.pyì˜ calculate_content_score ì‚¬ìš©)
        content_score = calculate_content_score(content_text, title_text)
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì´ìƒì˜ ê²Œì‹œê¸€ë§Œ ì €ì¥
        if content_score >= QUALITY_THRESHOLD:
            item = build_item(
                source="official",
                url=post_url,
                title=title_text,
                body=content_text,
                date=date_text,
                views=hit_count,
                likes=like_count
            )
            results.append(item)

        # ğŸ” ë³¸ë¬¸ ë‚´ ì¶”ê°€ ê²Œì‹œê¸€ ë§í¬ (depth ì œí•œ í¬í•¨)
        if content_div and depth < max_depth:
            for a in content_div.find_all("a", href=True):
                linked_href = a["href"]
                if linked_href.startswith("/community/dnfboard/article/"):
                    # ë§í¬ í…ìŠ¤íŠ¸(ì œëª©) ì¶”ì¶œ
                    link_text = a.get_text(strip=True)

                    # í‚¤ì›Œë“œ í•„í„°ë§ (utils.pyì˜ filter_by_keywords ì‚¬ìš©)
                    if not filter_by_keywords(link_text, FILTER_KEYWORDS, EXCLUDE_KEYWORDS):
                        continue
                    
                    full_link = BASE_URL + linked_href
                    results.extend(crawl_post_content(full_link, session, visited_urls, depth + 1, max_depth))

        # ìš”ì²­ ê°„ ë”œë ˆì´
        time.sleep(config.CRAWLER_DELAY)

    except requests.exceptions.RequestException as e:
        pass
    except Exception as e:
        pass

    return results

def crawl_guide_page(guide_no, session):
    """
    ë‹¨ì¼ ê°€ì´ë“œ í˜ì´ì§€ í¬ë¡¤ë§
    - <article class="content gg_template"> ì•ˆì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
    - ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì¼ìë¥¼ date í•„ë“œì— ì €ì¥ (YYYY-MM-DD)
    """
    url = f"{GUIDE_BASE}{guide_no}"
    try:
        resp = session.get(url, timeout=config.CRAWLER_TIMEOUT)
        resp.raise_for_status()
    except requests.exceptions.RequestException:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")
    article = soup.select_one("article.content.gg_template")
    if not article:
        return None

    # â‘  ì œëª©
    title_tag = article.find(["h1", "h2"])
    title = title_tag.get_text(strip=True) if title_tag else f"[ê°€ì´ë“œ] no={guide_no}"

    # â‘¡ ë³¸ë¬¸ - ì´ë¯¸ì§€ ALT í¬í•¨, <br> â†’ \n
    for br in article.find_all("br"):
        br.replace_with("\n")
    body_text = article.get_text("\n", strip=True)

    # â‘¢ ë‚ ì§œ
    date_tag = article.select_one("div.last_update")
    date_text = "[ë‚ ì§œ ì—†ìŒ]"
    if date_tag:
        m = re.search(r"(\d{4}-\d{2}-\d{2})", date_tag.get_text())
        if m:
            date_text = m.group(1)

    # â‘£ í’ˆì§ˆ ìŠ¤ì½”ì–´ & í•„í„°
    score = calculate_content_score(body_text, title)
    if score < GUIDE_QTHOLD:
        return None

    return build_item(
        source="guide",
        url=url,
        title=title,
        body=body_text,
        date=date_text,
        views=0,
        likes=0
    )

# ğŸ“Œ 4. ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
def crawl_df(max_pages=2, max_depth=2, visited_urls=None, is_incremental=True):
    """ê³µì‹ ì‚¬ì´íŠ¸ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
    # ì¦ë¶„ í¬ë¡¤ë§ì„ ìœ„í•œ ë°©ë¬¸ URL ê´€ë¦¬
    if visited_urls is None:
        visited_urls = set()
    
    # ê²°ê³¼ ë° ì„¸ì…˜ ì´ˆê¸°í™”
    session = requests.Session()
    session.headers.update(HEADERS)
    results = []
    notice_processed = False
    start_time = time.time()

    try:
        # í˜ì´ì§€ë³„ í¬ë¡¤ë§
        for page in range(1, max_pages + 1):
            posts = get_post_list(page, session)

            # ê²Œì‹œê¸€ë³„ ì²˜ë¦¬
            for post in posts:
                post_url, title_text = parse_post_info(post)
                if not post_url or not title_text:
                    continue
                    
                # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
                if not filter_by_keywords(title_text, FILTER_KEYWORDS, EXCLUDE_KEYWORDS):
                    continue

                # ê³µì§€ê¸€ / ì¼ë°˜ê¸€ êµ¬ë¶„
                is_notice = 'notice' in post.get("class", [])

                # ê³µì§€ê¸€ì€ í•œ ë²ˆë§Œ ì²˜ë¦¬
                if is_notice:
                    if not notice_processed:
                        results.extend(crawl_post_content(post_url, session, visited_urls, depth=0, max_depth=max_depth))
                    continue
                else:
                    # ì¼ë°˜ ê²Œì‹œê¸€ ì²˜ë¦¬
                    results.extend(crawl_post_content(post_url, session, visited_urls, depth=0, max_depth=max_depth))

            # ê³µì§€ê¸€ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            if not notice_processed:
                notice_processed = True
            
        # â”€â”€ ê²Œì‹œíŒ í¬ë¡¤ë§ ëë‚œ ë’¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # â‘¡ ê³µì‹ ê°€ì´ë“œ í¬ë¡¤ë§ : í•„ìš”í•  ë•Œ ì‚¬ìš©
        for gid in GUIDE_IDS:
            item = crawl_guide_page(gid, session)
            if item:
                item["quality_score"] = 9.0
                results.append(item)

        # ê²°ê³¼ ìš”ì•½
        elapsed_time = time.time() - start_time
        avg_time_per_post = elapsed_time / len(results) if results else 0

        # ê²°ê³¼ ì €ì¥ (ì¦ë¶„ ì²˜ë¦¬ ì§€ì›)
        from crawler_utils import save_official_data
        save_official_data(results, append=is_incremental)
        
    except Exception as e:
        pass
    
    return results

# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    crawl_df(max_pages=1, max_depth=1)
