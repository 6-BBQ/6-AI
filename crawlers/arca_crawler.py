import json
import time
import datetime
import cloudscraper
from bs4 import BeautifulSoup, NavigableString

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {"User-Agent": "Mozilla/5.0"}
BASE_URL = "https://arca.live"
SAVE_PATH = "data/raw/arca_row.json"
FILTER_KEYWORDS = ["ëª…ì„±", "ë˜ì „", "ìŠ¤í™ì—…", "ì¥ë¹„", "íŒŒë°", "ë‰´ë¹„", "ìœ ì…", "ì´ˆë³´ì", "ìœµí•©ì„", "ì¤‘ì²œ", "ì„¸íŠ¸", "ë‚˜ë²¨", "ë² ëˆ„ìŠ¤",
                   "ê°€ì´ë“œ", "ê³µëµ", "ì—í”½", "íƒœì´ˆ", "ì†Œìš¸", "ë ˆê¸°ì˜¨", "ë ˆì´ë“œ", "í˜„ì§ˆ", "ì„¸ë¦¬ì•„", "ì¤€ì¢…ê²°", "ì¢…ê²°"]
EXCLUDE_KEYWORDS = ["ì´ë²¤íŠ¸", "ì„ ê³„", "ì»¤ìŠ¤í…€", "ì¹´ì§€ë…¸", "ê¸°ë¡ì‹¤", "ì„œê³ ", "ë°”ì¹¼", "ã……ã…‚", "ã…‚ã……", "ã…„", "ã…—", "ì‹œë°œ", "ì”¨ë°œ", "ë³‘ì‹ ", "ì¢†"]
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ë‚ ì§œ í™•ì¸ í•¨ìˆ˜ ì¶”ê°€
def is_valid_date(date_text):
    # "[ë‚ ì§œ ì—†ìŒ]"ì¸ ê²½ìš° ìœ íš¨í•˜ì§€ ì•ŠìŒ
    if date_text == "[ë‚ ì§œ ì—†ìŒ]":
        return False
    
    # 2025ë…„ í™•ì¸ (í¬ë§·: "2025-05-12")
    return date_text.startswith("2025")

# ğŸ“Œ Cloudflare ìš°íšŒìš© ì„¸ì…˜ ìƒì„±
def get_new_scraper():
    return cloudscraper.create_scraper()

# ğŸ“Œ 1. ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (í•œ í˜ì´ì§€)
def get_post_list(page_num):
    url = f"{BASE_URL}/b/dunfa?mode=best&category=ê³µëµ&p={page_num}"
    scraper = get_new_scraper()
    resp = scraper.get(url)
    soup = BeautifulSoup(resp.text, "html.parser")
    return soup.select("a.vrow")

# ğŸ“Œ 2. ê²Œì‹œê¸€ URL ë° ì œëª© ì¶”ì¶œ
def parse_post_info(post):
    href = post.get("href", "").split("?")[0]
    if not href.startswith("/b/"):
        return None, None
        
    post_url = BASE_URL + href
    
    # ì œëª© í‚¤ì›Œë“œ ì¶”ì¶œ
    title_tag = post.select_one("span.title")
    if not title_tag:
        return post_url, None
        
    title_text = title_tag.get_text(strip=True)
    
    return post_url, title_text

# ğŸ“Œ 3. ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ì œëª© í•„í„°ë§
def crawl_post_content(post_url, visited_urls, depth=0, max_depth=2):
    if post_url in visited_urls:
        return []

    visited_urls.add(post_url)
    results = []
    
    try:
        scraper = get_new_scraper()
        resp = scraper.get(post_url)
        soup = BeautifulSoup(resp.text, "html.parser")

        title_tag = soup.select_one("div.title-row .title")
        if title_tag:
            title_text = ''.join(
                t for t in title_tag.contents if isinstance(t, NavigableString)
            ).strip()
        else:
            title_text = "[ì œëª© ì—†ìŒ]"
        
        date_tag = soup.select_one("div.article-info-section .date time")
        if date_tag:
            raw = date_tag.get("datetime")
            date_text = datetime.datetime.strptime(raw, "%Y-%m-%dT%H:%M:%S.000Z").strftime("%Y-%m-%d")
        else:
            date_text = "[ë‚ ì§œ ì—†ìŒ]"

        # 2025ë…„ ê²Œì‹œê¸€ë§Œ í—ˆìš©
        if not is_valid_date(date_text):
            return []

        content_div = soup.select_one("div.fr-view.article-content")
        content_text = content_div.get_text("\n", strip=True) if content_div else "[ë³¸ë¬¸ ì—†ìŒ]"

        # ì œëª©ì€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ í•„í„°ë§í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” í•„í„°ë§í•˜ì§€ ì•ŠìŒ
        
        post_data = {
            "url": post_url,
            "title": title_text,
            "date": date_text,
            "content": content_text
        }
    
        results.append(post_data)

        # ğŸ” ë³¸ë¬¸ ë‚´ ì¶”ê°€ ê²Œì‹œê¸€ ë§í¬ (depth ì œí•œ í¬í•¨)
        if content_div and depth < max_depth:
            for a in content_div.find_all("a", href=True):
                linked_href = a["href"]
                if linked_href.startswith("/b/dunfa"):
                    # ë§í¬ í…ìŠ¤íŠ¸(ì œëª©) ì¶”ì¶œ
                    link_text = a.get_text(strip=True)
                    
                    # ë§í¬ ì œëª© í•„í„°ë§ - ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬ëŠ” ê±´ë„ˆë›°ê¸°
                    if any(bad_word in link_text for bad_word in EXCLUDE_KEYWORDS):
                        continue
                    
                    # í¬í•¨ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸ - ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    if not any(keyword in link_text for keyword in FILTER_KEYWORDS):
                        continue
                    
                    full_link = BASE_URL + linked_href
                    results.extend(crawl_post_content(full_link, visited_urls, depth + 1, max_depth))

        time.sleep(0.01)

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

    return results

# ğŸ“Œ 4. ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
def crawl_arca(max_pages=2, max_depth=2):
    visited_urls = set()
    results = []
    notice_processed = False

    for page in range(1, max_pages + 1):
        print(f"\nğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        posts = get_post_list(page)

        for post in posts:
            post_url, title_text = parse_post_info(post)
            if not post_url or not title_text:
                continue
                
            # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œëª© í•„í„°ë§
            if not any(keyword in title_text for keyword in FILTER_KEYWORDS):
                continue
                
            if any(bad_word in title_text for bad_word in EXCLUDE_KEYWORDS):
                continue

            is_notice = 'notice' in post.get("class", [])

            if is_notice:
                if not notice_processed:
                    print(f"ğŸ“Œ ê³µì§€ê¸€ ìˆ˜ì§‘: {post_url}")
                    results.extend(crawl_post_content(post_url, visited_urls, depth=0, max_depth=max_depth))
                continue
            else:
                results.extend(crawl_post_content(post_url, visited_urls, depth=0, max_depth=max_depth))

        if not notice_processed:
            notice_processed = True

    with open(SAVE_PATH, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì´ {len(results)}ê°œì˜ ê²Œì‹œê¸€ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")