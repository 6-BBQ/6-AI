# arca_crawler.py (ê°œì„  ë²„ì „)
import json
import time
import cloudscraper
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup, NavigableString
from utils import (
    build_item, clean_text, calculate_content_score,
    should_process_url, filter_by_keywords
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {"User-Agent": "Mozilla/5.0"}
BASE_URL = "https://arca.live"
SAVE_PATH = "data/raw/arca_raw.json"

# í•„í„° í‚¤ì›Œë“œ (ì¤‘ìš”ë„ì— ë”°ë¼ ì •ë ¬)
FILTER_KEYWORDS = [
    "ëª…ì„±", "ìƒê¸‰ ë˜ì „", "ìŠ¤í™ì—…", "ì¥ë¹„", "íŒŒë°", "ë‰´ë¹„", "ìœµí•©ì„", "ì¤‘ì²œ", "ì„¸íŠ¸",
    "ê°€ì´ë“œ", "ì—í”½", "íƒœì´ˆ", "ë ˆê¸°ì˜¨", "ë ˆì´ë“œ", "í˜„ì§ˆ", "ì„¸ë¦¬ì•„", "ë§ˆë²•ë¶€ì—¬", 
    "ìŠ¤í‚¬íŠ¸ë¦¬", "ì¢…ë§ì˜ ìˆ­ë°°ì"
]

# ì œì™¸ í‚¤ì›Œë“œ
EXCLUDE_KEYWORDS = [
    "ì´ë²¤íŠ¸", "ì„ ê³„", "ì»¤ìŠ¤í…€", "ì¹´ì§€ë…¸", "ê¸°ë¡ì‹¤", "ì„œê³ ", "ë°”ì¹¼", "ì´ìŠ¤í•€ì¦ˆ", 
    "ì–´ë‘‘ì„¬", "ê¹¨ì–´ë‚œ ìˆ²", "ã……ã…‚", "ã…‚ã……", "ã…„", "ã…—", "ì‹œë°œ", "ì”¨ë°œ", "ë³‘ì‹ ", "ì¢†"
]

# í’ˆì§ˆ ì ìˆ˜ ì„ê³„ê°’ (ì´ ì ìˆ˜ ì´ìƒì¸ ê²Œì‹œê¸€ë§Œ ì €ì¥)
QUALITY_THRESHOLD = 38  # ì•„ì¹´ë¼ì´ë¸ŒëŠ” ë‚´ìš©ì´ ì¤‘ê°„ ì •ë„ì˜ ê¸¸ì´ê°€ ë§ìŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ë‚ ì§œ í™•ì¸ í•¨ìˆ˜
def is_valid_date(date_text):
    """ë‚ ì§œê°€ ìœ íš¨í•œì§€ í™•ì¸ (2025ë…„ ì´í›„ë§Œ ìœ íš¨)"""
    # "[ë‚ ì§œ ì—†ìŒ]"ì¸ ê²½ìš° ìœ íš¨í•˜ì§€ ì•ŠìŒ
    if date_text == "[ë‚ ì§œ ì—†ìŒ]":
        return False
    
    # 2025ë…„ í™•ì¸ (í¬ë§·: "2025-05-12")
    return date_text.startswith("2025")

# ğŸ“Œ Cloudflare ìš°íšŒìš© ì„¸ì…˜ ìƒì„±
def get_new_scraper():
    """Cloudflare ë³´í˜¸ë¥¼ ìš°íšŒí•˜ëŠ” ìŠ¤í¬ë˜í¼ ìƒì„±"""
    try:
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            }
        )
        scraper.headers.update(HEADERS)
        return scraper
    except Exception as e:
        # ê¸°ë³¸ ìŠ¤í¬ë˜í¼ë¡œ ëŒ€ì²´
        return cloudscraper.create_scraper()

# ğŸ“Œ 1. ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (í•œ í˜ì´ì§€)
def get_post_list(page_num):
    """ì•„ì¹´ë¼ì´ë¸Œì—ì„œ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{BASE_URL}/b/dunfa?mode=best&category=ê³µëµ&p={page_num}"
    try:
        scraper = get_new_scraper()
        resp = scraper.get(url, timeout=15)  # ì•„ì¹´ë¼ì´ë¸ŒëŠ” ë¡œë”©ì´ ëŠë¦´ ìˆ˜ ìˆì–´ íƒ€ì„ì•„ì›ƒ ì¦ê°€
        soup = BeautifulSoup(resp.text, "html.parser")
        posts = soup.select("a.vrow")
        return posts
    except Exception as e:
        return []

# ğŸ“Œ 2. ê²Œì‹œê¸€ URL ë° ì œëª© ì¶”ì¶œ
def parse_post_info(post):
    """ê²Œì‹œê¸€ì—ì„œ URLê³¼ ì œëª© ì¶”ì¶œ"""
    href = post.get("href", "").split("?")[0]
    if not href.startswith("/b/"):
        return None, None
        
    post_url = BASE_URL + href
    
    # ì œëª© í‚¤ì›Œë“œ ì¶”ì¶œ
    title_tag = post.select_one("span.title")
    if not title_tag:
        return post_url, None
        
    title_text = title_tag.get_text(strip=True)
    
    return post_url, title_text

# ğŸ“Œ 3. ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ì œëª© í•„í„°ë§
def crawl_post_content(post_url, visited_urls, depth=0, max_depth=2):
    """ê²Œì‹œê¸€ ë‚´ìš© í¬ë¡¤ë§ ë° ì¬ê·€ì ìœ¼ë¡œ ë§í¬ íƒìƒ‰"""
    # ì¦ë¶„ í¬ë¡¤ë§: ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ê±´ë„ˆëœ€
    if not should_process_url(post_url, visited_urls):
        return []

    # ë°©ë¬¸ ê¸°ë¡ì— ì¶”ê°€
    visited_urls.add(post_url)
    results = []
    
    try:
        # ê²Œì‹œê¸€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        scraper = get_new_scraper()
        resp = scraper.get(post_url, timeout=15)
        soup = BeautifulSoup(resp.text, "html.parser")

        # ì œëª© ì¶”ì¶œ
        title_tag = soup.select_one("div.title-row .title")
        if title_tag:
            title_text = ''.join(
                t for t in title_tag.contents if isinstance(t, NavigableString)
            ).strip()
        else:
            title_text = "[ì œëª© ì—†ìŒ]"
        
        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.select_one("div.article-info-section .date time")
        if date_tag:
            raw = date_tag.get("datetime")
            date_text = datetime.strptime(raw, "%Y-%m-%dT%H:%M:%S.000Z").strftime("%Y-%m-%d")
        else:
            date_text = "[ë‚ ì§œ ì—†ìŒ]"

        # 2025ë…„ ê²Œì‹œê¸€ë§Œ í—ˆìš©
        if not is_valid_date(date_text):
            return []

        # ì¡°íšŒìˆ˜ ì¶”ì¶œ
        hit_count = 0
        hit_tag = soup.select_one(
            "div.article-info-section span.head:-soup-contains('Views') + span.body"
        )
        if hit_tag:
            try:
                hits_text = hit_tag.get_text(strip=True)
                hit_count = int(hits_text.replace(",", ""))
            except ValueError:
                hit_count = 0

        # ì¶”ì²œìˆ˜ ì¶”ì¶œ
        like_count = 0
        like_tag = soup.select_one(
            "div.article-info-section span.head:-soup-contains('Like') + span.body"
        )
        if like_tag:
            try:
                like_text = like_tag.get_text(strip=True)
                like_count = int(like_text.replace(",", ""))
            except ValueError:
                like_count = 0

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_div = soup.select_one("div.fr-view.article-content")
        content_text = content_div.get_text("\n", strip=True) if content_div else "[ë³¸ë¬¸ ì—†ìŒ]"
        
        # ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        content_score = calculate_content_score(content_text, title_text)
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì´ìƒì˜ ê²Œì‹œê¸€ë§Œ ì €ì¥
        if content_score >= QUALITY_THRESHOLD:
            item = build_item(
                source="arca",
                url=post_url,
                title=title_text,
                body=content_text,
                date=date_text,
                views=hit_count,
                likes=like_count
            )
            results.append(item)

        # ğŸ” ë³¸ë¬¸ ë‚´ ì¶”ê°€ ê²Œì‹œê¸€ ë§í¬ (depth ì œí•œ í¬í•¨)
        if content_div and depth < max_depth:
            for a in content_div.find_all("a", href=True):
                linked_href = a["href"]
                if linked_href.startswith("/b/dunfa"):
                    # ë§í¬ í…ìŠ¤íŠ¸(ì œëª©) ì¶”ì¶œ
                    link_text = a.get_text(strip=True)
                    
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if not filter_by_keywords(link_text, FILTER_KEYWORDS, EXCLUDE_KEYWORDS):
                        continue
                    
                    full_link = BASE_URL + linked_href
                    results.extend(crawl_post_content(full_link, visited_urls, depth + 1, max_depth))

        # ìš”ì²­ ê°„ ë”œë ˆì´ (ì•„ì¹´ë¼ì´ë¸ŒëŠ” ë” ê¸´ ë”œë ˆì´ í•„ìš”)
        time.sleep(0.1)

    except Exception as e:
        pass

    return results

# ğŸ“Œ 4. ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
def crawl_arca(max_pages=2, max_depth=2, visited_urls=None):
    """ì•„ì¹´ë¼ì´ë¸Œ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
    # ì¦ë¶„ í¬ë¡¤ë§ì„ ìœ„í•œ ë°©ë¬¸ URL ê´€ë¦¬
    if visited_urls is None:
        visited_urls = set()
    
    results = []
    notice_processed = False
    start_time = time.time()
    
    try:
        # í˜ì´ì§€ë³„ í¬ë¡¤ë§
        for page in range(1, max_pages + 1):
            posts = get_post_list(page)

            # ê²Œì‹œê¸€ë³„ ì²˜ë¦¬
            for post in posts:
                post_url, title_text = parse_post_info(post)
                if not post_url or not title_text:
                    continue
                    
                # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
                if not filter_by_keywords(title_text, FILTER_KEYWORDS, EXCLUDE_KEYWORDS):
                    continue

                # ê³µì§€ê¸€ í™•ì¸
                is_notice = 'notice' in post.get("class", [])

                # ê³µì§€ê¸€ì€ í•œ ë²ˆë§Œ ì²˜ë¦¬
                if is_notice:
                    if not notice_processed:
                        results.extend(crawl_post_content(post_url, visited_urls, depth=0, max_depth=max_depth))
                    continue
                else:
                    # ì¼ë°˜ ê²Œì‹œê¸€ ì²˜ë¦¬
                    results.extend(crawl_post_content(post_url, visited_urls, depth=0, max_depth=max_depth))

            # ê³µì§€ê¸€ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            if not notice_processed:
                notice_processed = True

        # ê²°ê³¼ ìš”ì•½
        elapsed_time = time.time() - start_time
        avg_time_per_post = elapsed_time / len(results) if results else 0

        # ê²°ê³¼ ì €ì¥
        save_dir = Path(SAVE_PATH).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        with open(SAVE_PATH, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
    except Exception as e:
        pass
    
    return results

# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    crawl_arca(max_pages=2, max_depth=2)
