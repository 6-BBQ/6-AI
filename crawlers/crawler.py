# crawler.py (ê°„ê²°í•œ ë²„ì „)
from __future__ import annotations
import argparse, sys, textwrap, os, json, time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from official_crawler import crawl_df
from dc_crawler import crawl_dcinside
from arca_crawler import crawl_arca
from youtube_crawler import crawl_youtube
from utils import get_logger

# ë°©ë¬¸í•œ URL ì €ì¥ì†Œ (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)
VISITED_URLS_FILE = "data/visited_urls.json"

def load_visited_urls():
    """ì´ì „ì— ë°©ë¬¸í•œ URL ëª©ë¡ ë¡œë“œ"""
    try:
        if os.path.exists(VISITED_URLS_FILE):
            with open(VISITED_URLS_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        return set()
    except Exception as e:
        return set()

def save_visited_urls(urls):
    """ë°©ë¬¸í•œ URL ëª©ë¡ ì €ì¥"""
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(VISITED_URLS_FILE), exist_ok=True)
        
        with open(VISITED_URLS_FILE, "w", encoding="utf-8") as f:
            json.dump(list(urls), f, ensure_ascii=False)
    except Exception as e:
        pass

def run_crawler(crawler_func, *args, **kwargs):
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ í•¨ìˆ˜ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
    start_time = time.time()
    func_name = crawler_func.__name__
    
    try:
        result = crawler_func(*args, **kwargs)
        elapsed = time.time() - start_time
        return result
    except Exception as e:
        print(f"âš ï¸ {func_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

def main():
    # ë¡œê±° ì´ˆê¸°í™”
    logger = get_logger(__name__)
    
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent("""\
            â–¶ ë˜íŒŒ ìŠ¤í™ì—… ê°€ì´ë“œìš© í†µí•© í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
            
            ê¸°ë³¸ì ìœ¼ë¡œ ì¦ë¶„ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            ì „ì²´ í¬ë¡¤ë§ì´ í•„ìš”í•œ ê²½ìš° --full ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        """)
    )
    parser.add_argument("--pages", type=int, default=10, help="ê° ê²Œì‹œíŒ ìµœëŒ€ í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--depth", type=int, default=2, help="ë³¸ë¬¸ ë§í¬ ì¬ê·€ depth")
    parser.add_argument("--yt-mode", type=str, default="hybrid", 
                        choices=["channel", "search", "hybrid"],
                        help="YouTube í¬ë¡¤ë§ ëª¨ë“œ (channel: ì±„ë„ë§Œ, search: ê²€ìƒ‰ë§Œ, hybrid: ë‘˜ ë‹¤)")
    parser.add_argument("--yt-channel", type=str,
                        default="https://www.youtube.com/@zangzidnf",
                        help="YouTube ì±„ë„ URL(@í•¸ë“¤ ë˜ëŠ” /channel/ID)")
    parser.add_argument("--yt-query", type=str, default="ë˜íŒŒ ê°€ì´ë“œ",
                        help="YouTube ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, ë‹¤ì¤‘ ì¿¼ë¦¬ ì‚¬ìš©)")
    parser.add_argument("--yt-max", type=int, default=20,
                        help="ì±„ë„ì—ì„œ ê°€ì ¸ì˜¬ ìµœì‹  ì˜ìƒ ê°œìˆ˜")
    parser.add_argument("--parallel", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=4, help="ë³‘ë ¬ ì‘ì—…ì ìˆ˜")
    parser.add_argument("--incremental", action="store_true", default=True, help="ì¦ë¶„ í¬ë¡¤ë§ (ê¸°ë³¸ê°’)")
    parser.add_argument("--full", action="store_true", help="ì „ì²´ í¬ë¡¤ë§ (ì¦ë¶„ ë¬´ì‹œ)")
    parser.add_argument("--clear-history", action="store_true", help="ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™”")
    parser.add_argument("--sources", type=str, default="all", 
                        help="í¬ë¡¤ë§í•  ì†ŒìŠ¤ (ì½¤ë§ˆë¡œ êµ¬ë¶„: official,dc,arca,youtube,all)")
    parser.add_argument("--quality-threshold", type=int, default=0,
                        help="ìµœì†Œ í’ˆì§ˆ ì ìˆ˜ (ì´ ì ìˆ˜ ë¯¸ë§Œì˜ í•­ëª©ì€ ìµœì¢… ê²°ê³¼ì—ì„œ ì œì™¸)")
    parser.add_argument("--merge", action="store_true", help="ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©")

    args = parser.parse_args()
    
    # ì „ì²´ ëª¨ë“œ ê²€ì‚¬
    if args.full:
        args.incremental = False
    
    # ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™” ì˜µì…˜
    if args.clear_history:
        try:
            if os.path.exists(VISITED_URLS_FILE):
                os.remove(VISITED_URLS_FILE)
        except Exception as e:
            pass
        return

    logger.info(f"\nğŸ”” í†µí•© í¬ë¡¤ë§ ì‹œì‘ ({datetime.now():%Y-%m-%d %H:%M:%S})")
    logger.info(f"   - pages = {args.pages}, depth = {args.depth}")
    logger.info(f"   - YouTube ëª¨ë“œ = {args.yt_mode}")
    if args.yt_mode in ['hybrid', 'channel']:
        logger.info(f"   - yt-channel = {args.yt_channel}")
    if args.yt_mode in ['hybrid', 'search']:
        logger.info(f"   - yt-query = 'ë˜íŒŒ ê°€ì´ë“œ(10), í˜„ì§ˆê°€ì´ë“œ(5), ë‚˜ë²¨ê³µëµ(5)'")
    logger.info(f"   - yt-max = {args.yt_max}")
    logger.info(f"   - ë³‘ë ¬ ì²˜ë¦¬ = {args.parallel}, ì‘ì—…ì ìˆ˜ = {args.workers}")
    logger.info(f"   - ì¦ë¶„ í¬ë¡¤ë§ = {args.incremental}")
    
    # ì¦ë¶„ í¬ë¡¤ë§ì„ ìœ„í•œ ë°©ë¬¸ URL ë¡œë“œ
    if args.incremental:
        visited_urls = load_visited_urls()
    else:
        # ì „ì²´ ëª¨ë“œì¼ ë•ŒëŠ” ë¹ˆ ì§‘í•©ìœ¼ë¡œ ì‹œì‘ (ëª¨ë“  URL ì¬ì²˜ë¦¬)
        visited_urls = set()
    
    # í¬ë¡¤ë§í•  ì†ŒìŠ¤ ê²°ì •
    sources = args.sources.lower().split(',')
    crawl_all = "all" in sources
    
    # í¬ë¡¤ë§ ì‘ì—… ì •ì˜
    crawl_tasks = []
    
    # ì¦ë¶„ ëª¨ë“œ ì„¤ì •
    is_incremental = args.incremental
    
    if crawl_all or "official" in sources:
        crawl_tasks.append(("ê³µí™ˆ", lambda: run_crawler(crawl_df, args.pages, args.depth, visited_urls, is_incremental)))
    
    if crawl_all or "dc" in sources:
        crawl_tasks.append(("ë””ì‹œ", lambda: run_crawler(crawl_dcinside, args.pages, args.depth, visited_urls, is_incremental)))
    
    if crawl_all or "arca" in sources:
        crawl_tasks.append(("ì•„ì¹´", lambda: run_crawler(crawl_arca, args.pages, args.depth, visited_urls, is_incremental)))
    
    if crawl_all or "youtube" in sources:
        def youtube_crawl_task():
            youtube_results = []
            if args.yt_mode in ["hybrid", "search"]:
                # ì—¬ëŸ¬ ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ë§ (ì¹´í…Œê³ ë¦¬ë³„)
                from youtube_crawler import crawl_youtube_multi_query
                
                # ë˜íŒŒ ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •
                search_queries = [
                    ("ë˜íŒŒ ê°€ì´ë“œ", 10),       # ë˜íŒŒ ê°€ì´ë“œ 10ê°œ
                    ("ë˜íŒŒ í˜„ì§ˆ ê°€ì´ë“œ", 5),    # ë˜íŒŒ í˜„ì§ˆ ê°€ì´ë“œ 5ê°œ
                    ("ë˜íŒŒ ë‚˜ë²¨ ê³µëµ", 5),      # ë˜íŒŒ ë‚˜ë²¨ ê³µëµ 5ê°œ
                ]
                
                search_results = run_crawler(crawl_youtube_multi_query, search_queries, visited_urls)
                youtube_results.extend(search_results)
                
            if args.yt_mode in ["hybrid", "channel"]:
                # ì±„ë„ ê¸°ë°˜ í¬ë¡¤ë§, ì„ì‹œë¡œ êº¼ë‘ 
                from youtube_crawler import crawl_youtube_channel
                # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì¼ ë•ŒëŠ” ì±„ë„ í¬ë¡¤ë§ ê°œìˆ˜ë¥¼ ì¤„ì—¬ì„œ ê²€ìƒ‰ ê²°ê³¼ì™€ ê· í˜• ë§ì¶¤
                channel_results = run_crawler(crawl_youtube_channel, args.yt_channel, 3 if args.yt_mode == "hybrid" else args.yt_max, visited_urls)
                youtube_results.extend(channel_results)
            
            # YouTube ê²°ê³¼ë¥¼ ê°œë³„ íŒŒì¼ì— ì €ì¥ (ì¦ë¶„ ëª¨ë“œ ì§€ì›)
            if youtube_results:
                youtube_raw_path = "data/raw/youtube_raw.json"
                
                # ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(os.path.dirname(youtube_raw_path), exist_ok=True)
                
                try:
                    if args.incremental and os.path.exists(youtube_raw_path):
                        # ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„° ë¡œë“œ í›„ ë³‘í•©
                        with open(youtube_raw_path, "r", encoding="utf-8") as f:
                            existing_data = json.load(f)
                        
                        # URL ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê¸°ì¡´ URL ì§‘í•©
                        existing_urls = {item.get('url') for item in existing_data if isinstance(item, dict) and 'url' in item}
                        
                        # ìƒˆë¡œìš´ ë°ì´í„° ì¤‘ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ì¶”ê°€
                        new_data = [item for item in youtube_results if item.get('url') not in existing_urls]
                        
                        if new_data:
                            final_data = existing_data + new_data
                            print(f"ğŸ’¾ YouTube ì¦ë¶„ ì €ì¥: ê¸°ì¡´ {len(existing_data)}ê°œ + ìƒˆë¡œìš´ {len(new_data)}ê°œ")
                        else:
                            final_data = existing_data
                            print(f"ğŸ’¾ YouTube: ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ (ëª¨ë‘ ì¤‘ë³µ)")
                    else:
                        # ì „ì²´ ëª¨ë“œ ë˜ëŠ” íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°: ì „ì²´ ì €ì¥
                        final_data = youtube_results
                        print(f"ğŸ’¾ YouTube ì „ì²´ ì €ì¥: {len(youtube_results)}ê°œ")
                    
                    # íŒŒì¼ ì €ì¥
                    with open(youtube_raw_path, "w", encoding="utf-8") as f:
                        json.dump(final_data, f, ensure_ascii=False, indent=2)
                        
                except Exception as e:
                    print(f"âš ï¸ YouTube ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨ ì‹œ ê·¸ëƒ¥ ìƒˆ ë°ì´í„°ë§Œ ì €ì¥
                    with open(youtube_raw_path, "w", encoding="utf-8") as f:
                        json.dump(youtube_results, f, ensure_ascii=False, indent=2)
                
            return youtube_results
            
        crawl_tasks.append(("ìœ íŠœë¸Œ", youtube_crawl_task))
    
    # ê²°ê³¼ ë° í†µê³„
    results = {}
    all_results = []
    
    # ì‹œì‘ ì‹œê°„
    start_time = time.time()
    
    # ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨ ì‹¤í–‰
    if args.parallel:
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=min(args.workers, len(crawl_tasks))) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_task = {executor.submit(task_func): task_name for task_name, task_func in crawl_tasks}
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for i, future in enumerate(as_completed(future_to_task)):
                task_name = future_to_task[future]
                try:
                    task_results = future.result()
                    count = len(task_results)
                    results[task_name] = count
                    all_results.extend(task_results)
                except Exception as e:
                    results[task_name] = 0
    else:
        # ìˆœì°¨ ì‹¤í–‰
        for i, (task_name, task_func) in enumerate(crawl_tasks):
            try:
                task_results = task_func()
                count = len(task_results)
                results[task_name] = count
                all_results.extend(task_results)
            except Exception as e:
                results[task_name] = 0
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    elapsed_time = time.time() - start_time
    
    # í’ˆì§ˆ ì„ê³„ê°’ í•„í„°ë§ (content_score -> quality_score ë³€ê²½)
    if args.quality_threshold > 0:
        original_count = len(all_results)
        all_results = [item for item in all_results if item.get("quality_score", 0) >= args.quality_threshold]
        filtered_count = original_count - len(all_results)
    
    # ê²°ê³¼ ë³‘í•© ì €ì¥
    if args.merge and all_results:
        merged_file = f"data/merged/crawl_results_{datetime.now():%Y%m%d_%H%M%S}.json"
        os.makedirs(os.path.dirname(merged_file), exist_ok=True)
        
        with open(merged_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # ì¦ë¶„ í¬ë¡¤ë§ì¸ ê²½ìš°ë§Œ ë°©ë¬¸ URL ì €ì¥
    if args.incremental:
        save_visited_urls(visited_urls)
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\nëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
    logger.info(f"   ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    
    for source, count in results.items():
        logger.info(f"   - {source}: {count}ê°œ í•­ëª©")
    
    total_count = sum(results.values())
    logger.info(f"\n   ì´ {total_count}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ!")
    
    if args.quality_threshold > 0:
        logger.info(f"   í’ˆì§ˆ í•„í„°ë§ í›„ ë‚¨ì€ í•­ëª©: {len(all_results)}ê°œ")
    
    if args.merge:
        logger.info(f"   ë³‘í•© ê²°ê³¼ ì €ì¥: {merged_file}")

if __name__ == "__main__":
    # íŒ¨í‚¤ì§€ ë°–ì—ì„œ python crawler/crawler.pyë¡œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ê²½ë¡œ ë³´ì •
    if __package__ is None and __name__ == "__main__":
        sys.path.append(str(Path(__file__).resolve().parents[1]))
    main()
