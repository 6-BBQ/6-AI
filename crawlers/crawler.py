# crawler.py (ê°„ê²°í•œ ë²„ì „)
from __future__ import annotations
import argparse, sys, textwrap, os, json, time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

from official_crawler import crawl_df
from dc_crawler import crawl_dcinside
from arca_crawler import crawl_arca
from youtube_crawler import crawl_youtube

# ë¡œê¹… ì„¤ì •
log_file = f"logs/crawler_{datetime.now():%Y%m%d_%H%M%S}.log"
os.makedirs(os.path.dirname(log_file), exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file)
    ]
)
logger = logging.getLogger("crawler")

# ë°©ë¬¸í•œ URL ì €ì¥ì†Œ (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)
VISITED_URLS_FILE = "data/visited_urls.json"

def load_visited_urls():
    """ì´ì „ì— ë°©ë¬¸í•œ URL ëª©ë¡ ë¡œë“œ"""
    try:
        if os.path.exists(VISITED_URLS_FILE):
            with open(VISITED_URLS_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        return set()
    except Exception as e:
        logger.error(f"ë°©ë¬¸ URL ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return set()

def save_visited_urls(urls):
    """ë°©ë¬¸í•œ URL ëª©ë¡ ì €ì¥"""
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(VISITED_URLS_FILE), exist_ok=True)
        
        with open(VISITED_URLS_FILE, "w", encoding="utf-8") as f:
            json.dump(list(urls), f, ensure_ascii=False)
    except Exception as e:
        logger.error(f"ë°©ë¬¸ URL ëª©ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")

def run_crawler(crawler_func, *args, **kwargs):
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ í•¨ìˆ˜ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
    start_time = time.time()
    func_name = crawler_func.__name__
    logger.info(f"{func_name} ì‹œì‘")
    
    try:
        result = crawler_func(*args, **kwargs)
        elapsed = time.time() - start_time
        logger.info(f"{func_name} ì™„ë£Œ: {len(result)}ê°œ í•­ëª© ({elapsed:.1f}ì´ˆ)")
        return result
    except Exception as e:
        logger.error(f"{func_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent("""\
            â–¶ ë˜íŒŒ ìŠ¤í™ì—… ê°€ì´ë“œìš© í†µí•© í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
        """)
    )
    parser.add_argument("--pages", type=int, default=10, help="ê° ê²Œì‹œíŒ ìµœëŒ€ í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--depth", type=int, default=2, help="ë³¸ë¬¸ ë§í¬ ì¬ê·€ depth")
    parser.add_argument("--yt-channel", type=str,
                        default="https://www.youtube.com/@zangzidnf/videos",
                        help="YouTube ì±„ë„ URL(@í•¸ë“¤ ë˜ëŠ” /channel/ID)")
    parser.add_argument("--yt-max", type=int, default=20,
                        help="ì±„ë„ì—ì„œ ê°€ì ¸ì˜¬ ìµœì‹  ì˜ìƒ ê°œìˆ˜")
    parser.add_argument("--parallel", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”")
    parser.add_argument("--workers", type=int, default=4, help="ë³‘ë ¬ ì‘ì—…ì ìˆ˜")
    parser.add_argument("--incremental", action="store_true", help="ì¦ë¶„ í¬ë¡¤ë§ í™œì„±í™”")
    parser.add_argument("--clear-history", action="store_true", help="ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™”")
    parser.add_argument("--sources", type=str, default="all", 
                        help="í¬ë¡¤ë§í•  ì†ŒìŠ¤ (ì½¤ë§ˆë¡œ êµ¬ë¶„: official,dc,arca,youtube,all)")
    parser.add_argument("--quality-threshold", type=int, default=0,
                        help="ìµœì†Œ í’ˆì§ˆ ì ìˆ˜ (ì´ ì ìˆ˜ ë¯¸ë§Œì˜ í•­ëª©ì€ ìµœì¢… ê²°ê³¼ì—ì„œ ì œì™¸)")
    parser.add_argument("--merge", action="store_true", help="ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©")

    args = parser.parse_args()
    
    # ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™” ì˜µì…˜
    if args.clear_history:
        try:
            if os.path.exists(VISITED_URLS_FILE):
                os.remove(VISITED_URLS_FILE)
                logger.info("ë°©ë¬¸ URL ê¸°ë¡ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ë°©ë¬¸ URL ê¸°ë¡ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        return

    print(f"\nğŸ”” í†µí•© í¬ë¡¤ë§ ì‹œì‘ ({datetime.now():%Y-%m-%d %H:%M:%S})\n"
          f"   - pages = {args.pages}, depth = {args.depth}\n"
          f"   - yt-channel = {args.yt_channel}, yt-max = {args.yt_max}\n"
          f"   - ë³‘ë ¬ ì²˜ë¦¬ = {args.parallel}, ì‘ì—…ì ìˆ˜ = {args.workers}\n"
          f"   - ì¦ë¶„ í¬ë¡¤ë§ = {args.incremental}")
    
    # ì¦ë¶„ í¬ë¡¤ë§ì„ ìœ„í•œ ë°©ë¬¸ URL ë¡œë“œ
    visited_urls = load_visited_urls() if args.incremental else set()
    logger.info(f"ì´ì „ì— ë°©ë¬¸í•œ URL: {len(visited_urls)}ê°œ")
    
    # í¬ë¡¤ë§í•  ì†ŒìŠ¤ ê²°ì •
    sources = args.sources.lower().split(',')
    crawl_all = "all" in sources
    
    # í¬ë¡¤ë§ ì‘ì—… ì •ì˜
    crawl_tasks = []
    
    if crawl_all or "official" in sources:
        crawl_tasks.append(("ê³µí™ˆ", lambda: run_crawler(crawl_df, args.pages, args.depth, visited_urls)))
    
    if crawl_all or "dc" in sources:
        crawl_tasks.append(("ë””ì‹œ", lambda: run_crawler(crawl_dcinside, args.pages, args.depth, visited_urls)))
    
    if crawl_all or "arca" in sources:
        crawl_tasks.append(("ì•„ì¹´", lambda: run_crawler(crawl_arca, args.pages, args.depth, visited_urls)))
    
    if crawl_all or "youtube" in sources:
        crawl_tasks.append(("ìœ íŠœë¸Œ", lambda: run_crawler(crawl_youtube, args.yt_channel, args.yt_max, visited_urls)))
    
    # ê²°ê³¼ ë° í†µê³„
    results = {}
    all_results = []
    
    # ì‹œì‘ ì‹œê°„
    start_time = time.time()
    
    # ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨ ì‹¤í–‰
    if args.parallel:
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=min(args.workers, len(crawl_tasks))) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_task = {executor.submit(task_func): task_name for task_name, task_func in crawl_tasks}
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for i, future in enumerate(as_completed(future_to_task)):
                task_name = future_to_task[future]
                try:
                    task_results = future.result()
                    count = len(task_results)
                    results[task_name] = count
                    all_results.extend(task_results)
                    logger.info(f"[{i+1}/{len(crawl_tasks)}] {task_name} í¬ë¡¤ë§ ì™„ë£Œ: {count}ê°œ í•­ëª©")
                except Exception as e:
                    logger.error(f"{task_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                    results[task_name] = 0
    else:
        # ìˆœì°¨ ì‹¤í–‰
        for i, (task_name, task_func) in enumerate(crawl_tasks):
            logger.info(f"[{i+1}/{len(crawl_tasks)}] {task_name} í¬ë¡¤ë§ ì‹œì‘")
            try:
                task_results = task_func()
                count = len(task_results)
                results[task_name] = count
                all_results.extend(task_results)
                logger.info(f"[{i+1}/{len(crawl_tasks)}] {task_name} í¬ë¡¤ë§ ì™„ë£Œ: {count}ê°œ í•­ëª©")
            except Exception as e:
                logger.error(f"{task_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                results[task_name] = 0
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    elapsed_time = time.time() - start_time
    
    # í’ˆì§ˆ ì„ê³„ê°’ í•„í„°ë§
    if args.quality_threshold > 0:
        original_count = len(all_results)
        all_results = [item for item in all_results if item.get("content_score", 0) >= args.quality_threshold]
        filtered_count = original_count - len(all_results)
        logger.info(f"í’ˆì§ˆ í•„í„°ë§: {filtered_count}ê°œ í•­ëª© ì œì™¸ (ì„ê³„ê°’: {args.quality_threshold})")
    
    # ê²°ê³¼ ë³‘í•© ì €ì¥
    if args.merge and all_results:
        merged_file = f"data/merged/crawl_results_{datetime.now():%Y%m%d_%H%M%S}.json"
        os.makedirs(os.path.dirname(merged_file), exist_ok=True)
        
        with open(merged_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë³‘í•© ê²°ê³¼ ì €ì¥: {merged_file} ({len(all_results)}ê°œ í•­ëª©)")
    
    # ì¦ë¶„ í¬ë¡¤ë§ì¸ ê²½ìš° ë°©ë¬¸ URL ì €ì¥
    if args.incremental:
        save_visited_urls(visited_urls)
        logger.info(f"ë°©ë¬¸ URL ëª©ë¡ ì €ì¥ ì™„ë£Œ: {len(visited_urls)}ê°œ")
    
    # ê²°ê³¼ ìš”ì•½
    print("\nëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"   ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    
    for source, count in results.items():
        print(f"   - {source}: {count}ê°œ í•­ëª©")
    
    total_count = sum(results.values())
    print(f"\n   ì´ {total_count}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ!")
    
    if args.quality_threshold > 0:
        print(f"   í’ˆì§ˆ í•„í„°ë§ í›„ ë‚¨ì€ í•­ëª©: {len(all_results)}ê°œ")
    
    if args.merge:
        print(f"   ë³‘í•© ê²°ê³¼ ì €ì¥: {merged_file}")

if __name__ == "__main__":
    # íŒ¨í‚¤ì§€ ë°–ì—ì„œ python crawler/crawler.pyë¡œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ê²½ë¡œ ë³´ì •
    if __package__ is None and __name__ == "__main__":
        sys.path.append(str(Path(__file__).resolve().parents[1]))
    main()