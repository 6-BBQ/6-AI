from __future__ import annotations
import argparse, sys, json, textwrap, os
from pathlib import Path
from datetime import datetime

# 1ï¸âƒ£ ê°œë³„ í¬ë¡¤ëŸ¬ import
from official_crawler import crawl_df
from dc_crawler       import crawl_dcinside
from arca_crawler     import crawl_arca
from youtube_crawler  import crawl_youtube

def load_yt_ids(path: str | Path) -> list[str]:
    """í…ìŠ¤íŠ¸ íŒŒì¼(í•œ ì¤„ = video_id) â†’ ë¦¬ìŠ¤íŠ¸"""
    if not path:
        return []
    
    try:
        with open(path, "r", encoding="utf-8") as f:
            ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
            print(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
            return ids
    except Exception as e:
        print(f"âŒ ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
        try:
            project_root = Path(__file__).resolve().parents[1]
            abs_path = project_root / path
            print(f"ğŸ”„ ì ˆëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„: {abs_path}")
            with open(abs_path, "r", encoding="utf-8") as f:
                ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
                print(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
                return ids
        except Exception as e2:
            print(f"âŒ ì ˆëŒ€ ê²½ë¡œ ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
            return []

def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent("""\
            â–¶ ë˜íŒŒ ìŠ¤í™ì—… ê°€ì´ë“œìš© í†µí•© í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
            ì˜ˆì‹œ:
              python -m crawler.crawler --pages 8 --depth 2 \\
                    --yt-list data/youtube_ids.txt
        """)
    )
    parser.add_argument("--pages", type=int, default=2,  help="ê° ê²Œì‹œíŒ ìµœëŒ€ í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--depth", type=int, default=2,  help="ë³¸ë¬¸ ë§í¬ ì¬ê·€ depth")
    parser.add_argument("--yt-list", type=str, default="data/youtube_ids.txt", help="YouTube video_id ë¦¬ìŠ¤íŠ¸ txt ê²½ë¡œ")
    args = parser.parse_args()

    print(f"\nğŸ”” í†µí•© í¬ë¡¤ë§ ì‹œì‘ ({datetime.now():%Y-%m-%d %H:%M:%S})\n"
          f"   - pages = {args.pages}, depth = {args.depth}\n"
          f"   - yt-list = {args.yt_list}")



    # 5ï¸âƒ£ YouTube ìë§‰
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì¶œë ¥
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"ìœ íŠœë¸Œ ID íŒŒì¼ ê²½ë¡œ: {args.yt_list}")
    
    yt_ids = load_yt_ids(args.yt_list)
    if yt_ids:
        print(f"\nğŸŸ¨ [4/4] YouTube ìë§‰ (ID {len(yt_ids)}ê°œ)")
        crawl_youtube(yt_ids)
    else:
        print("\nğŸŸ¨ [4/4] YouTube ìë§‰ â–¶ï¸ (ID ë¦¬ìŠ¤íŠ¸ ì—†ìŒ â†’ ê±´ë„ˆëœ€)")

    print("\nâœ… ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    # íŒ¨í‚¤ì§€ ë°–ì—ì„œ python crawler/crawler.pyë¡œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ê²½ë¡œ ë³´ì •
    if __package__ is None and __name__ == "__main__":
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸(rpgpt/)ë¥¼ PYTHONPATHì— ì¶”ê°€
        sys.path.append(str(Path(__file__).resolve().parents[1]))
    main()