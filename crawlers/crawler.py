from __future__ import annotations

import argparse
import json
import os
import sys
import textwrap
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))
from typing import Callable, Tuple

from config import config
from dc_crawler import crawl_dcinside
from official_crawler import crawl_df
from arca_crawler import crawl_arca
from utils import get_logger

# ì¦ë¶„ í¬ë¡¤ë§ ê¸°ë¡ íŒŒì¼
VISITED_URLS_PATH = config.VISITED_URLS_PATH

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°©ë¬¸ URL ë¡œë“œ / ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_visited_urls() -> set[str]:
    if os.path.exists(VISITED_URLS_PATH):
        try:
            with open(VISITED_URLS_PATH, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            pass
    return set()


def save_visited_urls(urls: set[str]) -> None:
    os.makedirs(Path(VISITED_URLS_PATH).parent, exist_ok=True)
    try:
        with open(VISITED_URLS_PATH, "w", encoding="utf-8") as f:
            json.dump(list(urls), f, ensure_ascii=False)
    except Exception:
        pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ë¡¤ëŸ¬ ì‹¤í–‰ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_crawler(func: Callable, *args, **kwargs) -> list[dict]:
    start = time.time()
    try:
        result = func(*args, **kwargs)
        return result
    except Exception as e:
        print(f"âš ï¸  {func.__name__} ì˜¤ë¥˜: {e}")
        return []
    finally:
        elapsed = time.time() - start
        print(f"â±ï¸  {func.__name__} ì¢…ë£Œ â€” {elapsed:.1f}s")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì§„ì…ì 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    logger = get_logger("crawler")

    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent(
            """
            â–¶ ë˜íŒŒ ìŠ¤í™ì—… ê°€ì´ë“œìš© í†µí•© í¬ë¡¤ëŸ¬ (YouTube ì œì™¸)

            ê¸°ë³¸ = ì¦ë¶„ ëª¨ë“œ. ì „ì²´ ì´ˆê¸°í™”ë¥¼ ì›í•˜ë©´ --full ì‚¬ìš©.
            """
        ),
    )

    # ê³µí†µ ì˜µì…˜ (config ê¸°ë³¸ê°’ í™œìš©)
    parser.add_argument("--pages", type=int, default=config.DEFAULT_CRAWL_PAGES)
    parser.add_argument("--depth", type=int, default=config.DEFAULT_CRAWL_DEPTH)

    # ì‹¤í–‰ ë°©ì‹
    parser.add_argument("--parallel", action="store_true", help="Thread ë³‘ë ¬ ì‹¤í–‰")
    parser.add_argument("--workers", type=int, default=4)

    # ì¦ë¶„ / ì „ì²´ ëª¨ë“œ
    parser.add_argument("--incremental", action="store_true", default=True)
    parser.add_argument("--full", action="store_true")
    parser.add_argument("--clear-history", action="store_true", help="visited_urls.json ì´ˆê¸°í™”")

    # ì†ŒìŠ¤ ì„ íƒ
    parser.add_argument(
        "--sources",
        type=str,
        default="all",
        help="í¬ë¡¤ë§ ì†ŒìŠ¤: official,dc,arca,all (ì½¤ë§ˆêµ¬ë¶„)",
    )

    # í’ˆì§ˆ í•„í„° ë° ë³‘í•©
    parser.add_argument("--quality-threshold", type=int, default=0)
    parser.add_argument("--merge", action="store_true", help="ê²°ê³¼ë¥¼ data/merged/* ì— ì €ì¥")

    args = parser.parse_args()

    # ì „ì²´ ëª¨ë“œ â†’ ì¦ë¶„ í”Œë˜ê·¸ í•´ì œ
    if args.full:
        args.incremental = False

    # ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™”
    if args.clear_history and os.path.exists(VISITED_URLS_PATH):
        os.remove(VISITED_URLS_PATH)
        print("ğŸ—‘ï¸  ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™” ì™„ë£Œ")
        if not args.full:
            return  # ì´ˆê¸°í™”ë§Œ í•˜ê³  ì¢…ë£Œ

    logger.info("ğŸ”” í¬ë¡¤ë§ ì‹œì‘ (%s)", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    logger.info("   pages=%s depth=%s parallel=%s workers=%s", args.pages, args.depth, args.parallel, args.workers)

    visited = load_visited_urls() if args.incremental else set()

    # ì‘ì—… ëª©ë¡ ì‘ì„±
    tasks: list[Tuple[str, Callable[[], list[dict]]]] = []
    sel = args.sources.lower().split(",")
    all_sel = "all" in sel
    if all_sel or "official" in sel:
        tasks.append(("ê³µí™ˆ", lambda: run_crawler(crawl_df, args.pages, args.depth, visited, args.incremental)))
    if all_sel or "dc" in sel:
        tasks.append(("ë””ì‹œ", lambda: run_crawler(crawl_dcinside, args.pages, args.depth, visited, args.incremental)))
    if all_sel or "arca" in sel:
        tasks.append(("ì•„ì¹´", lambda: run_crawler(crawl_arca, args.pages, args.depth, visited, args.incremental)))

    results: dict[str, int] = {}
    all_items: list[dict] = []
    t0 = time.time()

    def collect(name: str, func: Callable[[], list[dict]]):
        items = func()
        results[name] = len(items)
        all_items.extend(items)

    if args.parallel and len(tasks) > 1:
        with ThreadPoolExecutor(max_workers=min(args.workers, len(tasks))) as ex:
            fut_map = {ex.submit(func): name for name, func in tasks}
            for fut in as_completed(fut_map):
                collect(fut_map[fut], lambda f=fut: f.result())
    else:
        for name, func in tasks:
            collect(name, func)

    # í’ˆì§ˆ í•„í„°
    if args.quality_threshold > 0:
        before = len(all_items)
        all_items = [x for x in all_items if x.get("quality_score", 0) >= args.quality_threshold]
        logger.info("í’ˆì§ˆ í•„í„°ë§: %s â†’ %s", before, len(all_items))

    # ê²°ê³¼ ë³‘í•© ì €ì¥
    if args.merge and all_items:
        merged_dir = Path(config.MERGED_DIR)
        merged_dir.mkdir(parents=True, exist_ok=True)
        file_path = merged_dir / f"crawl_results_{datetime.now():%Y%m%d_%H%M%S}.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(all_items, f, ensure_ascii=False, indent=2)
        logger.info("ğŸ’¾ ë³‘í•© ê²°ê³¼ ì €ì¥: %s (%s items)", file_path, len(all_items))

    # ë°©ë¬¸ ê¸°ë¡ ì €ì¥ (ì¦ë¶„)
    if args.incremental:
        save_visited_urls(visited)

    # ìš”ì•½ ì¶œë ¥
    elapsed = time.time() - t0
    logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ â€” %.1fs", elapsed)
    for src, cnt in results.items():
        logger.info("   %s: %s", src, cnt)
    logger.info("   ì´ ìˆ˜ì§‘: %s", sum(results.values()))


if __name__ == "__main__":
    # ëª¨ë“ˆ ê²½ë¡œ ë³´ì •: "python crawler.py" ì‹¤í–‰ ì‹œ
    if __package__ is None and __name__ == "__main__":
        sys.path.append(str(Path(__file__).resolve().parents[1]))
    main()
