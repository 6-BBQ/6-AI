# youtube_crawler.py ìˆ˜ì •ì•ˆ - í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ + append ëª¨ë“œ
from __future__ import annotations
from datetime import datetime, timezone
import json, time, sys, urllib.parse, os
from pathlib import Path

import yt_dlp                          # pip install yt-dlp
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    TranscriptsDisabled,
    NoTranscriptFound,
    CouldNotRetrieveTranscript
)

from utils import build_item           # quality_scoreÂ·class_name ìë™ ë¶€ì—¬

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAVE_PATH = Path("data/raw/youtube_raw.json")

# yt-dlp ê¸°ë³¸ ì˜µì…˜
YDL_FLAT_OPTS = {
    "quiet": True,
    "skip_download": True,
    "extract_flat": True,      # ì˜ìƒ IDÂ·ì œëª©ë§Œ
    "nocheckcertificate": True,
}
YDL_META_OPTS = {
    "quiet": True,
    "skip_download": True,
    "nocheckcertificate": True,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_youtube_videos(query, max_results=20):
    """
    YouTube ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì˜ìƒ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
    Returns:
        list[str]: ì˜ìƒ ID ëª©ë¡
    """
    # ê²€ìƒ‰ URL ì¸ì½”ë”©
    search_url = f"ytsearch{max_results}:{query}"
    
    try:
        with yt_dlp.YoutubeDL(YDL_FLAT_OPTS) as ydl:
            info = ydl.extract_info(search_url, download=False)
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì˜ìƒ ID ì¶”ì¶œ
            entries = info.get("entries", [])
            video_ids = []

            for entry in entries:
                # ì˜ìƒ IDê°€ ìˆì„ ê²½ìš° ì¶”ì¶œ
                vid = entry.get("id") or ""
                if len(vid) == 11:  # ìœ íŠœë¸Œ video_idëŠ” í•­ìƒ 11ìë¦¬
                    video_ids.append(vid)

            print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: '{query}'ì—ì„œ {len(video_ids)}ê°œ ì˜ìƒ ID ìˆ˜ì§‘")
            return video_ids
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def list_channel_video_ids(channel_url: str, limit: int) -> list[str]:
    """
    yt-dlp ë¡œ ì±„ë„ â†’ ì˜ìƒ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ limit ê°œ)
    """
    try:
        print(f"ğŸ“º ì±„ë„ ì²˜ë¦¬ ì¤‘: {channel_url}")
        
        # ì±„ë„ URL ì •ê·œí™” (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
        if "@" in channel_url and "/videos" not in channel_url:
            channel_url = f"{channel_url}/videos"
        
        with yt_dlp.YoutubeDL(YDL_FLAT_OPTS) as ydl:
            info = ydl.extract_info(channel_url, download=False)
            
            # ì±„ë„ì´ë©´ entries ì•ˆì— ë‹¤ì‹œ video entriesê°€ ìˆëŠ” ê²½ìš°
            entries = info.get("entries", [])
            video_ids = []

            for entry in entries[:limit]:
                # ì˜ìƒ IDê°€ ìˆì„ ê²½ìš° ì¶”ì¶œ
                vid = entry.get("id") or ""
                if len(vid) == 11:  # ìœ íŠœë¸Œ video_idëŠ” í•­ìƒ 11ìë¦¬
                    video_ids.append(vid)

            print(f"ğŸ“º ì±„ë„ ê²°ê³¼: {len(video_ids)}ê°œ ì˜ìƒ ID ìˆ˜ì§‘")
            return video_ids
    except Exception as e:
        print(f"âŒ ì±„ë„ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

def fetch_video_meta(video_id: str) -> dict:
    """yt-dlp ë¡œ ì˜ìƒ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
    url = f"https://www.youtube.com/watch?v={video_id}"
    try:
        with yt_dlp.YoutubeDL(YDL_META_OPTS) as ydl:
            info = ydl.extract_info(url, download=False)
        # upload_date: 'YYYYMMDD'
        date_obj = datetime.strptime(info["upload_date"], "%Y%m%d").replace(tzinfo=timezone.utc)
        return {
            "title": info["title"],
            "date": date_obj,
            "views": info.get("view_count", 0) or 0,
            "likes": info.get("like_count", 0) or 0,
        }
    except Exception as e:
        return {
            "title": f"YOUTUBE_{video_id}",
            "date": datetime.now(timezone.utc),
            "views": 0,
            "likes": 0,
        }

def fetch_caption_text(video_id: str) -> str:
    """ìë§‰ ê°€ì ¸ì˜¤ê¸°"""
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=("ko", "en"))
        return " ".join(seg["text"].strip() for seg in transcript if seg["text"].strip())
    except (TranscriptsDisabled, NoTranscriptFound, CouldNotRetrieveTranscript) as e:
        return ""
    except Exception as e:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_youtube_search(search_query: str, max_videos: int, visited_urls=None) -> list[dict]:
    """
    YouTube ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§
    
    Args:
        search_query: ê²€ìƒ‰ ì¿¼ë¦¬
        max_videos: ê°€ì ¸ì˜¬ ìµœëŒ€ ì˜ìƒ ìˆ˜
        visited_urls: ì´ë¯¸ ë°©ë¬¸í•œ URL ì§‘í•© (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)
    """
    # ì¦ë¶„ í¬ë¡¤ë§ì„ ìœ„í•œ ë°©ë¬¸ URL ê´€ë¦¬
    if visited_urls is None:
        visited_urls = set()
    
    # ê²€ìƒ‰ ê²°ê³¼ ì˜ìƒ ID ê°€ì ¸ì˜¤ê¸°
    video_ids = search_youtube_videos(search_query, max_videos)
    
    return process_video_ids(video_ids, visited_urls, "ê²€ìƒ‰")

def crawl_youtube_channel(channel_url: str, max_videos: int, visited_urls=None) -> list[dict]:
    """
    YouTube ì±„ë„ì˜ ì˜ìƒ í¬ë¡¤ë§
    
    Args:
        channel_url: YouTube ì±„ë„ URL
        max_videos: ê°€ì ¸ì˜¬ ìµœëŒ€ ì˜ìƒ ìˆ˜
        visited_urls: ì´ë¯¸ ë°©ë¬¸í•œ URL ì§‘í•© (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)
    """
    # ì¦ë¶„ í¬ë¡¤ë§ì„ ìœ„í•œ ë°©ë¬¸ URL ê´€ë¦¬
    if visited_urls is None:
        visited_urls = set()
    
    # ì˜ìƒ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    video_ids = list_channel_video_ids(channel_url, max_videos)
    
    return process_video_ids(video_ids, visited_urls, "ì±„ë„")

def process_video_ids(video_ids, visited_urls, source_name="youtube"):
    """
    ì˜ìƒ ID ëª©ë¡ì„ ì²˜ë¦¬í•˜ì—¬ ìë§‰ì´ ìˆëŠ” ì˜ìƒë§Œ ë°˜í™˜ (íŒŒì¼ ì €ì¥ ì œê±°)
    
    Args:
        video_ids: ì˜ìƒ ID ëª©ë¡
        visited_urls: ë°©ë¬¸í•œ URL ì§‘í•©
        source_name: ì†ŒìŠ¤ ì´ë¦„ (ë¡œê·¸ìš©)
    
    Returns:
        list[dict]: ì²˜ë¦¬ëœ ì˜ìƒ ë°ì´í„° ëª©ë¡
    """
    results = []
    start_time = time.time()
    print(f"ğŸ”„ {source_name} ì˜ìƒ ì²˜ë¦¬ ì‹œì‘: {len(video_ids)}ê°œ ì˜ìƒ")

    for i, vid in enumerate(video_ids, 1):
        url = f"https://www.youtube.com/watch?v={vid}"
        
        # ì¦ë¶„ í¬ë¡¤ë§: ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ê±´ë„ˆë›°ê¸°
        if url in visited_urls:
            print(f"   â­ï¸ {i}/{len(video_ids)}: ì´ë¯¸ ë°©ë¬¸í•œ URL ê±´ë„ˆë›°ê¸°")
            continue
            
        # ë°©ë¬¸ ê¸°ë¡ì— ì¶”ê°€
        visited_urls.add(url)

        # 1) ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        print(f"   ğŸ“º {i}/{len(video_ids)}: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        meta = fetch_video_meta(vid)

        # 2) ìë§‰ ê°€ì ¸ì˜¤ê¸°
        caption = fetch_caption_text(vid)
        if not caption:
            print(f"   âš ï¸ {i}/{len(video_ids)}: ìë§‰ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
            continue    # ìë§‰ ì—†ëŠ” ì˜ìƒì€ ë‰´ë¹„ ê°€ì´ë“œë¡œ ì“°ê¸° ì–´ë µë‹¤ íŒë‹¨

        # 3) utils.build_item â†’ quality_score / class_name ìë™ ë¶€ì—¬
        item = build_item(
            source="youtube",
            url=url,
            title=meta["title"],
            body=caption,
            date=meta["date"],
            views=meta["views"],
            likes=meta["likes"],
        )
        results.append(item)
        print(f"   âœ… {i}/{len(video_ids)}: '{meta['title'][:50]}...' ìˆ˜ì§‘ ì™„ë£Œ")

        # API/ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€
        time.sleep(2.0)

    # ê²°ê³¼ ìš”ì•½
    elapsed_time = time.time() - start_time
    print(f"âœ… {source_name} ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘ ({elapsed_time:.1f}ì´ˆ ì†Œìš”)")
    
    return results

def save_results_append(results: list[dict], source_name: str):
    """
    ê²°ê³¼ë¥¼ append ëª¨ë“œë¡œ ì €ì¥ (ë®ì–´ì“°ê¸° ë°©ì§€)
    """
    if not results:
        return
    
    save_dir = Path(SAVE_PATH).parent
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ
    existing_data = []
    if SAVE_PATH.exists():
        try:
            with open(SAVE_PATH, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            existing_data = []
    
    # ìƒˆ ë°ì´í„° ì¶”ê°€
    existing_data.extend(results)
    
    # ì „ì²´ ë°ì´í„° ì €ì¥
    with open(SAVE_PATH, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ {source_name} ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(results)}ê°œ ì¶”ê°€, ì´ {len(existing_data)}ê°œ")

# ì´ì „ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜
def crawl_youtube(source: str, max_videos: int, visited_urls=None) -> list[dict]:
    """
    YouTube í¬ë¡¤ë§ (ì±„ë„ URL ë˜ëŠ” ê²€ìƒ‰ì–´)
    
    Args:
        source: ì±„ë„ URL ë˜ëŠ” ê²€ìƒ‰ì–´
        max_videos: ê°€ì ¸ì˜¬ ìµœëŒ€ ì˜ìƒ ìˆ˜
        visited_urls: ì´ë¯¸ ë°©ë¬¸í•œ URL ì§‘í•© (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)
    """
    # sourceê°€ URLì¸ì§€ ê²€ìƒ‰ì–´ì¸ì§€ íŒë‹¨
    if source.startswith(("http://", "https://", "www.")):
        # URL - ì±„ë„ í¬ë¡¤ë§
        return crawl_youtube_channel(source, max_videos, visited_urls)
    else:
        # ê²€ìƒ‰ì–´ - ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§
        return crawl_youtube_search(source, max_videos, visited_urls)

# ì§ì ‘ ì‹¤í–‰ ì‹œ - í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (ê²€ìƒ‰ + ì‹ ë¢° ì±„ë„)
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="YouTube ë˜íŒŒ í¬ë¡¤ëŸ¬ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)")
    parser.add_argument("--incremental", action="store_true", default=True, help="ì¦ë¶„ í¬ë¡¤ë§ (ê¸°ë³¸ê°’)")
    parser.add_argument("--full", action="store_true", help="ì „ì²´ í¬ë¡¤ë§ (ì¦ë¶„ ë¬´ì‹œ)")
    parser.add_argument("--max-videos", type=int, default=3, help="ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ì˜ìƒ ìˆ˜")
    parser.add_argument("--search-query", type=str, default="ë˜íŒŒ ê°€ì´ë“œ", help="ê²€ìƒ‰ ì¿¼ë¦¬")
    args = parser.parse_args()
    
    print("ğŸ† YouTube í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì‹œì‘!")
    print("1ï¸âƒ£ ê²€ìƒ‰ ê¸°ë°˜: ë‹¤ì–‘í•œ ì±„ë„ì˜ ì½˜í…ì¸  ìˆ˜ì§‘")
    print("2ï¸âƒ£ ì‹ ë¢° ì±„ë„: ê³ í’ˆì§ˆ ì½˜í…ì¸  ë³´ì¥")
    print(f"ğŸ“‹ ì„¤ì •: ì¦ë¶„={args.incremental}, ìµœëŒ€ì˜ìƒ={args.max_videos}, ê²€ìƒ‰ì–´='{args.search_query}'")
    print()
    
    # ì „ì²´ ëª¨ë“œ ê²€ì‚¬
    if args.full:
        args.incremental = False
    
    # ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›
    visited_urls = set()
    if args.incremental:
        try:
            # visited_urls.json ë¡œë“œ
            sys.path.append(str(Path(__file__).parent))
            from crawler import load_visited_urls, save_visited_urls
            visited_urls = load_visited_urls()
            print(f"ğŸ”„ ê¸°ì¡´ ë°©ë¬¸ URL {len(visited_urls)}ê°œ ë¡œë“œë¨")
        except ImportError:
            print("âš ï¸ crawler.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤")
    else:
        # ë¹„ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ íŒŒì¼ ì´ˆê¸°í™”
        if SAVE_PATH.exists():
            print("ğŸ—‘ï¸ ê¸°ì¡´ YouTube í¬ë¡¤ë§ ê²°ê³¼ ì´ˆê¸°í™”")
            os.remove(SAVE_PATH)
    
    all_results = []
    
    # 1ï¸âƒ£ ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ë§ (ë‹¤ì–‘ì„± í™•ë³´)
    print(f"ğŸ” ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ë§: '{args.search_query}' (ìµœëŒ€ {args.max_videos}ê°œ)")
    try:
        search_results = crawl_youtube_search(args.search_query, args.max_videos, visited_urls)
        all_results.extend(search_results)
        save_results_append(search_results, "ê²€ìƒ‰")
        print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ìˆ˜ì§‘")
    except Exception as e:
        print(f"   âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    # 2ï¸âƒ£ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì±„ë„ë“¤ (í’ˆì§ˆ ë³´ì¥)
    trusted_channels = [
        "https://www.youtube.com/@zangzidnf",  # ë˜íŒŒ ê´€ë ¨ ì±„ë„
        # "ë‹¤ë¥¸ ì‹ ë¢° ì±„ë„ URLë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€"
    ]
    
    for i, channel_url in enumerate(trusted_channels, 1):
        print(f"ğŸ¥ ì‹ ë¢° ì±„ë„ {i}: {channel_url.split('/')[-1]} (ìµœëŒ€ {args.max_videos}ê°œ)")
        try:
            channel_results = crawl_youtube_channel(channel_url, args.max_videos, visited_urls)
            all_results.extend(channel_results)
            save_results_append(channel_results, f"ì±„ë„{i}")
            print(f"   âœ… ì±„ë„ {i} ê²°ê³¼: {len(channel_results)}ê°œ ìˆ˜ì§‘")
        except Exception as e:
            print(f"   âš ï¸ ì±„ë„ {i} ì˜¤ë¥˜: {e}")
    
    # ì¦ë¶„ í¬ë¡¤ë§ì¸ ê²½ìš° visited_urls ì €ì¥
    if args.incremental:
        try:
            save_visited_urls(visited_urls)
            print(f"ğŸ’¾ ë°©ë¬¸ URL {len(visited_urls)}ê°œ ì €ì¥ë¨")
        except NameError:
            print("âš ï¸ visited_urls ì €ì¥ ì‹¤íŒ¨")
    
    # ìµœì¢… ê²°ê³¼
    print()
    print(f"ğŸ† í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ì˜ìƒ ìˆ˜ì§‘ (ìë§‰ ìˆëŠ” ì˜ìƒë§Œ)")
    print(f"ğŸ”„ ë°©ë¬¸í•œ URL: {len(visited_urls)}ê°œ (ì¦ë¶„: {args.incremental})")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {SAVE_PATH}")
    print(f"âœ… YouTube í¬ë¡¤ë§ ì™„ë£Œ: {len(all_results)}ê°œ ìˆ˜ì§‘")
