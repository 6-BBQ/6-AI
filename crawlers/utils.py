# utils.py ìˆ˜ì •ì•ˆ
import math, re, json, logging
from datetime import datetime, timezone
from pathlib import Path
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("crawler")

# 1. ì§ì—…ëª…/ê°ì„±ëª… ì‚¬ì „ ë¡œë“œ  (job_names.json ì€ ê°™ì€ í´ë”ì— ë‘ì„¸ìš”)
_JOB_JSON = Path(__file__).with_name("job_names.json")
if _JOB_JSON.exists():
    _JOB_SET: set[str] = set(json.loads(_JOB_JSON.read_text(encoding="utf-8")))
else:  # í˜¹ì‹œ íŒŒì¼ ì—†ì„ ë•Œ ìµœì†Œ ì§ì—… ì„¸íŠ¸
    _JOB_SET = {}
_JOB_PATTERN = re.compile("|".join(map(re.escape, sorted(_JOB_SET, key=len, reverse=True))))

def detect_class_name(title: str, body: str | None = None) -> str | None:
    """ì œëª©/ë³¸ë¬¸ì—ì„œ ì§ì—…Â·ê°ì„±ëª…ì´ ë³´ì´ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ None"""
    text = f"{title} {body or ''}"
    m = _JOB_PATTERN.search(text)
    return m.group(0) if m else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (HTML íƒœê·¸ ì œê±°, ì—°ì† ê³µë°± ì œê±° ë“±)"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    if '<' in text and '>' in text:  # HTMLë¡œ ë³´ì´ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
        text = BeautifulSoup(text, "html.parser").get_text(separator=" ")
    
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì¤„ë°”ê¿ˆ í‘œì¤€í™”
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    return text.strip()

def calculate_content_score(text, title=""):
    """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
    # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ 0ì 
    if not text:
        return 0
    
    score = 0
    
    # 1. ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 40ì )
    length = len(text)
    if length < 100:
        length_score = length / 5  # ìµœëŒ€ 20ì 
    elif length < 500:
        length_score = 20 + (length - 100) / 20  # 20-40ì 
    else:
        length_score = 40
    
    # 2. êµ¬ì¡° ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 30ì )
    # ì¤„ë°”ê¿ˆ ìˆ˜ (ë¬¸ë‹¨ êµ¬ë¶„ì´ ì˜ ëœ í…ìŠ¤íŠ¸ëŠ” ì ìˆ˜ ë†’ìŒ)
    newlines = text.count('\n')
    paragraphs = max(1, len([p for p in text.split('\n') if p.strip()]))
    structure_score = min(30, paragraphs + newlines/2)
    
    # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 30ì )
    keywords = ["ìŠ¤í™ì—…", "ê°€ì´ë“œ", "ê³µëµ", "ì¶”ì²œ", "íŒ", "ë…¸í•˜ìš°", "ì¥ë¹„", "ìŠ¤í‚¬", "ì¢…ë§ì˜ ìˆ­ë°°ì",
                "ìƒê¸‰ ë˜ì „", "ë ˆì´ë“œ", "ì—í”½", "í™œìš©", "ì¤‘ì²œ", "ìœµí•©ì„", "ë‰´ë¹„", "ë ˆê¸°ì˜¨"]
    
    combined = (title + " " + text).lower()
    keyword_count = sum(1 for kw in keywords if kw.lower() in combined)
    keyword_score = min(30, keyword_count * 3)
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    score = length_score + structure_score + keyword_score
    
    return min(100, score)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš°ì„ ìˆœìœ„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SRC_WEIGHT = {"official":1.0, "youtube":0.9, "arca":0.75, "dcinside":0.7}

def _freshness_w(date: datetime, today: datetime) -> float:
    return max(0.0, 1 - (today - date).days/90)      # 90ì¼ ë‚´ : 0~1

def _engage_w(views: int, likes: int) -> float:
    import math
    v = min(1.0, math.log1p(views)    / 10)
    l = min(1.0, math.log1p(likes)    /  7)
    return (v*0.7)+(l*0.3)             # 0~1

def calc_priority(
    *, source: str, date: datetime,
    views: int = 0, likes: int = 0,
    class_name: str | None = None,
    today: datetime | None = None,
    content_score: float = 0.0
) -> float:
    today = today or datetime.now(timezone.utc)
    
    # ì½˜í…ì¸  ì ìˆ˜ ë°˜ì˜ (0~1 ì‚¬ì´ë¡œ ì •ê·œí™”)
    content_weight = min(1.0, content_score / 100)
    
    return round(
        (_SRC_WEIGHT.get(source,0.5) * 4) +
        (_engage_w(views,likes) * 3) +
        (_freshness_w(date,today) * 1.5) +
        ((1.0 if class_name is None else 0.3) * 0.5) +
        (content_weight * 2),  # ì½˜í…ì¸  ì ìˆ˜ ê°€ì¤‘ì¹˜ ë°˜ì˜
        2
    )

def load_yt_ids(path: str | Path) -> list[str]:
    if not path:
        return []
    
    try:
        with open(path, "r", encoding="utf-8") as f:
            ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
            logger.info(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
            return ids
    except Exception as e:
        logger.error(f"âŒ ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
        try:
            project_root = Path(__file__).resolve().parents[1]
            abs_path = project_root / path
            logger.info(f"ğŸ”„ ì ˆëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„: {abs_path}")
            with open(abs_path, "r", encoding="utf-8") as f:
                ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
                logger.info(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
                return ids
        except Exception as e2:
            logger.error(f"âŒ ì ˆëŒ€ ê²½ë¡œ ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
            return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ dict ë¹Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_item(
    *, source: str, url: str, title: str, body: str,
    date: str, views: int = 0, likes: int = 0
) -> dict:
    
    # 1) ë‚ ì§œ ë¬¸ìì—´ â†’ datetime
    if isinstance(date, str):
        try:
            # "YYYY-MM-DD" ë˜ëŠ” "YYYY.MM.DD" ëª¨ë‘ ëŒ€ì‘
            clean = date.replace(".", "-")[:10]
            date_obj = datetime.strptime(clean, "%Y-%m-%d")
        except ValueError:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ëŒ€ì²´
            date_obj = datetime.now()
    else:
        date_obj = date

    # 2) tzinfo ì—†ìœ¼ë©´ UTC ë¡œ ë¶€ì—¬
    if isinstance(date_obj, datetime) and date_obj.tzinfo is None:
        date_obj = date_obj.replace(tzinfo=timezone.utc)

    # 3) ë³¸ë¬¸ ì •ë¦¬
    clean_body = clean_text(body)
    
    # 4) ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    content_score = calculate_content_score(clean_body, title)

    # 5) ì§ì—… ì´ë¦„ ê°ì§€
    cls = detect_class_name(title, clean_body)
    
    # 6) ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
    score = calc_priority(
        source=source, date=date_obj,
        views=views, likes=likes,
        class_name=cls,
        content_score=content_score
    )
    
    return {
        "url": url,
        "title": title,
        "date": date_obj.strftime("%Y-%m-%d"),
        "views": views, 
        "likes": likes,
        "class_name": cls,
        "source": source,
        "priority_score": score,
        "content_score": content_score,
        "body": clean_body,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•„í„°ë§ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_process_url(url, visited_urls=None):
    """URL ì²˜ë¦¬ ì—¬ë¶€ ê²°ì • (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)"""
    if visited_urls is None:
        return True
    
    # ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ê±´ë„ˆë›°ê¸°
    if url in visited_urls:
        return False
    
    return True

def filter_by_keywords(text, include_keywords, exclude_keywords):
    """í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§"""
    if not text:
        return False
    
    # ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
    for keyword in exclude_keywords:
        if keyword in text:
            return False
    
    # í¬í•¨ í‚¤ì›Œë“œ í™•ì¸
    for keyword in include_keywords:
        if keyword in text:
            return True
    
    return False