# utils.py ìˆ˜ì •ì•ˆ
import math, re, json, logging
from datetime import datetime, timezone
from pathlib import Path
from bs4 import BeautifulSoup
from typing import Optional
from rapidfuzz import process, fuzz

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("crawler")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (HTML íƒœê·¸ ì œê±°, ì—°ì† ê³µë°± ì œê±° ë“±)"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    if '<' in text and '>' in text:  # HTMLë¡œ ë³´ì´ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
        text = BeautifulSoup(text, "html.parser").get_text(separator=" ")
    
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì¤„ë°”ê¿ˆ í‘œì¤€í™”
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    return text.strip()

def calculate_content_score(text, title=""):
    """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
    # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ 0ì 
    if not text:
        return 0
    
    score = 0
    
    # 1. ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 40ì )
    length = len(text)
    if length < 100:
        length_score = length / 5  # ìµœëŒ€ 20ì 
    elif length < 500:
        length_score = 20 + (length - 100) / 20  # 20-40ì 
    else:
        length_score = 40
    
    # 2. êµ¬ì¡° ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 30ì )
    # ì¤„ë°”ê¿ˆ ìˆ˜ (ë¬¸ë‹¨ êµ¬ë¶„ì´ ì˜ ëœ í…ìŠ¤íŠ¸ëŠ” ì ìˆ˜ ë†’ìŒ)
    newlines = text.count('\n')
    paragraphs = max(1, len([p for p in text.split('\n') if p.strip()]))
    structure_score = min(30, paragraphs + newlines/2)
    
    # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 30ì )
    keywords = ["ìŠ¤í™ì—…", "ê°€ì´ë“œ", "ê³µëµ", "ì¶”ì²œ", "íŒ", "ë…¸í•˜ìš°", "ì¥ë¹„", "ìŠ¤í‚¬", "ì¢…ë§ì˜ ìˆ­ë°°ì",
                "ìƒê¸‰ ë˜ì „", "ë ˆì´ë“œ", "ì—í”½", "í™œìš©", "ì¤‘ì²œ", "ìœµí•©ì„", "ë‰´ë¹„", "ë ˆê¸°ì˜¨"]
    
    combined = (title + " " + text).lower()
    keyword_count = sum(1 for kw in keywords if kw.lower() in combined)
    keyword_score = min(30, keyword_count * 3)
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    score = length_score + structure_score + keyword_score
    
    return min(100, score)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´íŠ¸ë³„ ì •ê·œí™” ê¸°ì¤€ (í˜„ì‹¤ì  ìˆ˜ì¹˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SITE_NORMALIZATION = {
    "youtube": {
        "views_base": 15_000,
        "likes_base": 400,
        "likes_ratio_range": (0.01, 0.04)
    },

    # ì•„ì¹´ë¼ì´ë¸Œ
    "arca": {
        "views_base": 5_000,
        "likes_base": 20,
        "likes_ratio_range": (0.003, 0.015)
    },

    # ë””ì‹œì¸ì‚¬ì´ë“œ
    "dcinside": {
        "views_base": 15_000,
        "likes_base": 30,
        "likes_ratio_range": (0.0015, 0.008)
    },

    # ê³µì‹ í™ˆí˜ì´ì§€
    "official": {
        "views_base": 120_000,
        "likes_base": 50,
        "likes_ratio_range": (0.0002, 0.002)
    }
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš°ì„ ìˆœìœ„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SRC_WEIGHT = {"official":1.0, "youtube":0.95, "arca":0.9, "dcinside":0.9}

def _freshness_w(date: datetime, today: datetime) -> float:
    return max(0.0, 1 - (today - date).days/90)

def _engage_w(views: int, likes: int, source: str) -> float:
    """ì‚¬ì´íŠ¸ë³„ ì •ê·œí™”ëœ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° (í˜„ì‹¤ì  ë¹„ìœ¨ ë°˜ì˜)"""
    import math
    
    # ì‚¬ì´íŠ¸ë³„ ì •ê·œí™” ê¸°ì¤€ ê°€ì ¸ì˜¤ê¸°
    norm = SITE_NORMALIZATION.get(source, {"views_base": 15000, "likes_base": 30})
    views_base = norm["views_base"]
    likes_base = norm["likes_base"]
    
    # í˜„ì‹¤ì ì¸ ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜ ë¹„ìœ¨ ì²´í¬
    if views > 0:
        actual_ratio = likes / views
        expected_range = norm.get("likes_ratio_range", (0.001, 0.010))
        
        # ë¹„ìœ¨ì´ í˜„ì‹¤ì  ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ í˜ë„í‹° ì ìš©
        ratio_penalty = 1.0
        if actual_ratio > expected_range[1] * 2:  # ë„ˆë¬´ ë†’ì€ ë¹„ìœ¨
            ratio_penalty = 0.8
        elif actual_ratio < expected_range[0] * 0.5:  # ë„ˆë¬´ ë‚®ì€ ë¹„ìœ¨
            ratio_penalty = 0.9
    else:
        ratio_penalty = 1.0
    
    # ì‚¬ì´íŠ¸ë³„ ìƒëŒ€ì  ì¸ê¸°ë„ë¡œ ì •ê·œí™”
    normalized_views = views / views_base
    normalized_likes = likes / likes_base
    
    # ë¡œê·¸ ì •ê·œí™” ì ìš© (í˜„ì‹¤ì  ë²”ìœ„ ê³ ë ¤)
    v = min(1.0, math.log1p(normalized_views) / 3.0)
    l = min(1.0, math.log1p(normalized_likes) / 2.5)
    
    # ë¹„ìœ¨ í˜ë„í‹° ì ìš©
    engagement = ((v * 0.6) + (l * 0.4)) * ratio_penalty
    
    return min(1.0, engagement)

def calc_quality_score(
    *, source: str, date: datetime,
    views: int = 0, likes: int = 0,
    today: datetime | None = None,
    content_score: float = 0.0
) -> float:
    """ì½˜í…ì¸  í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì§ì—… ì ìˆ˜ ì œì™¸, í†µí•© ì ìˆ˜)"""
    today = today or datetime.now(timezone.utc)
    
    # ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ë°˜ì˜ (0~1 ì‚¬ì´ë¡œ ì •ê·œí™”)
    content_weight = min(1.0, content_score / 100)
    
    # ì‚¬ì´íŠ¸ë³„ ì •ê·œí™”ëœ ì¸ê¸°ë„ ê³„ì‚°
    engagement_score = _engage_w(views, likes, source)
    
    # í†µí•© ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 10ì  ì •ë„)
    total_score = (
        (_SRC_WEIGHT.get(source, 0.5) * 2.5) +       # ì‚¬ì´íŠ¸ ì˜í–¥ ì•½í™”
        (engagement_score * 2.0) +                  # ì¸ê¸°ë„ ì˜í–¥ ì™„í™”
        (_freshness_w(date, today) * 2.0) +         
        (content_weight * 3.5)                      # ì½˜í…ì¸  í’ˆì§ˆ ê°•í™”
    )
    
    return round(total_score, 2)

def load_yt_ids(path: str | Path) -> list[str]:
    if not path:
        return []
    
    try:
        with open(path, "r", encoding="utf-8") as f:
            ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
            logger.info(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
            return ids
    except Exception as e:
        logger.error(f"âŒ ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
        try:
            project_root = Path(__file__).resolve().parents[1]
            abs_path = project_root / path
            logger.info(f"ğŸ”„ ì ˆëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„: {abs_path}")
            with open(abs_path, "r", encoding="utf-8") as f:
                ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
                logger.info(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
                return ids
        except Exception as e2:
            logger.error(f"âŒ ì ˆëŒ€ ê²½ë¡œ ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
            return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ë¡¤ëŸ¬ë³„ ì €ì¥ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_official_data(data: list, append: bool = True):
    """ê³µì‹ ì‚¬ì´íŠ¸ ë°ì´í„° ì €ì¥"""
    save_crawler_data("data/raw/official_raw.json", data, append)

def save_dc_data(data: list, append: bool = True):
    """ë””ì‹œì¸ì‚¬ì´ë“œ ë°ì´í„° ì €ì¥"""
    save_crawler_data("data/raw/dc_raw.json", data, append)

def save_arca_data(data: list, append: bool = True):
    """ì•„ì¹´ë¼ì´ë¸Œ ë°ì´í„° ì €ì¥"""
    save_crawler_data("data/raw/arca_raw.json", data, append)

def save_youtube_data(data: list, append: bool = True):
    """YouTube ë°ì´í„° ì €ì¥"""
    save_crawler_data("data/raw/youtube_raw.json", data, append)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ dict ë¹Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_item(
    *, source: str, url: str, title: str, body: str,
    date: str, views: int = 0, likes: int = 0
) -> dict:
    
    # 1) ë‚ ì§œ ë¬¸ìì—´ â†’ datetime
    if isinstance(date, str):
        try:
            # "YYYY-MM-DD" ë˜ëŠ” "YYYY.MM.DD" ëª¨ë‘ ëŒ€ì‘
            clean = date.replace(".", "-")[:10]
            date_obj = datetime.strptime(clean, "%Y-%m-%d")
        except ValueError:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ëŒ€ì²´
            date_obj = datetime.now()
    else:
        date_obj = date

    # 2) tzinfo ì—†ìœ¼ë©´ UTC ë¡œ ë¶€ì—¬
    if isinstance(date_obj, datetime) and date_obj.tzinfo is None:
        date_obj = date_obj.replace(tzinfo=timezone.utc)

    # 3) ë³¸ë¬¸ ì •ë¦¬
    clean_body = clean_text(body)
    
    # 4) ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    content_score = calculate_content_score(clean_body, title)
    
    # 6) í†µí•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì§ì—… ì ìˆ˜ ì œì™¸)
    quality_score = calc_quality_score(
        source=source, date=date_obj,
        views=views, likes=likes,
        content_score=content_score
    )
    
    return {
        "url": url,
        "title": title,
        "date": date_obj.strftime("%Y-%m-%d"),
        "views": views, 
        "likes": likes,
        "class_name": None,  # í›„ì²˜ë¦¬ì—ì„œ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜ ì˜ˆì •
        "source": source,
        "quality_score": quality_score,  # í†µí•©ëœ ë‹¨ì¼ ì ìˆ˜
        "body": clean_body,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•„í„°ë§ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_process_url(url, visited_urls=None):
    """URL ì²˜ë¦¬ ì—¬ë¶€ ê²°ì • (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)"""
    if visited_urls is None:
        return True
    
    # ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ê±´ë„ˆë›°ê¸°
    if url in visited_urls:
        return False
    
    return True

def filter_by_keywords(text, include_keywords, exclude_keywords):
    """í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§"""
    if not text:
        return False
    
    # ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
    for keyword in exclude_keywords:
        if keyword in text:
            return False
    
    # í¬í•¨ í‚¤ì›Œë“œ í™•ì¸
    for keyword in include_keywords:
        if keyword in text:
            return True
    
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¦ë¶„ ì €ì¥ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_crawler_data(file_path: str, data: list, append: bool = True):
    """í¬ë¡¤ë§ ë°ì´í„° ì¦ë¶„ ì €ì¥ í•¨ìˆ˜"""
    import os
    import json
    from pathlib import Path
    
    if not data:
        logger.info(f"ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŒ: {file_path}")
        return
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    
    try:
        if append and os.path.exists(file_path):
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            with open(file_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
            
            # URL ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê¸°ì¡´ URL ì§‘í•©
            existing_urls = {item.get('url') for item in existing_data if isinstance(item, dict) and 'url' in item}
            
            # ìƒˆë¡œìš´ ë°ì´í„° ì¤‘ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ì¶”ê°€
            new_data = [item for item in data if item.get('url') not in existing_urls]
            
            if new_data:
                final_data = existing_data + new_data
                logger.info(f"ì¦ë¶„ ì €ì¥: ê¸°ì¡´ {len(existing_data)}ê°œ + ìƒˆë¡œìš´ {len(new_data)}ê°œ = ì´ {len(final_data)}ê°œ")
            else:
                final_data = existing_data
                logger.info(f"ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ (ëª¨ë‘ ì¤‘ë³µ): {file_path}")
        else:
            # ì „ì²´ ì €ì¥ ëª¨ë“œ
            final_data = data
            logger.info(f"ì „ì²´ ì €ì¥: {len(final_data)}ê°œ ë°ì´í„°")
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path}")
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({file_path}): {e}")
        # ì‹¤íŒ¨ ì‹œ ìƒˆ ë°ì´í„°ë§Œ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)