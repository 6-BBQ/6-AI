# utils.py ìˆ˜ì •ì•ˆ
import math, re, json, logging
from datetime import datetime, timezone
from pathlib import Path
from bs4 import BeautifulSoup
from typing import Optional
from rapidfuzz import process
from kiwipiepy import Kiwi

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("crawler")

_JOB_JSON = Path(__file__).with_name("job_names.json")
kiwi = Kiwi()

try:
    _JOB_LIST: list[str] = json.loads(_JOB_JSON.read_text(encoding="utf-8"))
    _JOB_SET: set[str] = set(_JOB_LIST)
except Exception as e:
    print(f"âš ï¸ ì§ì—…ëª… JSON ë¡œë”© ì˜¤ë¥˜: {e}")
    _JOB_LIST = []
    _JOB_SET = set()

_BOUNDARY = r"(?<![ê°€-í£A-Za-z0-9])(?:{names})(?![ê°€-í£A-Za-z0-9])"
_JOB_PATTERN = re.compile(
    _BOUNDARY.format(
        names="|".join(map(re.escape, sorted(_JOB_SET, key=len, reverse=True)))
    ),
    re.IGNORECASE,
)

CONTEXT_KEYWORDS = {"ê³µëµ", "ê°€ì´ë“œ", "ì„¸íŒ…", "ìŠ¤í™ì—…", "ë‰´ë¹„", "íŒ"}

def detect_class_name(title: str, body: Optional[str] = None) -> Optional[str]:
    if not title:
        return None

    text = f"{title} {body or ''}".lower()

    # 1ï¸âƒ£ ì •ê·œì‹ ê¸°ë°˜ ì§ì—… íƒì§€
    m = _JOB_PATTERN.search(text)
    if m:
        return m.group(0)

    # 2ï¸âƒ£ Kiwi ê¸°ë°˜ í† í°í™”
    try:
        tokens = set(word.form for word in kiwi.tokenize(text))
        hit = _JOB_SET.intersection(tokens)
        if hit:
            return next(iter(hit))
    except Exception as e:
        logger.warning(f"Kiwi ì˜¤ë¥˜: {e}")

    # 3ï¸âƒ£ Fuzzy Matching (ì¡°ê±´ë¶€)
    if len(title) < 25 and any(k in text for k in CONTEXT_KEYWORDS):
        result = process.extractOne(
            title.replace(" ", ""), _JOB_LIST, score_cutoff=92
        )
        if result:  # None ì²´í¬ ì¶”ê°€
            match, score, _ = result
            if score >= 92:
                return match

    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (HTML íƒœê·¸ ì œê±°, ì—°ì† ê³µë°± ì œê±° ë“±)"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    if '<' in text and '>' in text:  # HTMLë¡œ ë³´ì´ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
        text = BeautifulSoup(text, "html.parser").get_text(separator=" ")
    
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì¤„ë°”ê¿ˆ í‘œì¤€í™”
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    return text.strip()

def calculate_content_score(text, title=""):
    """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
    # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ 0ì 
    if not text:
        return 0
    
    score = 0
    
    # 1. ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 40ì )
    length = len(text)
    if length < 100:
        length_score = length / 5  # ìµœëŒ€ 20ì 
    elif length < 500:
        length_score = 20 + (length - 100) / 20  # 20-40ì 
    else:
        length_score = 40
    
    # 2. êµ¬ì¡° ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 30ì )
    # ì¤„ë°”ê¿ˆ ìˆ˜ (ë¬¸ë‹¨ êµ¬ë¶„ì´ ì˜ ëœ í…ìŠ¤íŠ¸ëŠ” ì ìˆ˜ ë†’ìŒ)
    newlines = text.count('\n')
    paragraphs = max(1, len([p for p in text.split('\n') if p.strip()]))
    structure_score = min(30, paragraphs + newlines/2)
    
    # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ (ìµœëŒ€ 30ì )
    keywords = ["ìŠ¤í™ì—…", "ê°€ì´ë“œ", "ê³µëµ", "ì¶”ì²œ", "íŒ", "ë…¸í•˜ìš°", "ì¥ë¹„", "ìŠ¤í‚¬", "ì¢…ë§ì˜ ìˆ­ë°°ì",
                "ìƒê¸‰ ë˜ì „", "ë ˆì´ë“œ", "ì—í”½", "í™œìš©", "ì¤‘ì²œ", "ìœµí•©ì„", "ë‰´ë¹„", "ë ˆê¸°ì˜¨"]
    
    combined = (title + " " + text).lower()
    keyword_count = sum(1 for kw in keywords if kw.lower() in combined)
    keyword_score = min(30, keyword_count * 3)
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    score = length_score + structure_score + keyword_score
    
    return min(100, score)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´íŠ¸ë³„ ì •ê·œí™” ê¸°ì¤€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SITE_NORMALIZATION = {
    "youtube": {"views_base": 50000, "likes_base": 500},
    "arca": {"views_base": 5000, "likes_base": 50}, 
    "dcinside": {"views_base": 1000, "likes_base": 10},
    "official": {"views_base": 10000, "likes_base": 100}  # ê³µí™ˆì€ ì¤‘ê°„ê°’
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìš°ì„ ìˆœìœ„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SRC_WEIGHT = {"official":1.0, "youtube":0.9, "arca":0.75, "dcinside":0.7}

def _freshness_w(date: datetime, today: datetime) -> float:
    return max(0.0, 1 - (today - date).days/180)      # 180ì¼ ë‚´ : 0~1

def _engage_w(views: int, likes: int, source: str) -> float:
    """ì‚¬ì´íŠ¸ë³„ ì •ê·œí™”ëœ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°"""
    import math
    
    # ì‚¬ì´íŠ¸ë³„ ì •ê·œí™” ê¸°ì¤€ ê°€ì ¸ì˜¤ê¸°
    norm = SITE_NORMALIZATION.get(source, {"views_base": 10000, "likes_base": 100})
    views_base = norm["views_base"]
    likes_base = norm["likes_base"]
    
    # ì‚¬ì´íŠ¸ë³„ ìƒëŒ€ì  ì¸ê¸°ë„ë¡œ ì •ê·œí™”
    normalized_views = views / views_base
    normalized_likes = likes / likes_base
    
    # ë¡œê·¸ ì •ê·œí™” ì ìš©
    v = min(1.0, math.log1p(normalized_views) / 2.5)  # 0~1 ì‚¬ì´
    l = min(1.0, math.log1p(normalized_likes) / 2.0)   # 0~1 ì‚¬ì´
    
    return (v * 0.7) + (l * 0.3)  # 0~1

def calc_quality_score(
    *, source: str, date: datetime,
    views: int = 0, likes: int = 0,
    today: datetime | None = None,
    content_score: float = 0.0
) -> float:
    """ì½˜í…ì¸  í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì§ì—… ì ìˆ˜ ì œì™¸, í†µí•© ì ìˆ˜)"""
    today = today or datetime.now(timezone.utc)
    
    # ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ë°˜ì˜ (0~1 ì‚¬ì´ë¡œ ì •ê·œí™”)
    content_weight = min(1.0, content_score / 100)
    
    # ì‚¬ì´íŠ¸ë³„ ì •ê·œí™”ëœ ì¸ê¸°ë„ ê³„ì‚°
    engagement_score = _engage_w(views, likes, source)
    
    # í†µí•© ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 10ì  ì •ë„)
    total_score = (
        (_SRC_WEIGHT.get(source, 0.5) * 3.0) +     # ì¶œì²˜ ì‹ ë¢°ë„ (0~3.0)
        (engagement_score * 2.5) +                  # ì‚¬ì´íŠ¸ë³„ ì •ê·œí™”ëœ ì¸ê¸°ë„ (0~2.5)
        (_freshness_w(date, today) * 2.0) +         # ì‹ ì„ ë„ (0~2.0)
        (content_weight * 2.5)                      # ì½˜í…ì¸  í’ˆì§ˆ (0~2.5)
    )
    
    return round(total_score, 2)

def load_yt_ids(path: str | Path) -> list[str]:
    if not path:
        return []
    
    try:
        with open(path, "r", encoding="utf-8") as f:
            ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
            logger.info(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
            return ids
    except Exception as e:
        logger.error(f"âŒ ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
        try:
            project_root = Path(__file__).resolve().parents[1]
            abs_path = project_root / path
            logger.info(f"ğŸ”„ ì ˆëŒ€ ê²½ë¡œë¡œ ì¬ì‹œë„: {abs_path}")
            with open(abs_path, "r", encoding="utf-8") as f:
                ids = [line.strip() for line in f if line.strip() and not line.startswith("#")]
                logger.info(f"âœ… ìœ íŠœë¸Œ ID íŒŒì¼ ë¡œë“œ ì„±ê³µ: {len(ids)}ê°œ ID ì°¾ìŒ")
                return ids
        except Exception as e2:
            logger.error(f"âŒ ì ˆëŒ€ ê²½ë¡œ ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
            return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ dict ë¹Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_item(
    *, source: str, url: str, title: str, body: str,
    date: str, views: int = 0, likes: int = 0
) -> dict:
    
    # 1) ë‚ ì§œ ë¬¸ìì—´ â†’ datetime
    if isinstance(date, str):
        try:
            # "YYYY-MM-DD" ë˜ëŠ” "YYYY.MM.DD" ëª¨ë‘ ëŒ€ì‘
            clean = date.replace(".", "-")[:10]
            date_obj = datetime.strptime(clean, "%Y-%m-%d")
        except ValueError:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°ìœ¼ë¡œ ëŒ€ì²´
            date_obj = datetime.now()
    else:
        date_obj = date

    # 2) tzinfo ì—†ìœ¼ë©´ UTC ë¡œ ë¶€ì—¬
    if isinstance(date_obj, datetime) and date_obj.tzinfo is None:
        date_obj = date_obj.replace(tzinfo=timezone.utc)

    # 3) ë³¸ë¬¸ ì •ë¦¬
    clean_body = clean_text(body)
    
    # 4) ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    content_score = calculate_content_score(clean_body, title)

    # 5) ì§ì—… ì´ë¦„ ê°ì§€ (ë©”íƒ€ë°ì´í„°ìš©ë§Œ, ì ìˆ˜ì—ëŠ” ë¯¸ë°˜ì˜)
    cls = detect_class_name(title, clean_body)
    
    # 6) í†µí•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì§ì—… ì ìˆ˜ ì œì™¸)
    quality_score = calc_quality_score(
        source=source, date=date_obj,
        views=views, likes=likes,
        content_score=content_score
    )
    
    return {
        "url": url,
        "title": title,
        "date": date_obj.strftime("%Y-%m-%d"),
        "views": views, 
        "likes": likes,
        "class_name": cls,
        "source": source,
        "quality_score": quality_score,  # í†µí•©ëœ ë‹¨ì¼ ì ìˆ˜
        "body": clean_body,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•„í„°ë§ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def should_process_url(url, visited_urls=None):
    """URL ì²˜ë¦¬ ì—¬ë¶€ ê²°ì • (ì¦ë¶„ í¬ë¡¤ë§ ì§€ì›)"""
    if visited_urls is None:
        return True
    
    # ì´ë¯¸ ë°©ë¬¸í•œ URLì´ë©´ ê±´ë„ˆë›°ê¸°
    if url in visited_urls:
        return False
    
    return True

def filter_by_keywords(text, include_keywords, exclude_keywords):
    """í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§"""
    if not text:
        return False
    
    # ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
    for keyword in exclude_keywords:
        if keyword in text:
            return False
    
    # í¬í•¨ í‚¤ì›Œë“œ í™•ì¸
    for keyword in include_keywords:
        if keyword in text:
            return True
    
    return False