#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒŒì¼ â†’ official_raw.json ì „ì²˜ë¦¬ ë„êµ¬
ì‚¬ìš©ë²•: python txt_to_official.py <txt_íŒŒì¼_ê²½ë¡œ> [ì œëª©] [URL]
"""

import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime, timezone
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

try:
    from crawlers.utils import build_item, save_official_data
except ImportError:
    print("âš ï¸ crawlers/utils.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def build_item(*, source, url, title, body, date, views=0, likes=0):
        """ê¸°ë³¸ build_item í•¨ìˆ˜"""
        return {
            "url": url,
            "title": title,
            "date": date.strftime("%Y-%m-%d") if isinstance(date, datetime) else date,
            "views": views,
            "likes": likes,
            "class_name": None,
            "source": source,
            "quality_score": 6.0,
            "body": clean_text(body),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    def save_official_data(data, append=True):
        """ê¸°ë³¸ save_official_data í•¨ìˆ˜"""
        save_path = project_root / "data/raw/official_raw.json"
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        if append and save_path.exists():
            with open(save_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
            existing_data.extend(data)
            final_data = existing_data
        else:
            final_data = data
        
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    if '<' in text and '>' in text:
        text = BeautifulSoup(text, "html.parser").get_text(separator="\n")
    
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    # ì—°ì† ì¤„ë°”ê¿ˆ ì œê±° (3ê°œ ì´ìƒ â†’ 2ê°œ)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

def extract_title_from_content(content):
    """ë‚´ìš©ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„"""
    lines = content.strip().split('\n')
    
    # ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
    for line in lines:
        line = line.strip()
        if line and not line.startswith('#'):
            # ë„ˆë¬´ ê¸´ ê²½ìš° ìë¥´ê¸°
            if len(line) > 100:
                line = line[:97] + "..."
            return line
    
    return "ë˜íŒŒ ê°€ì´ë“œ ë¬¸ì„œ"

def process_txt_file(txt_path, title=None, url=None):
    """í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ official_raw.jsonì— ì¶”ê°€"""
    
    txt_path = Path(txt_path)
    
    if not txt_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {txt_path}")
        return False
    
    try:
        # íŒŒì¼ ì½ê¸°
        print(f"ğŸ“– íŒŒì¼ ì½ëŠ” ì¤‘: {txt_path}")
        with open(txt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if not content.strip():
            print("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        
        # ì œëª© ì„¤ì •
        if not title:
            title = extract_title_from_content(content)
        
        # ë‚´ìš© ì •ë¦¬
        cleaned_content = clean_text(content)
        
        # build_item ì‚¬ìš©í•´ì„œ ì•„ì´í…œ ìƒì„±
        item = build_item(
            source="official",
            url=url,
            title=title,
            body=cleaned_content,
            date=datetime.now(timezone.utc),
            views=0,
            likes=0
        )
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ğŸ“„ ì œëª©: {title}")
        print(f"   ğŸ”— URL: {url}")
        print(f"   ğŸ“Š ë‚´ìš© ê¸¸ì´: {len(cleaned_content)}ì")
        print(f"   â­ í’ˆì§ˆ ì ìˆ˜: {item.get('quality_score', 'N/A')}")
        
        # official_raw.jsonì— ì¶”ê°€
        print(f"ğŸ’¾ ì €ì¥ ì¤‘...")
        save_official_data([item], append=True)
        
        print(f"ğŸ‰ ì™„ë£Œ! official_raw.jsonì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python txt_to_official.py <txt_íŒŒì¼_ê²½ë¡œ> [ì œëª©] [URL]")
        print("\nì˜ˆì‹œ:")
        print("  python txt_to_official.py guide.txt")
        print("  python txt_to_official.py guide.txt \"ë˜íŒŒ ìŠ¤í™ì—… ê°€ì´ë“œ\"")
        print("  python txt_to_official.py guide.txt \"ë˜íŒŒ ìŠ¤í™ì—… ê°€ì´ë“œ\" \"https://df.nexon.com/guide/specup\"")
        return
    
    txt_path = sys.argv[1]
    title = sys.argv[2] if len(sys.argv) > 2 else None
    url = sys.argv[3] if len(sys.argv) > 3 else None
    
    print("ğŸ”§ ë˜íŒŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì „ì²˜ë¦¬ ë„êµ¬")
    print("=" * 50)
    
    success = process_txt_file(txt_path, title, url)
    
    if success:
        print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("   python pipeline.py --skip-crawl")
    else:
        print("\nâŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
