from __future__ import annotations

import argparse
import subprocess
import sys
import textwrap
from datetime import datetime
from pathlib import Path
from typing import List

from config import config
from utils import get_logger

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CRAWLER_SCRIPT: Path = Path(config.CRAWLER_SCRIPT)
PREPROCESS_SCRIPT: Path = Path(config.PREPROCESS_SCRIPT)
BUILD_VECTORDB_SCRIPT: Path = Path(config.BUILD_VECTORDB_SCRIPT)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ ì‹¤í–‰ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_script(script: Path, args: List[str]) -> None:
    """í•˜ìœ„ Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ"""
    cmd = [sys.executable, str(script), *args]
    logger.info("â–¶ %s", " ".join(cmd))
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as exc:
        logger.error("âŒ %s ì‹¤íŒ¨ â€” %s", script.name, exc)
        sys.exit(exc.returncode)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    global logger
    logger = get_logger("pipeline")

    # í•„ìˆ˜ ë””ë ‰í„°ë¦¬ ë³´ì¥
    config.create_directories()

    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="ë˜íŒŒ ìŠ¤í™ì—… AI íŒŒì´í”„ë¼ì¸ (YouTube ì œì™¸)",
        epilog=textwrap.dedent(
            """
            ì‚¬ìš© ì˜ˆì‹œ:
              python pipeline.py                # ì¦ë¶„ ëª¨ë“œ
              python pipeline.py --full         # ì „ì²´ ì¬ì²˜ë¦¬
              python pipeline.py --skip-crawl   # ì „ì²˜ë¦¬ë¶€í„° ì‹¤í–‰
            """,
        ),
    )

    # ì²˜ë¦¬ ëª¨ë“œ í”Œë˜ê·¸
    mode = parser.add_mutually_exclusive_group()
    mode.add_argument("--incremental", action="store_true", default=True, help="ì¦ë¶„ ì²˜ë¦¬(ê¸°ë³¸)")
    mode.add_argument("--full", action="store_true", help="ì „ì²´ ì¬ì²˜ë¦¬")

    # í¬ë¡¤ëŸ¬ ê¸°ë³¸ ì˜µì…˜ (config ê°’ ë°˜ì˜)
    parser.add_argument("--pages", type=int, default=config.DEFAULT_CRAWL_PAGES)
    parser.add_argument("--depth", type=int, default=config.DEFAULT_CRAWL_DEPTH)
    parser.add_argument(
        "--sources",
        type=str,
        default="all",
        help="í¬ë¡¤ë§ ì†ŒìŠ¤ ì§€ì • (official,dc,arca,all)",
    )

    # ë‹¨ê³„ ìŠ¤í‚µ
    parser.add_argument("--skip-crawl", action="store_true")
    parser.add_argument("--skip-preprocess", action="store_true")
    parser.add_argument("--skip-vectordb", action="store_true")
    parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ ì‚°ì¶œë¬¼ ê°•ì œ ë®ì–´ì“°ê¸°")

    args = parser.parse_args()

    # ì „ì²´ ëª¨ë“œì¼ ë•Œ increment í”Œë˜ê·¸ ì˜¤ë²„ë¼ì´ë“œ
    if args.full:
        args.incremental = False

    logger.info("â•" * 60)
    logger.info("ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ â€” %s ëª¨ë“œ", "ì¦ë¶„" if args.incremental else "ì „ì²´")
    logger.info("ğŸ“… %s", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

    t_start = datetime.now()

    # 1ï¸âƒ£ í¬ë¡¤ë§ ë‹¨ê³„ ---------------------------------------------------------
    if not args.skip_crawl:
        crawl_cli = [
            "--pages", str(args.pages),
            "--depth", str(args.depth),
            "--sources", args.sources,
            "--merge",
            "--incremental" if args.incremental else "--full",
        ]
        run_script(CRAWLER_SCRIPT, crawl_cli)
    else:
        logger.info("â­ í¬ë¡¤ë§ ë‹¨ê³„ ìŠ¤í‚µ")

    # 2ï¸âƒ£ ì „ì²˜ë¦¬ ë‹¨ê³„ ---------------------------------------------------------
    if not args.skip_preprocess:
        pre_cli: List[str] = []
        if args.incremental:
            pre_cli.append("--incremental")
        if args.force:
            pre_cli.append("--force")
        run_script(PREPROCESS_SCRIPT, pre_cli)
    else:
        logger.info("â­ ì „ì²˜ë¦¬ ë‹¨ê³„ ìŠ¤í‚µ")

    # 3ï¸âƒ£ ë²¡í„° DB êµ¬ì¶• ë‹¨ê³„ ----------------------------------------------------
    if not args.skip_vectordb:
        vec_cli: List[str] = []
        if args.incremental:
            vec_cli.append("--incremental")
        if args.force:
            vec_cli.append("--force")
        run_script(BUILD_VECTORDB_SCRIPT, vec_cli)
    else:
        logger.info("â­ ë²¡í„° DB ë‹¨ê³„ ìŠ¤í‚µ")

    elapsed = (datetime.now() - t_start).total_seconds()
    logger.info("ğŸ‰ ì „ì²´ ì™„ë£Œ â€” %.1fs (%.1fm)", elapsed, elapsed / 60)
    logger.info("â•" * 60)


if __name__ == "__main__":
    main()