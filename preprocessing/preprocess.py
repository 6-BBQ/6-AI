from __future__ import annotations

import json
import logging
import re
import sys
import hashlib
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))
from config import config
from typing import Any, Dict, List, Set
from datetime import datetime

from kiwipiepy import Kiwi
from langchain.text_splitter import RecursiveCharacterTextSplitter

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ì„¤ì •
MERGED_DIR           = Path(config.MERGED_DIR)
RAW_DIR              = Path(config.RAW_DIR)
PROCESSED_SAVE_PATH  = Path(config.PROCESSED_SAVE_PATH)
PROCESSED_CACHE_PATH = Path(config.PROCESSED_CACHE_PATH)
CHUNK_SIZE           = config.CHUNK_SIZE
CHUNK_OVERLAP        = config.CHUNK_OVERLAP

# ì•½ì–´ â†’ ì •ì‹ ìš©ì–´ ë§¤í•‘
DNF_TERMS = {
}
DNF_CLASSES = {
    # â€¦ í•„ìš”í•œ ë§Œí¼ ì¶”ê°€
}

# ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
log = logging.getLogger("preprocess")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ ì¦ë¶„ ì²˜ë¦¬ ë„êµ¬

def get_file_hash(file_path: Path) -> str:
    """íŒŒì¼ì˜ MD5 í•´ì‹œ ê³„ì‚°"""
    hasher = hashlib.md5()
    try:
        with file_path.open('rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception:
        return ""

def load_processed_cache() -> Dict[str, str]:
    """ì²˜ë¦¬ëœ íŒŒì¼ ìºì‹œ ë¡œë“œ (file_path -> file_hash)"""
    try:
        if PROCESSED_CACHE_PATH.exists():
            with PROCESSED_CACHE_PATH.open('r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        log.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return {}

def save_processed_cache(cache: Dict[str, str]) -> None:
    """ì²˜ë¦¬ëœ íŒŒì¼ ìºì‹œ ì €ì¥"""
    try:
        PROCESSED_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
        with PROCESSED_CACHE_PATH.open('w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_new_and_updated_files(processed_cache: Dict[str, str]) -> List[Path]:
    """ìƒˆë¡œìš´ íŒŒì¼ê³¼ ì—…ë°ì´íŠ¸ëœ íŒŒì¼ íƒì§€"""
    new_files = []
    
    # MERGED_DIRê³¼ RAW_DIR ëª¨ë‘ ê²€ì‚¬
    search_dirs = [MERGED_DIR, RAW_DIR]
    
    for search_dir in search_dirs:
        if not search_dir.exists():
            continue
            
        for path in search_dir.rglob("*"):
            if not path.is_file():
                continue
            if path.suffix.lower() not in {".json", ".jsonl"}:
                continue
                
            # ìƒëŒ€ ê²½ë¡œë¥¼ ê³„ì‚°í•  ë•Œ ì–´ëŠ ë””ë ‰í† ë¦¬ì—ì„œ ì˜¨ íŒŒì¼ì¸ì§€ êµ¬ë¶„
            try:
                if search_dir == MERGED_DIR:
                    file_path_str = f"merged/{path.relative_to(MERGED_DIR)}"
                else:
                    file_path_str = f"raw/{path.relative_to(RAW_DIR)}"
            except ValueError:
                continue
                
            current_hash = get_file_hash(path)
            
            # ìƒˆ íŒŒì¼ì´ê±°ë‚˜ í•´ì‹œê°€ ë³€ê²½ëœ ê²½ìš°
            if file_path_str not in processed_cache or processed_cache[file_path_str] != current_hash:
                new_files.append(path)
                processed_cache[file_path_str] = current_hash
    
    return new_files

def load_existing_processed_docs() -> Set[str]:
    """ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ ID ì§‘í•© ë¡œë“œ"""
    existing_ids = set()
    try:
        if PROCESSED_SAVE_PATH.exists():
            with PROCESSED_SAVE_PATH.open('r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        existing_ids.add(data.get('id', ''))
    except Exception as e:
        log.warning(f"ê¸°ì¡´ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return existing_ids

def generate_document_id(doc: Dict[str, Any], chunk_index: int = 0) -> str:
    """ë¬¸ì„œì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•œ ID ìƒì„±"""
    # ê³ ìœ  ì‹ë³„ì ìƒì„±ì„ ìœ„í•œ ìš”ì†Œë“¤
    id_components = []
    
    # 1. URLì´ ìˆìœ¼ë©´ ìµœìš°ì„  ì‚¬ìš©
    if doc.get('url'):
        id_components.append(doc['url'])
    
    # 2. ì œëª© ì‚¬ìš©
    title = doc.get('title', '').strip()
    if title:
        id_components.append(title)
    
    # 3. ë‚´ìš© ì¼ë¶€ ì‚¬ìš© (ì²« 100ì)
    content = doc.get('body', '') or doc.get('content', '')
    if content:
        content_preview = clean_html(content)[:100]
        id_components.append(content_preview)
    
    # 4. íŒŒì¼ ì†ŒìŠ¤ ì •ë³´
    if doc.get('_file_source'):
        id_components.append(doc['_file_source'])
    
    # 5. íƒ€ì„ìŠ¤íƒ¬í”„ (ë‚ ì§œ ì •ë³´)
    if doc.get('date'):
        id_components.append(str(doc['date']))
    elif doc.get('timestamp'):
        id_components.append(str(doc['timestamp']))
    
    # ì¡°í•©ëœ ë¬¸ìì—´ì„ í•´ì‹œí™”
    combined = '|'.join(id_components)
    doc_hash = hashlib.md5(combined.encode('utf-8')).hexdigest()[:12]  # 12ìë¦¬ë¡œ ì¶•ì•½
    
    # ì²­í¬ ì¸ë±ìŠ¤ì™€ ì¡°í•©í•˜ì—¬ ìµœì¢… ID ìƒì„±
    return f"doc_{doc_hash}_chunk_{chunk_index}"

def check_id_uniqueness(doc_id: str, existing_ids: Set[str]) -> str:
    """ID ì¤‘ë³µ ê²€ì‚¬ ë° ê³ ìœ  ID ë³´ì¥"""
    original_id = doc_id
    counter = 1
    
    while doc_id in existing_ids:
        # ì¤‘ë³µì´ë©´ suffix ì¶”ê°€
        if "_chunk_" in original_id:
            base_part, chunk_part = original_id.rsplit('_chunk_', 1)
            doc_id = f"{base_part}_dup{counter}_chunk_{chunk_part}"
        else:
            doc_id = f"{original_id}_dup{counter}"
        counter += 1
    
    return doc_id

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ í—¬í¼
kiwi = Kiwi()

RE_HTML_TAG = re.compile(r"<[^>]*>")        # very naive stripper


def clean_html(text: str) -> str:
    """íƒœê·¸ ì œê±° + ë¼ì¸ ì •ë¦¬"""
    text = RE_HTML_TAG.sub("", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_abbr(text: str) -> str:
    """DNF ì•½ì–´/ì§ì—…ëª… í†µì¼"""
    for k, v in {**DNF_TERMS, **DNF_CLASSES}.items():
        text = re.sub(rf"\b{re.escape(k)}\b", v, text, flags=re.IGNORECASE)
    return text


def sent_tokenize(text: str) -> List[str]:
    """Korean sentence splitter (Kiwi)"""
    spans = kiwi.split_into_sents(text)
    return [s.text for s in spans]


def load_raw_files(incremental: bool = False) -> List[Dict[str, Any]]:
    """ë¡œ íŒŒì¼ ë¡œë“œ (ì¦ë¶„ ì²˜ë¦¬ ì§€ì›)"""
    docs: List[Dict[str, Any]] = []
    
    if incremental:
        # ì¦ë¶„ ëª¨ë“œ: ìƒˆë¡œìš´/ì—…ë°ì´íŠ¸ëœ íŒŒì¼ë§Œ ì²˜ë¦¬
        processed_cache = load_processed_cache()
        new_files = get_new_and_updated_files(processed_cache)
        
        if not new_files:
            log.info("ğŸ”„ ì¦ë¶„ ëª¨ë“œ: ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        log.info(f"ğŸ”„ ì¦ë¶„ ëª¨ë“œ: {len(new_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")
        files_to_process = new_files
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        save_processed_cache(processed_cache)
    else:
        # ì „ì²´ ëª¨ë“œ: ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        files_to_process = []
        
        # MERGED_DIRê³¼ RAW_DIR ëª¨ë‘ ì²˜ë¦¬
        search_dirs = [MERGED_DIR, RAW_DIR]
        for search_dir in search_dirs:
            if search_dir.exists():
                files_to_process.extend(list(search_dir.rglob("*")))
        
        files_to_process = [p for p in files_to_process if p.is_file()]
        log.info(f"ğŸ“‹ ì „ì²´ ëª¨ë“œ: {len(files_to_process)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")
    
    for path in files_to_process:
        if path.suffix.lower() in {".json", ".jsonl"}:
            with path.open(encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    if isinstance(data, list):
                        # ìƒˆë¡œìš´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                        for item in data:
                            if isinstance(item, dict):
                                # íŒŒì¼ì´ ì–´ëŠ ë””ë ‰í† ë¦¬ì—ì„œ ì™”ëŠ”ì§€ ê²°ì •
                                try:
                                    if RAW_DIR in path.parents or path.parent == RAW_DIR:
                                        item['_file_source'] = f"raw/{path.relative_to(RAW_DIR)}"
                                    else:
                                        item['_file_source'] = f"merged/{path.relative_to(MERGED_DIR)}"
                                except ValueError:
                                    item['_file_source'] = str(path.name)
                                item['_processed_at'] = datetime.now().isoformat()
                        docs.extend(data)
                    else:
                        # íŒŒì¼ì´ ì–´ëŠ ë””ë ‰í† ë¦¬ì—ì„œ ì™”ëŠ”ì§€ ê²°ì •
                        try:
                            if RAW_DIR in path.parents or path.parent == RAW_DIR:
                                data['_file_source'] = f"raw/{path.relative_to(RAW_DIR)}"
                            else:
                                data['_file_source'] = f"merged/{path.relative_to(MERGED_DIR)}"
                        except ValueError:
                            data['_file_source'] = str(path.name)
                        data['_processed_at'] = datetime.now().isoformat()
                        docs.append(data)
                except json.JSONDecodeError:
                    log.warning("JSON decode failed: %s", path)
        else:
            # html / txt
            with path.open(encoding="utf-8") as f:
                # íŒŒì¼ì´ ì–´ëŠ ë””ë ‰í† ë¦¬ì—ì„œ ì™”ëŠ”ì§€ ê²°ì •
                try:
                    if RAW_DIR in path.parents or path.parent == RAW_DIR:
                        file_source = f"raw/{path.relative_to(RAW_DIR)}"
                    else:
                        file_source = f"merged/{path.relative_to(MERGED_DIR)}"
                except ValueError:
                    file_source = str(path.name)
                    
                doc_data = {
                    "title": path.stem, 
                    "body": f.read(), 
                    "source": str(path),
                    '_file_source': file_source,
                    '_processed_at': datetime.now().isoformat()
                }
                docs.append(doc_data)
    
    return docs


def extract_metadata(doc: Dict[str, Any]) -> Dict[str, Any]:
    """ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {}
    
    # ê¸°ë³¸ í•„ë“œë“¤
    if "url" in doc:
        metadata["url"] = doc["url"]
    if "source" in doc:
        metadata["source"] = doc["source"]
    if "date" in doc:
        metadata["date"] = doc["date"]
    if "timestamp" in doc:
        metadata["timestamp"] = doc["timestamp"]
    
    # ìˆ˜ì¹˜ ì •ë³´
    if "views" in doc:
        metadata["views"] = doc["views"]
    if "likes" in doc:
        metadata["likes"] = doc["likes"]
    if "quality_score" in doc:
        metadata["quality_score"] = doc["quality_score"]
    
    # ì¹´í…Œê³ ë¦¬/í´ë˜ìŠ¤ ì •ë³´
    if "class_name" in doc:
        metadata["class_name"] = doc["class_name"]
    
    return metadata


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ ê°œì„ ëœ ë©”ì¸ íŒŒì´í”„ë¼ì¸
def main(incremental: bool = False) -> None:
    log.info("ğŸ” raw ë¬¸ì„œ ë¡œë“œ ì¤‘â€¦")
    raw_docs = load_raw_files(incremental=incremental)
    log.info("âœ… %dê°œ ë¡œë“œ ì™„ë£Œ", len(raw_docs))

    # ì¦ë¶„ ëª¨ë“œì—ì„œ ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if incremental and not raw_docs:
        log.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! (ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ ì—†ìŒ)")
        return

    # ì¦ë¶„ ëª¨ë“œì¼ ë•Œë§Œ ê¸°ì¡´ ID ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)
    existing_ids = set()
    if incremental:
        existing_ids = load_existing_processed_docs()
        log.info(f"ğŸ“‹ ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ ID: {len(existing_ids)}ê°œ")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ".", "!", "?", " ", ""],
    )

    # ì¦ë¶„ ëª¨ë“œëŠ” append, ì „ì²´ ëª¨ë“œëŠ” ìƒˆë¡œ ìƒì„±
    mode = "a" if incremental and PROCESSED_SAVE_PATH.exists() else "w"
    out_f = PROCESSED_SAVE_PATH.open(mode, encoding="utf-8")

    processed = 0
    skipped_duplicates = 0
    
    for idx, doc in enumerate(raw_docs):
        # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
        title = doc.get("title", "").strip()
        # 'body' í‚¤ë¥¼ ìš°ì„  í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ 'content' í™•ì¸
        content = doc.get("body", "") or doc.get("content", "")
        content = clean_html(content)
        
        # titleê³¼ contentê°€ ë™ì¼í•˜ê±°ë‚˜ contentê°€ ë¹„ì–´ìˆìœ¼ë©´ contentë§Œ ì‚¬ìš©
        if not content or content.strip() == title:
            merged = title
        else:
            merged = f"{title}\n{content}" if title else content
        merged = normalize_abbr(merged)

        # ê¸´ ë¬¸ì„œëŠ” sentence ë‹¨ìœ„ë¡œ split í›„ chunk
        sentences = sent_tokenize(merged)
        merged_clean = "\n".join(sentences)

        chunks = splitter.split_text(merged_clean)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_metadata(doc)
        metadata.update({
            "title": title,
            "doc_index": idx,  # id_raw ëŒ€ì‹  doc_index ì‚¬ìš©
            "_file_source": doc.get('_file_source', ''),
            "_processed_at": doc.get('_processed_at', '')
        })

        # ì¦ë¶„ ëª¨ë“œì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
        if incremental:
            # ê¸°ë³¸ ë¬¸ì„œIDë¡œ ì¤‘ë³µ ì²´í¬ (ì²­í¬ 0 ê¸°ì¤€)
            base_doc_id = generate_document_id(doc, 0)
            base_prefix = base_doc_id.replace('_chunk_0', '')
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
            already_processed = any(existing_id.startswith(base_prefix) for existing_id in existing_ids)
            
            if already_processed:
                skipped_duplicates += len(chunks)
                log.debug(f"ë¬¸ì„œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì²˜ë¦¬ë¨): {title[:50]}...")
                continue

        for chunk_idx, chunk in enumerate(chunks):
            # ìƒˆë¡œìš´ ID ìƒì„± ë¡œì§ ì‚¬ìš©
            doc_id = generate_document_id(doc, chunk_idx)
            
            # ì¦ë¶„ ëª¨ë“œì—ì„œ ID ê³ ìœ ì„± ë³´ì¥
            if incremental:
                doc_id = check_id_uniqueness(doc_id, existing_ids)
                existing_ids.add(doc_id)
            
            rec = {
                "id": doc_id,
                "content": chunk,
                "metadata": metadata.copy(),  # ê° ì²­í¬ë§ˆë‹¤ ë³µì‚¬ë³¸ ìƒì„±
            }
            out_f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            processed += 1

    out_f.close()
    log.info("ğŸš€ %dê°œ ì²­í¬ ì €ì¥ â†’ %s", processed, PROCESSED_SAVE_PATH)
    
    # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
    log.info("ğŸ“Š ì²˜ë¦¬ í†µê³„:")
    log.info(f"   - ì›ë³¸ ë¬¸ì„œ: {len(raw_docs)}ê°œ")
    log.info(f"   - ìƒì„±ëœ ì²­í¬: {processed}ê°œ")
    if skipped_duplicates > 0:
        log.info(f"   - ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {skipped_duplicates}ê°œ")
    if len(raw_docs) > 0:
        log.info(f"   - í‰ê·  ì²­í¬/ë¬¸ì„œ: {processed/len(raw_docs):.1f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë˜íŒŒ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (ê°œì„ ëœ ì¦ë¶„ ì²˜ë¦¬)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ì „ì²´ ì „ì²˜ë¦¬ (ê¸°ë³¸ ëª¨ë“œ)
  python preprocess.py
  
  # ì¦ë¶„ ì „ì²˜ë¦¬ (ìƒˆë¡œìš´ ë°ì´í„°ë§Œ)
  python preprocess.py --incremental
  
  # ì¦ë¶„ ì „ì²˜ë¦¬ + ìƒì„¸ ë¡œê·¸
  python preprocess.py --incremental --verbose
        """
    )
    
    parser.add_argument(
        "--incremental", 
        action="store_true", 
        default=True,
        help="ì¦ë¶„ ì „ì²˜ë¦¬ ëª¨ë“œ (ê¸°ë³¸ê°’)"
    )
    
    parser.add_argument(
        "--full", 
        action="store_true", 
        help="ì „ì²´ ì „ì²˜ë¦¬ ëª¨ë“œ (ì¦ë¶„ ë¬´ì‹œ)"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true", 
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=CHUNK_SIZE,
        help=f"ì²­í¬ í¬ê¸° (ê¸°ë³¸: {CHUNK_SIZE})"
    )
    
    parser.add_argument(
        "--chunk-overlap",
        type=int, 
        default=CHUNK_OVERLAP,
        help=f"ì²­í¬ ê²¹ì¹¨ í¬ê¸° (ê¸°ë³¸: {CHUNK_OVERLAP})"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="ê¸°ì¡´ ì²˜ë¦¬ ê²°ê³¼ ê°•ì œ ë®ì–´ì“°ê¸°"
    )
    
    args = parser.parse_args()
    
    # ì „ì²´ ëª¨ë“œ ê²€ì‚¬
    if args.full:
        args.incremental = False
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ì²­í¬ ì„¤ì • ì—…ë°ì´íŠ¸
    CHUNK_SIZE = args.chunk_size
    CHUNK_OVERLAP = args.chunk_overlap
    
    # ê°•ì œ ëª¨ë“œ ì²˜ë¦¬
    if args.force:
        if PROCESSED_SAVE_PATH.exists():
            PROCESSED_SAVE_PATH.unlink()
            log.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ì‚­ì œ: {PROCESSED_SAVE_PATH}")
        if PROCESSED_CACHE_PATH.exists():
            PROCESSED_CACHE_PATH.unlink() 
            log.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ìºì‹œ íŒŒì¼ ì‚­ì œ: {PROCESSED_CACHE_PATH}")
    
    # ì‹œì‘ ë©”ì‹œì§€
    mode_emoji = "ğŸ”„" if args.incremental else "ğŸ“‹"
    mode_name = "ì¦ë¶„" if args.incremental else "ì „ì²´"
    log.info(f"{mode_emoji} {mode_name} ì „ì²˜ë¦¬ ëª¨ë“œ ì‹œì‘")
    log.info(f"   - ì²­í¬ í¬ê¸°: {CHUNK_SIZE}")
    log.info(f"   - ì²­í¬ ê²¹ì¹¨: {CHUNK_OVERLAP}")
    log.info(f"   - ì…ë ¥ ë””ë ‰í† ë¦¬: {MERGED_DIR}")
    log.info(f"   - ì¶œë ¥ íŒŒì¼: {PROCESSED_SAVE_PATH}")
    
    try:
        main(incremental=args.incremental)
        log.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    except KeyboardInterrupt:
        log.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        log.error(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise
