from __future__ import annotations

import json
import logging
import re
from pathlib import Path
from typing import Any, Dict, List

from kiwipiepy import Kiwi
from langchain.text_splitter import RecursiveCharacterTextSplitter

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ì„¤ì •
MERGED_DIR = Path("data/merged")      # í†µí•©ëœ í¬ë¡¤ë§ ê²°ê³¼ í´ë”
SAVE_PATH = Path("data/processed_docs.jsonl")
CHUNK_SIZE = 400
CHUNK_OVERLAP = 100

# ì•½ì–´ â†’ ì •ì‹ ìš©ì–´ ë§¤í•‘
DNF_TERMS = {
}
DNF_CLASSES = {
    # â€¦ í•„ìš”í•œ ë§Œí¼ ì¶”ê°€
}

# ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
log = logging.getLogger("preprocess")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ í—¬í¼
kiwi = Kiwi()

RE_HTML_TAG = re.compile(r"<[^>]*>")        # very naive stripper


def clean_html(text: str) -> str:
    """íƒœê·¸ ì œê±° + ë¼ì¸ ì •ë¦¬"""
    text = RE_HTML_TAG.sub("", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def normalize_abbr(text: str) -> str:
    """DNF ì•½ì–´/ì§ì—…ëª… í†µì¼"""
    for k, v in {**DNF_TERMS, **DNF_CLASSES}.items():
        text = re.sub(rf"\b{re.escape(k)}\b", v, text, flags=re.IGNORECASE)
    return text


def sent_tokenize(text: str) -> List[str]:
    """Korean sentence splitter (Kiwi)"""
    spans = kiwi.split_into_sents(text)
    return [s.text for s in spans]


def load_raw_files() -> List[Dict[str, Any]]:
    docs: List[Dict[str, Any]] = []
    for path in MERGED_DIR.rglob("*"):
        if not path.is_file():
            continue
        if path.suffix.lower() in {".json", ".jsonl"}:
            with path.open(encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    docs.extend(data if isinstance(data, list) else [data])
                except json.JSONDecodeError:
                    log.warning("JSON decode failed: %s", path)
        else:
            # html / txt
            with path.open(encoding="utf-8") as f:
                docs.append(
                    {"title": path.stem, "body": f.read(), "source": str(path)}
                )
    return docs


def extract_metadata(doc: Dict[str, Any]) -> Dict[str, Any]:
    """ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {}
    
    # ê¸°ë³¸ í•„ë“œë“¤
    if "url" in doc:
        metadata["url"] = doc["url"]
    if "source" in doc:
        metadata["source"] = doc["source"]
    if "date" in doc:
        metadata["date"] = doc["date"]
    if "timestamp" in doc:
        metadata["timestamp"] = doc["timestamp"]
    
    # ìˆ˜ì¹˜ ì •ë³´
    if "views" in doc:
        metadata["views"] = doc["views"]
    if "likes" in doc:
        metadata["likes"] = doc["likes"]
    if "quality_score" in doc:
        metadata["quality_score"] = doc["quality_score"]
    
    # ì¹´í…Œê³ ë¦¬/í´ë˜ìŠ¤ ì •ë³´
    if "class_name" in doc:
        metadata["class_name"] = doc["class_name"]
    
    return metadata


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ ë©”ì¸ íŒŒì´í”„ë¼ì¸
def main() -> None:
    log.info("ğŸ” raw ë¬¸ì„œ ë¡œë“œ ì¤‘â€¦")
    raw_docs = load_raw_files()
    log.info("âœ… %dê°œ ë¡œë“œ ì™„ë£Œ", len(raw_docs))

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ".", "!", "?", " ", ""],
    )

    out_f = SAVE_PATH.open("w", encoding="utf-8")

    processed = 0
    for idx, doc in enumerate(raw_docs):
        # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
        title = doc.get("title", "").strip()
        # 'body' í‚¤ë¥¼ ìš°ì„  í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ 'content' í™•ì¸
        content = doc.get("body", "") or doc.get("content", "")
        content = clean_html(content)
        
        # titleê³¼ contentê°€ ë™ì¼í•˜ê±°ë‚˜ contentê°€ ë¹„ì–´ìˆìœ¼ë©´ contentë§Œ ì‚¬ìš©
        if not content or content.strip() == title:
            merged = title
        else:
            merged = f"{title}\n{content}" if title else content
        merged = normalize_abbr(merged)

        # ê¸´ ë¬¸ì„œëŠ” sentence ë‹¨ìœ„ë¡œ split í›„ chunk
        sentences = sent_tokenize(merged)
        merged_clean = "\n".join(sentences)

        chunks = splitter.split_text(merged_clean)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_metadata(doc)
        metadata.update({
            "title": title,
            "id_raw": idx,
        })

        for n, chunk in enumerate(chunks):
            rec = {
                "id": f"{idx}_{n}",
                "content": chunk,
                "metadata": metadata.copy(),  # ê° ì²­í¬ë§ˆë‹¤ ë³µì‚¬ë³¸ ìƒì„±
            }
            out_f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            processed += 1

    out_f.close()
    log.info("ğŸš€ %dê°œ ì²­í¬ ì €ì¥ â†’ %s", processed, SAVE_PATH)
    
    # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
    log.info("ğŸ“Š ì²˜ë¦¬ í†µê³„:")
    log.info(f"   - ì›ë³¸ ë¬¸ì„œ: {len(raw_docs)}ê°œ")
    log.info(f"   - ìƒì„±ëœ ì²­í¬: {processed}ê°œ")
    log.info(f"   - í‰ê·  ì²­í¬/ë¬¸ì„œ: {processed/len(raw_docs):.1f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()