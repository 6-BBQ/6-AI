"""
ë¶„ë¦¬ëœ RAG ì„œë¹„ìŠ¤ - êµ¬ì¡°í™”ëœ ë²„ì „
"""
from __future__ import annotations
import os
import time
from typing import Dict, List, Optional, Any
from pathlib import Path
from dotenv import load_dotenv
import torch

# LLM & ì„ë² ë”©
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Google Gemini SDK for grounding
from google import genai

# ê²€ìƒ‰ ê´€ë ¨
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.prompts import PromptTemplate

# ë¶„ë¦¬ëœ ìœ í‹¸ë¦¬í‹°ë“¤
from .cache_utils import CacheManager
from .text_utils import TextProcessor
from .retrievers import MetadataAwareRetriever
from .search_factory import SearcherFactory

load_dotenv()


class StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    # --- ìƒìˆ˜ ì •ì˜ ---
    CACHE_DIR_NAME = "cache"
    VECTOR_DB_DIR = "vector_db/chroma"
    EMBED_MODEL_NAME = "dragonkue/bge-m3-ko"
    BM25_CACHE_FILE = "bm25_retriever.pkl"
    CROSS_ENCODER_CACHE_FILE = "cross_encoder.pkl"
    LLM_MODEL_NAME = "gemini-2.5-pro-preview-05-06"
    CROSS_ENCODER_MODEL_HF = "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"
    
    # ê·¸ë¼ìš´ë”© í™œì„±í™” ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
    ENABLE_WEB_GROUNDING = os.getenv("ENABLE_WEB_GROUNDING", "true").lower() == "true"

    CACHE_EXPIRY_SHORT = 60 * 60 * 12  # 12ì‹œê°„
    CACHE_EXPIRY_LONG = 60 * 60 * 24   # 24ì‹œê°„

    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self._setup_environment()
        self._initialize_utilities()
        self._initialize_core_components()
        self._initialize_retrievers()
        self._setup_llm_and_prompt()

    def _setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        self.cache_dir = Path(self.CACHE_DIR_NAME)
        self.cache_dir.mkdir(exist_ok=True)
        
        # API í‚¤ ì„¤ì •
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        
        if not self.gemini_api_key:
            raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        print("âœ… API í‚¤ í™•ì¸ ì™„ë£Œ - Gemini LLM + ì„ë² ë”© ì‚¬ìš©")

    def _initialize_utilities(self):
        """ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        self.cache_manager = CacheManager(self.cache_dir, self.CACHE_EXPIRY_SHORT, self.CACHE_EXPIRY_LONG)
        self.text_processor = TextProcessor()
        self.search_factory = SearcherFactory()

    def _initialize_core_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        print("ğŸš€ RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        
        # Groundingì„ ìœ„í•œ Google SDK ì´ˆê¸°í™”
        print("Google GenAI SDK ì‚¬ìš© - ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì§€ì›")
        self.genai_client = genai.Client(api_key=self.gemini_api_key)
        
        # ê·¸ë¼ìš´ë”© í™œì„±í™” ì—¬ë¶€ ì„¤ì • (True: í™œì„±í™”, False: ë¹„í™œì„±í™”)
        self.enable_grounding = self.ENABLE_WEB_GROUNDING
        
        # ì„ë² ë”© í•¨ìˆ˜ ë³€ê²½ (í•œêµ­ì–´ ì„±ëŠ¥ í–¥ìƒ)
        print("âœ… ì„ë² ë”© ì‚¬ìš© - í•œêµ­ì–´ ì„±ëŠ¥ ìµœì í™”")
        self.embedding_fn = HuggingFaceEmbeddings(
            model_name=self.EMBED_MODEL_NAME,
            model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
            encode_kwargs={"normalize_embeddings": True}  # BGE ì‹œë¦¬ì¦ˆëŠ” ë³´í†µ ì •ê·œí™” í•„ìš”
        )
        
        self.vectordb = Chroma(
            persist_directory=self.VECTOR_DB_DIR,
            embedding_function=self.embedding_fn
        )
        
        print("âœ… í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_retrievers(self):
        """ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        print("ğŸ”„ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • (ê²€ìƒ‰ ê°œìˆ˜ ëŒ€í­ ì¦ê°€)
        self.vector_retriever = self.vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 40, "fetch_k": 120, "lambda_mult": 0.5},
        )
        
        # BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ ì‚¬ìš©)
        self.bm25_retriever = self._get_bm25_retriever()
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„± - ê¸°ë³¸ ì„¤ì •
        # ë™ì  ê°€ì¤‘ì¹˜ëŠ” rag_searchì—ì„œ ì²˜ë¦¬
        self.rrf_retriever = None  # ë‚˜ì¤‘ì— ë™ì ìœ¼ë¡œ ìƒì„±
        
        # CrossEncoder ëª¨ë¸ë§Œ ë¯¸ë¦¬ ë¡œë“œ
        self.cross_encoder_model = self._get_cross_encoder_model()
        
        # internal_retrieverëŠ” rag_searchì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
        self.internal_retriever = None
        
        elapsed_time = time.time() - start_time
        print(f"ğŸ‰ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

    def _setup_llm_and_prompt(self):
        """LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë˜íŒŒ ì „ë¬¸ê°€ ë²„ì „)"""
        self.prompt = PromptTemplate(
            input_variables=["internal_context", "question", "character_info", "conversation_history"],
            template="""
ë‹¹ì‹ ì€ "ì•„ë¼ë“œ ìµœê³ ì˜ ê³µëµ ë„¤ë¹„ê²Œì´í„°" AI ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë˜ì „ì•¤íŒŒì´í„° ìºë¦­í„° ì„±ì¥ ë° ê²Œì„ í”Œë ˆì´ì— í•„ìš”í•œ ì¢…í•© ê³µëµ, ìµœì‹  ì •ë³´, ë§ì¶¤í˜• ê°€ì´ë“œë¥¼ ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.

â€» ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ì •ë³´ì™€ ì§€ì¹¨ì— ë”°ë¼ì„œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

[ìºë¦­í„° ì •ë³´]
{character_info}

[ì´ì „ ëŒ€í™” ê¸°ë¡]
{conversation_history}

[ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤]
{internal_context}

[ì •ë³´ í™œìš© ì „ëµ ë° ìš°ì„ ìˆœìœ„]
1.  **ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ({internal_context}) ìš°ì„  í™œìš©:** ìµœì‹  ì •ë³´ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì¶©ëŒ ì‹œ ìµœì‹  ì •ë³´ë§Œ ì±„íƒ (ë²„ì „/ë‚ ì§œ ê¸°ì¤€).
2.  **ì›¹ ê²€ìƒ‰ (Web Grounding) í™œìš©:**
    *   **í™œìš© ì¡°ê±´:** ë‚´ë¶€ ì •ë³´ ë¶€ì¬/ë¶€ì¡±, ìµœì‹  ì—…ë°ì´íŠ¸ í•„ìš” ì‹œ (íŒ¨ì¹˜, ì´ë²¤íŠ¸, ì‹œì„¸ ë“±), ì‚¬ìš©ì ëª…ì‹œì  ìµœì‹  ì •ë³´ ìš”êµ¬ ì‹œ.
    *   **ê²€ìƒ‰ ì§€ì¹¨:**
        *   **ìµœì‹ ì„± í™•ë³´:** ê²€ìƒ‰ ì‹œ "2025ë…„" ì´í›„ ë‚ ì§œ, í˜„ì¬ ì‹œì¦Œëª…, ìµœì‹  íŒ¨ì¹˜ëª…ì„ í‚¤ì›Œë“œì— í¬í•¨ (ì˜ˆ: "2025 [ì§ì—…] ìŠ¤í‚¬íŠ¸ë¦¬", "ìµœì‹  ë˜íŒŒ ì •ë³´").
        *   **ì¶œì²˜ ë° ê²€ì¦:** ê³µì‹ ì›¹ì‚¬ì´íŠ¸, ì£¼ìš” ì»¤ë®¤ë‹ˆí‹°/ê³µëµ ì‚¬ì´íŠ¸, ê²Œì„ ë§¤ì²´ë¥¼ ì°¸ê³ . ê°ê´€ì ì´ê³  ê²€ì¦ëœ ì •ë³´ë¥¼ ì„ íƒí•˜ë©°, ì»¤ë®¤ë‹ˆí‹° ì •ë³´ëŠ” êµì°¨ í™•ì¸ìœ¼ë¡œ ì‹ ë¢°ë„ í™•ë³´.
    *   **ì •ë³´ í†µí•©:** ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë‚´ë¶€ ì •ë³´ì™€ êµì°¨ ê²€ì¦ í›„, ìµœì‹ ì˜ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ë†’ì€ ì •ë³´ë¥¼ ì‚¬ìš©. í•µì‹¬ë§Œ ìš”ì•½ ì „ë‹¬.
3.  **ì •ë³´ ë¶€ì¬ ì‹œ ì²˜ë¦¬:** ë‚´ë¶€/ì›¹ ê²€ìƒ‰ìœ¼ë¡œë„ ì •ë³´ í™•ì¸ ë¶ˆê°€ ì‹œ, "ìš”ì²­í•˜ì‹  ì •ë³´ í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."ë¡œ ì•ˆë‚´.

[ë‹µë³€ ìƒì„± ê·œì¹™]
1.  **í˜ë¥´ì†Œë‚˜ ë° ì–´íˆ¬:** í•­ìƒ "ì•„ë¼ë“œ ìµœê³ ì˜ ê³µëµ ë„¤ë¹„ê²Œì´í„°"ë¡œì„œ ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆëŠ” ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ë©°, ì¹œì ˆí•˜ê³  ëª…ë£Œí•˜ê²Œ, ì‚¬ìš©ì ì´í•´ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì„¤ëª….
2.  **ë‹µë³€ ë²”ìœ„ ë° í˜•ì‹:** ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ì¶° ìºë¦­í„° ì„±ì¥(ìŠ¤í™ì—…, ì•„ì´í…œ, ì¬í™”), ì½˜í…ì¸  ê³µëµ, ê²Œì„ ì‹œìŠ¤í…œ, ì§ì—… ì •ë³´, ìµœì‹  ì†Œì‹(ì—…ë°ì´íŠ¸, ì´ë²¤íŠ¸) ë“± ë˜íŒŒ ê´€ë ¨ ê´‘ë²”ìœ„í•œ ì£¼ì œì— ëŒ€í•´ ë‹µë³€. ì§ˆë¬¸ ë²”ìœ„ ë‚´ì—ì„œ í•µì‹¬ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ, í•„ìš”ì‹œ ë‹¨ê³„ë³„/ìˆœì„œëŒ€ë¡œ ì„¤ëª….
3.  **ì½˜í…ì¸  ê´€ë ¨ ë‹µë³€:**
    *   'ëª…ì„±' ê¸°ì¤€ ë‹µë³€, 'ê¶Œì¥ ëª…ì„±' ìš°ì„ .
    *   'ëª…ì„±'ê³¼ 'ë”œì»·(ë˜ë‹´ë”œ)'ì€ ë³„ê°œì„ì„ ì¸ì§€.
    *   ì»¤ë®¤ë‹ˆí‹° 'ë˜ë‹´ì»·' í˜•ì‹(ì˜ˆ: "30ì–µ/400ë§Œ" - ë”œëŸ¬ ë°ë¯¸ì§€/ë²„í¼ ë²„í”„ë ¥)ì„ ì´í•´í•˜ê³  ë§¥ë½ì— ë§ì¶° ì°¸ê³ . ì´ëŠ” ëª…ì„±ê³¼ ë‹¤ë¥¸ ë³´ì¡° ì§€í‘œì´ë©°, ìˆ˜ì¹˜ëŠ” ë§¥ë½ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.
    *   ë‚¨ì/ì—¬ì ì§ì—…, ì „ì§ ë“± ëª…í™•íˆ êµ¬ë¶„.
4.  **ì´ë²¤íŠ¸ ì•ˆë‚´ ê¸°ì¤€:** ì¢…ë£Œ ì‹œ "ì¢…ë£Œë¨". ì¢…ë£Œì¼ ë¯¸í™•ì¸ ì‹œ "ì¢…ë£Œì¼ ë¯¸í™•ì¸, ê³µì‹ í™ˆí˜ì´ì§€ í™•ì¸ ìš”ë§". ì§„í–‰ ì¤‘ì´ë©´ ê¸°ê°„/ë³´ìƒ/ì°¸ì—¬ ë°©ë²• ì•ˆë‚´.

[ì œí•œ ì‚¬í•­ ë° ê¸ˆì§€ ì‚¬í•­]
*   ë˜íŒŒ ì™¸ ì§ˆë¬¸ ê±°ë¶€.
*   ì œê³µëœ ì •ë³´ ì™¸ ì‚¬ìš© ë° í™˜ê° ê¸ˆì§€.
*   ì¶”ì¸¡/ë¶ˆí™•ì‹¤ ì •ë³´ ê¸°ë°˜ ë‹µë³€ ê¸ˆì§€.
*   ì£¼ê´€ì  ì˜ê²¬/íŒë‹¨ ë°°ì œ.

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€ - ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ]
"""
        )
        print("âœ… LLM í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ")

    def _get_bm25_retriever(self):
        """BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            docs_for_bm25 = self.search_factory.create_bm25_data_from_vectordb(self.vectordb)
            return self.search_factory.create_bm25_retriever(docs_for_bm25)
        
        return self.cache_manager.load_or_create_cached_item(
            self.BM25_CACHE_FILE, creation_func, self.CACHE_EXPIRY_SHORT, "BM25 Retriever"
        )

    def _get_cross_encoder_model(self):
        """CrossEncoder ëª¨ë¸ ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            return self.search_factory.create_cross_encoder_model(self.CROSS_ENCODER_MODEL_HF)
        
        return self.cache_manager.load_or_create_cached_item(
            self.CROSS_ENCODER_CACHE_FILE, creation_func, self.CACHE_EXPIRY_LONG, "CrossEncoder ëª¨ë¸"
        )
    
    def _determine_weights(self, query: str, character_info: Optional[Dict]) -> List[float]:
        """ì¿¼ë¦¬ì™€ ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê²°ì •"""
        query_lower = query.lower()

        # ê¸°ë³¸ê°’: BM25 ìš°ì„  (ì§ì—…ë„, ìºë¦­í„° ì •ë³´ë„ ì´ë¯¸ ê°–ê³  ìˆìŒ)
        weights = [0.3, 0.7]

        # â€œìµœì‹ Â·ì—…ë°ì´íŠ¸â€ ë¥˜ í‚¤ì›Œë¦¬ë©´ ë²¡í„° ê°€ì¤‘ì¹˜ë¡œ ìŠ¤ì™‘
        if any(k in query_lower for k in ["ìµœì‹ ", "ì—…ë°ì´íŠ¸", "í˜„ì¬", "íŒ¨ì¹˜", "ì¢…ê²°"]):
            weights = [0.7, 0.3]
            print("ğŸ”„ ìµœì‹ Â·íŒ¨ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€ â†’ ë²¡í„° ê°€ì¤‘ì¹˜ ì¦ê°€")

        return weights
    
    def _build_conversation_context_for_llm(self, conversation_history: Optional[List[Dict]]) -> str:
        """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if not conversation_history or len(conversation_history) == 0:
            return "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, message in enumerate(conversation_history, 1):
            role = message.get('role', 'unknown')
            content = message.get('content', '')
            
            if role == 'user':
                context_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸ {i//2 + 1}: {content}")
            elif role == 'assistant':
                context_parts.append(f"ì´ì „ ë‹µë³€ {i//2 + 1}: {content}")
        
        return "\n".join(context_parts) if context_parts else "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

    def rag_search(self, query: str, character_info: Optional[Dict]) -> Dict[str, Any]:
        # ìºì‹œ í™•ì¸
        cached_result = self.cache_manager.get_cached_search_result(query, 'rag_search', character_info)
        if cached_result:
            print("ğŸ”„ ìºì‹œëœ RAG ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        search_start_time = time.time()
        # ìºë¦­í„° ì •ë³´ë¡œ ì¿¼ë¦¬ ê°•í™”
        enhanced_query = self.text_processor.enhance_query_with_character(query, character_info)
        times = {"internal_search": 0.0}
        
        # ë™ì  ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = self._determine_weights(query, character_info)
        print(f"ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜: ë²¡í„°={weights[0]:.2f}, BM25={weights[1]:.2f}")
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ë™ì  ìƒì„±
        self.rrf_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=weights,
        )
        
        # CrossEncoder ì¬ë­í‚¹ ì¶”ê°€
        compressor = CrossEncoderReranker(model=self.cross_encoder_model, top_n=60)
        base_retriever = ContextualCompressionRetriever(
            base_retriever=self.rrf_retriever,
            base_compressor=compressor,
        )
        
        # ë©”íƒ€ë°ì´í„° ì¸ì‹ ê²€ìƒ‰ê¸°ë¡œ ë˜í•‘
        self.internal_retriever = MetadataAwareRetriever(base_retriever)

        def _search_internal():
            start = time.time()
            try:
                print("ğŸ”„ ë‚´ë¶€ RAG ê²€ìƒ‰ ì‹œì‘...")
                docs = self.internal_retriever.get_relevant_documents(enhanced_query)
                times["internal_search"] = time.time() - start
                print(f"âœ… ë‚´ë¶€ RAG ê²€ìƒ‰ ì™„ë£Œ: {times['internal_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["internal_search"] = time.time() - start
                print(f"âŒ ë‚´ë¶€ RAG ê²€ìƒ‰ ì˜¤ë¥˜ ({times['internal_search']:.2f}ì´ˆ): {e}")
                return []

        internal_docs = _search_internal()
        
        times["internal_search"] = time.time() - search_start_time
        print(f"ğŸ¯ ë‚´ë¶€ ê²€ìƒ‰ ì™„ë£Œ - ì´ {times['internal_search']:.2f}ì´ˆ")

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
        internal_context_str = self.text_processor.format_docs_to_context_string(internal_docs, "ë‚´ë¶€")
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "internal_docs": internal_docs,
            "internal_context_provided_to_llm": internal_context_str,
            "enhanced_query": enhanced_query,
            "search_times": times
        }
        
        # ìºì‹œì— ì €ì¥
        self.cache_manager.save_search_result_to_cache(query, result, 'rag_search', character_info)
        return result

    def get_answer(self, query: str, character_info: Optional[Dict] = None, conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """RAG ë‹µë³€ ìƒì„± (ë©”ì¸ API)"""
        total_start_time = time.time()
        
        print(f"\n[INFO] ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: \"{query}\"")
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ë¡œê·¸ ì¶œë ¥
        if conversation_history and len(conversation_history) > 0:
            print(f"[INFO] ì´ì „ ëŒ€í™” ê¸°ë¡: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        else:
            print("[INFO] ì´ì „ ëŒ€í™” ê¸°ë¡ ì—†ìŒ")

        # ìºë¦­í„° ì •ë³´ë¥¼ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        char_context_for_llm = self.text_processor.build_character_context_for_llm(character_info)
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        conversation_context_for_llm = self._build_conversation_context_for_llm(conversation_history)
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.rag_search(query, character_info)
        
        # LLM ë‹µë³€ ìƒì„±
        llm_start_time = time.time()
        print("ğŸ”„ LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        formatted_prompt = self.prompt.format(
            internal_context=search_results["internal_context_provided_to_llm"],
            question=query,
            character_info=char_context_for_llm,
            conversation_history=conversation_context_for_llm
        )
        
        try:
            from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
            
            # ê·¸ë¼ìš´ë”© ë„êµ¬ ì„¤ì •
            tools = []
            if self.enable_grounding:
                google_search_tool = Tool(
                    google_search = GoogleSearch()
                )
                tools.append(google_search_tool)
                print("ğŸ” ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© í™œì„±í™”ë¨")
            else:
                print("ğŸš« ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ë¹„í™œì„±í™”ë¨")
            
            # LLM í˜¸ì¶œ
            response = self.genai_client.models.generate_content(
                model=self.LLM_MODEL_NAME,
                contents=formatted_prompt,
                config=GenerateContentConfig(
                    tools=tools,
                    temperature=0,
                )
            )
            
            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            llm_response = ""
            for part in response.candidates[0].content.parts:
                if part.text:
                    llm_response += part.text
            
            # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° í™•ì¸
            if self.enable_grounding and hasattr(response.candidates[0], 'grounding_metadata'):
                grounding = response.candidates[0].grounding_metadata
                if hasattr(grounding, 'search_entry_point') and grounding.search_entry_point:
                    print("ğŸŒ ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”©ì´ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    # ê²€ìƒ‰ëœ ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    if grounding.search_entry_point.rendered_content:
                        print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {grounding.search_entry_point.rendered_content[:200]}...")
        except Exception as e:
            print(f"âŒ LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            print(f"ìƒì„¸ ì—ëŸ¬: {str(e)}")
            print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            llm_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        llm_elapsed_time = time.time() - llm_start_time
        total_elapsed_time = time.time() - total_start_time
        
        print(f"âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ ({llm_elapsed_time:.2f}ì´ˆ)")
        print(f"[INFO] ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        
        # ìƒì„±ëœ ë‹µë³€ ì¶œë ¥
        print("\n" + "="*50)
        print("[ë‹µë³€]")
        print("="*50)
        print(llm_response)
        print("="*50 + "\n")
        
        # FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í‚¤ë¡œ ë°˜í™˜ê°’ êµ¬ì„±
        return {
            "result": llm_response,
            "internal_docs": search_results["internal_docs"],
            "enhanced_query": search_results["enhanced_query"],
            "execution_times": {
                "total": total_elapsed_time,
                "llm": llm_elapsed_time,
                "search": search_results["search_times"]
            },
            "internal_context": search_results["internal_context_provided_to_llm"],
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_structured_rag_service_instance: Optional[StructuredRAGService] = None

def get_structured_rag_service() -> StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _structured_rag_service_instance
    if _structured_rag_service_instance is None:
        print("âœ¨ ìƒˆë¡œìš´ StructuredRAGService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± âœ¨")
        _structured_rag_service_instance = StructuredRAGService()
    return _structured_rag_service_instance

def get_structured_rag_answer(query: str, character_info: Optional[Dict] = None, conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜"""
    service = get_structured_rag_service()
    return service.get_answer(query, character_info, conversation_history)
