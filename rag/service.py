"""
ë¶„ë¦¬ëœ RAG ì„œë¹„ìŠ¤ - êµ¬ì¡°í™”ëœ ë²„ì „
"""
from __future__ import annotations
import os
import time
from typing import Dict, List, Optional, Any
from pathlib import Path

# LLM & ì„ë² ë”©
from langchain_chroma import Chroma

# Google Gemini SDK for grounding
from google import genai

# ê²€ìƒ‰ ê´€ë ¨
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.prompts import PromptTemplate

# ë¶„ë¦¬ëœ ìœ í‹¸ë¦¬í‹°ë“¤
from .cache_utils import CacheManager
from .text_utils import TextProcessor
from .retrievers import MetadataAwareRetriever
from .search_factory import SearcherFactory
from utils import get_logger
from config import config  # ì¤‘ì•™í™”ëœ ì„¤ì • ì‚¬ìš©


class StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.logger = get_logger(__name__)
        self.logger.info("=== RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘ ===")
        
        # ì„¤ì •ê°’ë“¤ì„ configì—ì„œ ê°€ì ¸ì˜¤ê¸°
        self.cache_dir = Path(config.CACHE_DIR)
        self.vector_db_dir = config.VECTOR_DB_DIR
        self.embed_model_name = config.EMBED_MODEL_NAME
        self.cross_encoder_model_hf = config.CROSS_ENCODER_MODEL
        self.llm_model_name = config.LLM_MODEL_NAME
        self.enable_web_grounding = config.ENABLE_WEB_GROUNDING
        self.cache_expiry_short = config.CACHE_EXPIRY_SHORT
        self.cache_expiry_long = config.CACHE_EXPIRY_LONG
        
        # ìºì‹œ íŒŒì¼ëª…ë“¤
        self.bm25_cache_file = "bm25_retriever.pkl"
        self.cross_encoder_cache_file = "cross_encoder.pkl"
        
        start_time = time.time()
        
        self._setup_environment()
        self._initialize_utilities()
        self._initialize_core_components()
        self._initialize_retrievers()
        self._setup_llm_and_prompt()
        
        total_time = time.time() - start_time
        self.logger.info(f"=== RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ ({total_time:.2f}ì´ˆ) ===")

    def _setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        self.logger.debug("í™˜ê²½ ì„¤ì • ì‹œì‘")
        
        self.cache_dir.mkdir(exist_ok=True)
        self.logger.debug(f"ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •: {self.cache_dir}")
        
        # API í‚¤ ì„¤ì •
        self.gemini_api_key = config.GEMINI_API_KEY
        
        if not self.gemini_api_key:
            self.logger.error("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        self.logger.info("âœ… API í‚¤ í™•ì¸ ì™„ë£Œ - Gemini LLM + ì„ë² ë”© ì‚¬ìš©")

    def _initialize_utilities(self):
        """ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        self.logger.debug("ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹œì‘")
        
        self.cache_manager = CacheManager(
            self.cache_dir, 
            self.cache_expiry_short, 
            self.cache_expiry_long
        )
        self.text_processor = TextProcessor()
        self.search_factory = SearcherFactory()
        
        self.logger.debug("ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_core_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        self.logger.info("ğŸš€ RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        
        # Groundingì„ ìœ„í•œ Google SDK ì´ˆê¸°í™”
        self.logger.info("Google GenAI SDK ì‚¬ìš© - ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì§€ì›")
        try:
            self.genai_client = genai.Client(api_key=self.gemini_api_key)
            self.logger.debug("Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            self.logger.error(f"Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # ê·¸ë¼ìš´ë”© í™œì„±í™” ì—¬ë¶€ ì„¤ì •
        self.logger.info(f"ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”©: {'ON' if self.enable_web_grounding else 'OFF'}")
        
        # ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” (config ì„¤ì •ì— ë”°ë¼ ë™ì  ìƒì„±)
        embedding_type = config.EMBEDDING_TYPE
        self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embed_model_name} (íƒ€ì…: {embedding_type})")
        
        try:
            self.embedding_fn = config.create_embedding_function()
            self.logger.debug(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ ({embedding_type})")
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        # ë²¡í„° DB ì´ˆê¸°í™”
        try:
            self.vectordb = Chroma(
                persist_directory=self.vector_db_dir,
                embedding_function=self.embedding_fn
            )
            self.logger.info(f"ë²¡í„° DB ì—°ê²° ì„±ê³µ: {self.vector_db_dir}")
        except Exception as e:
            self.logger.error(f"ë²¡í„° DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

        self.logger.info(f"LLM ëª¨ë¸ ì„¤ì •: {self.llm_model_name}")
        
        self.logger.info("âœ… í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_retrievers(self):
        """ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        self.logger.info("ğŸ”„ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • (ê²€ìƒ‰ ê°œìˆ˜ ëŒ€í­ ì¦ê°€)
        self.vector_retriever = self.vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 40, "fetch_k": 120, "lambda_mult": 0.5},
        )
        self.logger.debug("ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ")
        
        # BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ ì‚¬ìš©)
        self.bm25_retriever = self._get_bm25_retriever()
        self.logger.debug("BM25 ê²€ìƒ‰ê¸° ë¡œë“œ ì™„ë£Œ")
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„± - ê¸°ë³¸ ì„¤ì •
        # ë™ì  ê°€ì¤‘ì¹˜ëŠ” rag_searchì—ì„œ ì²˜ë¦¬
        self.rrf_retriever = None  # ë‚˜ì¤‘ì— ë™ì ìœ¼ë¡œ ìƒì„±
        
        # CrossEncoder ëª¨ë¸ë§Œ ë¯¸ë¦¬ ë¡œë“œ
        self.cross_encoder_model = self._get_cross_encoder_model()
        self.logger.debug("CrossEncoder ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # internal_retrieverëŠ” rag_searchì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
        self.internal_retriever = None
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

    def _setup_llm_and_prompt(self):
        """LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë˜íŒŒ ì „ë¬¸ê°€ ë²„ì „)"""
        self.prompt = PromptTemplate(
            input_variables=["internal_context", "question", "character_info", "conversation_history"],
            template="""
ë‹¹ì‹ ì€ 'RPGPT ë˜íŒŒ'ì˜ "ë˜íŒŒ ìµœê³ ì˜ ë„¤ë¹„ê²Œì´í„°" AI ì±—ë´‡ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ í•µì‹¬ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ë˜ì „ì•¤íŒŒì´í„° ìºë¦­í„° ì„±ì¥ ë° ê²Œì„ í”Œë ˆì´ì— ê´€í•œ ì§ˆë¬¸ì— ëŒ€í•´, ê°€ì¥ ì •í™•í•˜ê³  íš¨ìœ¨ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì¤‘ìš” ì§€ì¹¨: ë°˜ë“œì‹œ ë‹¤ìŒ ëª¨ë“  ì§€ì¹¨ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.]

1.  **ì •ë³´ ì²˜ë¦¬ ë° í™œìš© ì „ëµ:**
    *   **ì •ë³´ ì¶œì²˜ ìš°ì„ ìˆœìœ„:**
        1.  **[ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼] ({internal_context}):** ì œê³µëœ ë‚´ë¶€ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì •ë³´ ê°„ ì¶©ëŒì´ ë°œìƒí•  ê²½ìš°, ê°€ì¥ ìµœì‹  ë²„ì „(ë²„ì „ ì •ë³´ ë˜ëŠ” ë‚ ì§œ ê¸°ì¤€)ì˜ ì •ë³´ë¥¼ ì±„íƒí•©ë‹ˆë‹¤.
        2.  **[ì›¹ ê²€ìƒ‰ (Grounding)]:** ë‚´ë¶€ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•  ë•Œ, ìµœì‹  ì •ë³´(ì˜ˆ: ê¸´ê¸‰/ì£¼ê°„ íŒ¨ì¹˜ ë…¸íŠ¸, ì‹ ê·œ ì´ë²¤íŠ¸, ì•„ì´í…œ ì‹œì„¸ ë³€ë™ ë“±)ê°€ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë  ë•Œ, ë˜ëŠ” ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìµœì‹  ì •ë³´ë¥¼ ìš”êµ¬í•  ê²½ìš°ì—ë§Œ í™œìš©í•©ë‹ˆë‹¤.
            *   **ì›¹ ê²€ìƒ‰ ì‹œ:** "2025ë…„", "ìµœì‹ ", "[í˜„ì¬ ì‹œì¦Œëª…]", "[ìµœì‹  íŒ¨ì¹˜ëª…]" ë“±ì˜ í‚¤ì›Œë“œë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ì •ë³´ì˜ ìµœì‹ ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤. (ì˜ˆ: "2025ë…„ ë¸”ë ˆì´ë“œ ìŠ¤í‚¬íŠ¸ë¦¬", "ë˜íŒŒ ìµœì‹  ì´ë²¤íŠ¸ ëª©ë¡")
            *   **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜:** ê³µì‹ í™ˆí˜ì´ì§€(df.nexon.com), ì£¼ìš” ì»¤ë®¤ë‹ˆí‹°(ì˜ˆ: ë””ì‹œ ë˜íŒŒIPê°¤, ì•„ì¹´ë¼ì´ë¸Œ ë˜íŒŒì±„ë„), ê³µì¸ëœ ê³µëµ ì‚¬ì´íŠ¸, ê²Œì„ ì „ë¬¸ ë§¤ì²´ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•©ë‹ˆë‹¤. ì»¤ë®¤ë‹ˆí‹° ì •ë³´ëŠ” êµì°¨ í™•ì¸ì„ í†µí•´ ì‹ ë¢°ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
            *   **ì •ë³´ í†µí•©:** ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë‚´ë¶€ ì •ë³´ì™€ ë¹„êµ/ê²€ì¦ í›„, ê°€ì¥ ì •í™•í•˜ê³  ìµœì‹ ì´ë©° ì‹ ë¢°ë„ ë†’ì€ ì •ë³´ë¥¼ ì„ íƒí•˜ì—¬ ë‹µë³€ì— ë°˜ì˜í•©ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì „ë‹¬í•©ë‹ˆë‹¤.
    *   **[ìºë¦­í„° ì •ë³´] ({character_info}) í™œìš©:**
        *   ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ìºë¦­í„°ì˜ ì„±ì¥, ìŠ¤í™(ì¥ë¹„, ìŠ¤í‚¬, ì•„ë°”íƒ€ ë“±), íŠ¹ì • ì½˜í…ì¸  ê³µëµ ë“± **ìºë¦­í„°ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ê²½ìš°**, ì œê³µëœ ìºë¦­í„° ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ë§ì¶¤í˜• ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
        *   ì§ˆë¬¸ì´ **ì¼ë°˜ì ì¸ ê²Œì„ ì •ë³´**(ì˜ˆ: ìµœì‹  ì—…ë°ì´íŠ¸ ë‚´ìš© ì „ë°˜, ëª¨ë“  ì§ì—… ê³µí†µ ì´ë²¤íŠ¸, íŠ¹ì • ì•„ì´í…œì˜ ì¼ë°˜ì ì¸ ì„±ëŠ¥ ë° íšë“ì²˜ ë“±)ì— ê´€í•œ ê²ƒì´ë¼ë©´, ìºë¦­í„° ì •ë³´ì— ì–½ë§¤ì´ì§€ ì•Šê³  ê°€ì¥ ì ì ˆí•˜ê³  ì¼ë°˜ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìºë¦­í„° ì •ë³´ê°€ ë‹µë³€ì— ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ë¬´ì‹œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
    *   **[ì´ì „ ëŒ€í™” ê¸°ë¡] ({conversation_history}):** ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ íŒŒì•…í•˜ëŠ” ë° ì°¸ê³ í•˜ë˜, í•­ìƒ í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ê°€ í˜„ì¬ ì§ˆë¬¸ê³¼ ë¬´ê´€í•˜ë‹¤ë©´ ê³ ë ¤í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.

2.  **ë‹µë³€ ìƒì„± ì›ì¹™:**
    *   **í˜ë¥´ì†Œë‚˜ ë° ì–´íˆ¬:** í•­ìƒ "ì•„ë¼ë“œ ìµœê³ ì˜ ê³µëµ ë„¤ë¹„ê²Œì´í„°"ë¡œì„œ, ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆëŠ” ì–´íˆ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¹œì ˆí•˜ê³  ëª…ë£Œí•˜ê²Œ, ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
    *   **ì •í™•ì„± ë° ì‹ ë¢°ì„±:** ë°˜ë“œì‹œ ê²€ì¦ëœ ì‚¬ì‹¤ê³¼ ì œê³µëœ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë©°, ì¶”ì¸¡ì´ë‚˜ ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    *   **í•µì‹¬ ì§‘ì¤‘ ë° ê°„ê²°ì„±:** ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³ , ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì—ë§Œ ì´ˆì ì„ ë§ì¶° ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
    *   **ì§ˆë¬¸ ë²”ìœ„ ì—„ìˆ˜ (ë§¤ìš° ì¤‘ìš”!):**
        *   **ì˜¤ì§ ì‚¬ìš©ìê°€ ì§ì ‘ì ìœ¼ë¡œ ì§ˆë¬¸í•œ ë‚´ìš©ì— ëŒ€í•´ì„œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.**
        *   ì§ˆë¬¸ì—ì„œ ë²—ì–´ë‚˜ëŠ” ë‚´ìš©, ì—°ê´€ì„±ì´ ë‚®ì€ ë¶€ê°€ ì •ë³´, ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•˜ì§€ ì•Šì€ ìƒì„¸ ì„¤ëª…ì´ë‚˜ íŒ ë“±ì€ **ì ˆëŒ€ ë¨¼ì € ì œê³µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**
        *   ì‚¬ìš©ìê°€ ì¶”ê°€ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ê²½ìš°ì—ë§Œ í•´ë‹¹ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        *   **ì˜ˆì‹œ:** ì‚¬ìš©ìê°€ "ì—˜ë¸ë‚˜ì´íŠ¸ì˜ 'ì²´ì¸ëŸ¬ì‰¬' ìŠ¤í‚¬ ë°ë¯¸ì§€ ê³„ìˆ˜ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"ë¼ê³  ë¬¼ì—ˆë‹¤ë©´, ì˜¤ì§ 'ì²´ì¸ëŸ¬ì‰¬' ìŠ¤í‚¬ì˜ ë°ë¯¸ì§€ ê³„ìˆ˜ ì •ë³´ë§Œ ë‹µë³€í•©ë‹ˆë‹¤. 'ì²´ì¸ëŸ¬ì‰¬' ìŠ¤í‚¬ì˜ ì—­ì‚¬, ë‹¤ë¥¸ í™œìš©ë²•, ë‹¤ë¥¸ ìŠ¤í‚¬ê³¼ì˜ ë¹„êµ, ì—˜ë¸ë‚˜ì´íŠ¸ì˜ ì „ë°˜ì ì¸ ìš´ì˜ë²• ë“±ì€ ì‚¬ìš©ìê°€ ë¨¼ì € ë¬»ì§€ ì•ŠëŠ” í•œ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

3.  **ë˜ì „ì•¤íŒŒì´í„° íŠ¹ì • ì§€ì‹ ë° ì£¼ì˜ì‚¬í•­:**
    *   **ëª…ì„± ì‹œìŠ¤í…œ:** ë‹µë³€ ì‹œ 'ëª…ì„±' ìˆ˜ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì½˜í…ì¸ ì˜ 'ê¶Œì¥ ëª…ì„±'ì„ ìš°ì„ ì ìœ¼ë¡œ ì–¸ê¸‰í•©ë‹ˆë‹¤. 'ëª…ì„±'ê³¼ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ í†µìš©ë˜ëŠ” 'ë˜ë‹´ (ë˜ì „ì•¤íŒŒì´í„° ë°ë¯¸ì§€ ë¶„ì„ ì‚¬ì´íŠ¸) ì»·'ì€ ë‹¤ë¥¸ ê°œë…ì„ì„ ëª…í™•íˆ ì¸ì§€í•˜ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    *   **ë˜ë‹´ì»· ì°¸ê³ :** ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì‚¬ìš©ë˜ëŠ” 'ë”œì»·(ë˜ë‹´ì»·)' í˜•ì‹(ì˜ˆ: "30/400"ì€ ë”œëŸ¬ 30ì–µ, ë²„í¼ 400ë§Œ)ì„ ì´í•´í•˜ê³ , í•„ìš”ì‹œ ë§¥ë½ì— ë§ê²Œ ì°¸ê³ í•  ìˆ˜ ìˆìœ¼ë‚˜, ì´ëŠ” ì ˆëŒ€ì ì¸ ê¸°ì¤€ì´ ì•„ë‹Œ ìœ ì €ë“¤ ì‚¬ì´ì˜ ì°¸ê³  ì§€í‘œì´ë©°, ì‹œê¸°ë‚˜ ìƒí™©ì— ë”°ë¼ ë³€ë™ë  ìˆ˜ ìˆìŒì„ ëª…ì‹¬í•©ë‹ˆë‹¤.
    *   **ì§ì—… êµ¬ë¶„:** ë‚¨ì/ì—¬ì ì§ì—…êµ°(ì˜ˆ: ë‚¨ë ˆì¸ì €/ì—¬ë ˆì¸ì €, ë‚¨ëŸ°ì²˜/ì—¬ëŸ°ì²˜), 1ì°¨/2ì°¨/ì§„ ê°ì„±ëª… ë“±ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ í˜¼ë™ ì—†ì´ ë‹µë³€í•©ë‹ˆë‹¤.
    *   **ë ˆì´ë“œ ì½˜í…ì¸  êµ¬ë¶„ (ë§¤ìš° ì¤‘ìš”!):**
        *   **"ì•ˆê°œì‹  ë ˆì´ë“œ" (ì •ì‹ ëª…ì¹­: ì•„ìŠ¤ë¼í•œ: ë¬´ì˜ ì¥ë§‰, 2024ë…„ ì¶œì‹œ ì½˜í…ì¸ )ì™€ "ë‚˜ë²¨ ë ˆì´ë“œ" (ì •ì‹ ëª…ì¹­: ë§Œë“¤ì–´ì§„ ì‹  ë‚˜ë²¨, 2025ë…„ ì¶œì‹œ ì½˜í…ì¸ )ëŠ” ì™„ì „íˆ ë³„ê°œì˜ ë ˆì´ë“œ ì½˜í…ì¸ ì…ë‹ˆë‹¤.**
        *   ì•ˆê°œì‹ ì˜ ë³¸ëª…ì´ 'ë‚˜ë²¨'ì´ë¼ëŠ” ì„¤ì •ì´ ìˆì§€ë§Œ, ê²Œì„ ë‚´ ì½˜í…ì¸ ë¡œì„œëŠ” ëª…í™•íˆ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. **ë‘ ë ˆì´ë“œë¥¼ ì ˆëŒ€ í˜¼ë™í•˜ì—¬ ì„¤ëª…í•˜ê±°ë‚˜ ì—°ê´€ ì§€ì–´ ë‹µë³€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**
    *   **ì´ë²¤íŠ¸ ì •ë³´ ì•ˆë‚´:**
        *   **ì§„í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸:** ì´ë²¤íŠ¸ ê¸°ê°„, ì£¼ìš” ë³´ìƒ, í•µì‹¬ ì°¸ì—¬ ë°©ë²•ì„ ëª…í™•íˆ ì•ˆë‚´í•©ë‹ˆë‹¤.
        *   **ì¢…ë£Œëœ ì´ë²¤íŠ¸:** "í•´ë‹¹ ì´ë²¤íŠ¸ëŠ” ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë‹µë³€í•©ë‹ˆë‹¤.
        *   **ì¢…ë£Œì¼ ë¯¸í™•ì¸ ì´ë²¤íŠ¸:** "í•´ë‹¹ ì´ë²¤íŠ¸ì˜ ì •í™•í•œ ì¢…ë£Œì¼ì´ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì‹  ì •ë³´ëŠ” ë˜ì „ì•¤íŒŒì´í„° ê³µì‹ í™ˆí˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."ë¼ê³  ì•ˆë‚´í•©ë‹ˆë‹¤.

4.  **ì—„ê²©í•œ ê¸ˆì§€ ì‚¬í•­:**
    *   ë˜ì „ì•¤íŒŒì´í„°ì™€ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "ë‹¤ë¥¸ ê²Œì„ ì¶”ì²œí•´ì¤˜" ë“±ì˜ ì§ˆë¬¸ì—ëŠ” "ì €ëŠ” ë˜ì „ì•¤íŒŒì´í„° ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•´ ë“œë¦´ ìˆ˜ ìˆì–´ìš”." ì™€ ê°™ì´ ì‘ë‹µí•©ë‹ˆë‹¤.)
    *   ì œê³µëœ [ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼] ë° [ì›¹ ê²€ìƒ‰ (Grounding)] ê²°ê³¼ ì´ì™¸ì˜ ì •ë³´ë¥¼ ì„ì˜ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš©ì„ ì¶”ì¸¡í•˜ì—¬ ê¾¸ë©°ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤ (í™˜ê° í˜„ìƒ ì ˆëŒ€ ê¸ˆì§€).
    *   ê°œì¸ì ì¸ ì˜ê²¬, ì£¼ê´€ì ì¸ íŒë‹¨, ì„ í˜¸ë„ë¥¼ ë°°ì œí•˜ê³  í•­ìƒ ê°ê´€ì ì¸ ì •ë³´ë§Œì„ ì „ë‹¬í•©ë‹ˆë‹¤.
    *   **ê°€ì¥ ì¤‘ìš”: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì— ëŒ€í•´ ì„ ì œì ìœ¼ë¡œ ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ì„¤ëª…ì„ í™•ì¥í•˜ëŠ” í–‰ìœ„ë¥¼ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤. í•­ìƒ ì§ˆë¬¸ì˜ ë²”ìœ„ ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.**

[ì…ë ¥ ì •ë³´]
ìºë¦­í„° ì •ë³´: {character_info}
ì´ì „ ëŒ€í™” ê¸°ë¡: {conversation_history}
ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: {internal_context}
ì‚¬ìš©ì ì§ˆë¬¸: {question}

[ë‹µë³€ - ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ì •í™•íˆ ë¶€í•©í•˜ë„ë¡, ìœ„ì˜ ëª¨ë“  ì§€ì¹¨ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì—¬ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±]
"""
        )
        self.logger.debug("âœ… LLM í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ")

    def _get_bm25_retriever(self):
        """BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            docs_for_bm25 = self.search_factory.create_bm25_data_from_vectordb(self.vectordb)
            return self.search_factory.create_bm25_retriever(docs_for_bm25)
        
        return self.cache_manager.load_or_create_cached_item(
            self.bm25_cache_file, creation_func, self.cache_expiry_short, "BM25 Retriever"
        )

    def _get_cross_encoder_model(self):
        """CrossEncoder ëª¨ë¸ ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            return self.search_factory.create_cross_encoder_model(self.cross_encoder_model_hf)
        
        return self.cache_manager.load_or_create_cached_item(
            self.cross_encoder_cache_file, creation_func, self.cache_expiry_long, "CrossEncoder ëª¨ë¸"
        )
    
    def _determine_weights(self, query: str, character_info: Optional[Dict]) -> List[float]:
        """ì¿¼ë¦¬ì™€ ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê²°ì •"""
        query_lower = query.lower()

        # ê¸°ë³¸ê°’: BM25 ìš°ì„  (ì§ì—…ë„, ìºë¦­í„° ì •ë³´ë„ ì´ë¯¸ ê°–ê³  ìˆìŒ)
        weights = [0.3, 0.7]

        # â€œìµœì‹ Â·ì—…ë°ì´íŠ¸â€ ë¥˜ í‚¤ì›Œë¦¬ë©´ ë²¡í„° ê°€ì¤‘ì¹˜ë¡œ ìŠ¤ì™‘
        if any(k in query_lower for k in ["ìµœì‹ ", "ì—…ë°ì´íŠ¸", "í˜„ì¬", "íŒ¨ì¹˜", "ì¢…ê²°"]):
            weights = [0.7, 0.3]
            self.logger.debug("ğŸ”„ ìµœì‹ Â·íŒ¨ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€ â†’ ë²¡í„° ê°€ì¤‘ì¹˜ ì¦ê°€")

        return weights
    
    def _build_conversation_context_for_llm(self, conversation_history: Optional[List[Dict]]) -> str:
        """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if not conversation_history or len(conversation_history) == 0:
            return "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, message in enumerate(conversation_history, 1):
            role = message.get('role', 'unknown')
            content = message.get('content', '')
            
            if role == 'user':
                context_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸ {i//2 + 1}: {content}")
            elif role == 'assistant':
                context_parts.append(f"ì´ì „ ë‹µë³€ {i//2 + 1}: {content}")
        
        return "\n".join(context_parts) if context_parts else "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

    def rag_search(self, query: str, character_info: Optional[Dict]) -> Dict[str, Any]:
        # ìºì‹œ í™•ì¸
        cached_result = self.cache_manager.get_cached_search_result(query, 'rag_search', character_info)
        if cached_result:
            self.logger.debug("ğŸ”„ ìºì‹œëœ RAG ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        search_start_time = time.time()
        # ìºë¦­í„° ì •ë³´ë¡œ ì¿¼ë¦¬ ê°•í™”
        enhanced_query = self.text_processor.enhance_query_with_character(query, character_info)
        times = {"internal_search": 0.0}
        
        # ë™ì  ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = self._determine_weights(query, character_info)
        self.logger.debug(f"ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜: ë²¡í„°={weights[0]:.2f}, BM25={weights[1]:.2f}")
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ë™ì  ìƒì„±
        self.rrf_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=weights,
        )
        
        # CrossEncoder ì¬ë­í‚¹ ì¶”ê°€
        compressor = CrossEncoderReranker(model=self.cross_encoder_model, top_n=60)
        base_retriever = ContextualCompressionRetriever(
            base_retriever=self.rrf_retriever,
            base_compressor=compressor,
        )
        
        # ë©”íƒ€ë°ì´í„° ì¸ì‹ ê²€ìƒ‰ê¸°ë¡œ ë˜í•‘
        self.internal_retriever = MetadataAwareRetriever(base_retriever)

        def _search_internal():
            start = time.time()
            try:
                self.logger.debug("ğŸ”„ ë‚´ë¶€ RAG ê²€ìƒ‰ ì‹œì‘...")
                docs = self.internal_retriever.get_relevant_documents(enhanced_query)
                times["internal_search"] = time.time() - start
                self.logger.info(f"âœ… ë‚´ë¶€ RAG ê²€ìƒ‰ ì™„ë£Œ: {times['internal_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["internal_search"] = time.time() - start
                self.logger.error(f"âŒ ë‚´ë¶€ RAG ê²€ìƒ‰ ì˜¤ë¥˜ ({times['internal_search']:.2f}ì´ˆ): {e}")
                return []

        internal_docs = _search_internal()
        
        times["internal_search"] = time.time() - search_start_time
        self.logger.debug(f"ğŸ¯ ë‚´ë¶€ ê²€ìƒ‰ ì™„ë£Œ - ì´ {times['internal_search']:.2f}ì´ˆ")

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
        internal_context_str = self.text_processor.format_docs_to_context_string(internal_docs, "ë‚´ë¶€")
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "internal_docs": internal_docs,
            "internal_context_provided_to_llm": internal_context_str,
            "enhanced_query": enhanced_query,
            "search_times": times
        }
        
        # ìºì‹œì— ì €ì¥
        self.cache_manager.save_search_result_to_cache(query, result, 'rag_search', character_info)
        return result

    def get_answer(self, query: str, character_info: Optional[Dict] = None, conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """RAG ë‹µë³€ ìƒì„± (ë©”ì¸ API)"""
        total_start_time = time.time()
        
        self.logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: \"{query}\"")
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ë¡œê·¸ ì¶œë ¥
        if conversation_history and len(conversation_history) > 0:
            self.logger.info(f"ì´ì „ ëŒ€í™” ê¸°ë¡: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        else:
            self.logger.info("ì´ì „ ëŒ€í™” ê¸°ë¡ ì—†ìŒ")

        # ìºë¦­í„° ì •ë³´ë¥¼ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        char_context_for_llm = self.text_processor.build_character_context_for_llm(character_info)
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        conversation_context_for_llm = self._build_conversation_context_for_llm(conversation_history)
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.rag_search(query, character_info)
        
        # LLM ë‹µë³€ ìƒì„±
        llm_start_time = time.time()
        self.logger.info("ğŸ”„ LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        formatted_prompt = self.prompt.format(
            internal_context=search_results["internal_context_provided_to_llm"],
            question=query,
            character_info=char_context_for_llm,
            conversation_history=conversation_context_for_llm
        )
        
        try:
            from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
            
            # ê·¸ë¼ìš´ë”© ë„êµ¬ ì„¤ì •
            tools = []
            if self.enable_web_grounding:
                google_search_tool = Tool(
                    google_search = GoogleSearch()
                )
                tools.append(google_search_tool)
                self.logger.debug("ğŸ” ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© í™œì„±í™”ë¨")
            else:
                self.logger.debug("ğŸš« ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ë¹„í™œì„±í™”ë¨")
            
            # LLM í˜¸ì¶œ
            response = self.genai_client.models.generate_content(
                model=self.llm_model_name,
                contents=formatted_prompt,
                config=GenerateContentConfig(
                    tools=tools,
                    temperature=0,
                )
            )
            
            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            llm_response = ""
            for part in response.candidates[0].content.parts:
                if part.text:
                    llm_response += part.text
            
            # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° í™•ì¸
            if self.enable_web_grounding and hasattr(response.candidates[0], 'grounding_metadata'):
                grounding = response.candidates[0].grounding_metadata
                if hasattr(grounding, 'search_entry_point') and grounding.search_entry_point:
                    self.logger.info("ğŸŒ ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”©ì´ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    # ê²€ìƒ‰ëœ ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    if grounding.search_entry_point.rendered_content:
                        self.logger.debug(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {grounding.search_entry_point.rendered_content[:200]}...")
        except Exception as e:
            self.logger.error(f"âŒ LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            self.logger.error(f"ìƒì„¸ ì—ëŸ¬: {str(e)}")
            self.logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            llm_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        llm_elapsed_time = time.time() - llm_start_time
        total_elapsed_time = time.time() - total_start_time
        
        self.logger.info(f"âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ ({llm_elapsed_time:.2f}ì´ˆ)")
        self.logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        
        # ìƒì„±ëœ ë‹µë³€ ì¶œë ¥ (ë””ë²„ê·¸ ë ˆë²¨ì—ì„œ)
        self.logger.debug("\n" + "="*50)
        self.logger.debug("[ë‹µë³€]")
        self.logger.debug("="*50)
        self.logger.debug(llm_response[:200] + "..." if len(llm_response) > 200 else llm_response)
        self.logger.debug("="*50)
        
        # FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í‚¤ë¡œ ë°˜í™˜ê°’ êµ¬ì„±
        return {
            "result": llm_response,
            "internal_docs": search_results["internal_docs"],
            "enhanced_query": search_results["enhanced_query"],
            "execution_times": {
                "total": total_elapsed_time,
                "llm": llm_elapsed_time,
                "search": search_results["search_times"]
            },
            "internal_context": search_results["internal_context_provided_to_llm"],
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_structured_rag_service_instance: Optional[StructuredRAGService] = None

def get_structured_rag_service() -> StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _structured_rag_service_instance
    if _structured_rag_service_instance is None:
        logger = get_logger(__name__)
        logger.info("âœ¨ ìƒˆë¡œìš´ StructuredRAGService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± âœ¨")
        _structured_rag_service_instance = StructuredRAGService()
    return _structured_rag_service_instance

def get_structured_rag_answer(query: str, character_info: Optional[Dict] = None, conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜"""
    service = get_structured_rag_service()
    return service.get_answer(query, character_info, conversation_history)
