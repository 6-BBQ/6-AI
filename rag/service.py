"""
ë¶„ë¦¬ëœ RAG ì„œë¹„ìŠ¤ - êµ¬ì¡°í™”ëœ ë²„ì „
"""
from __future__ import annotations
import os
import time
from typing import Dict, List, Optional, Any
from pathlib import Path

# LLM & ì„ë² ë”©
from langchain_chroma import Chroma

# Google Gemini SDK for grounding
from google import genai

# ê²€ìƒ‰ ê´€ë ¨
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.prompts import PromptTemplate

# ë¶„ë¦¬ëœ ìœ í‹¸ë¦¬í‹°ë“¤
from .cache_utils import CacheManager
from .text_utils import TextProcessor
from .retrievers import MetadataAwareRetriever
from .search_factory import SearcherFactory
from utils import get_logger
from config import config  # ì¤‘ì•™í™”ëœ ì„¤ì • ì‚¬ìš©


class StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.logger = get_logger(__name__)
        self.logger.info("=== RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘ ===")
        
        # ì„¤ì •ê°’ë“¤ì„ configì—ì„œ ê°€ì ¸ì˜¤ê¸°
        self.cache_dir = Path(config.CACHE_DIR)
        self.vector_db_dir = config.VECTOR_DB_DIR
        self.embed_model_name = config.EMBED_MODEL_NAME
        self.cross_encoder_model_hf = config.CROSS_ENCODER_MODEL
        self.llm_model_name = config.LLM_MODEL_NAME
        self.enable_web_grounding = config.ENABLE_WEB_GROUNDING
        self.cache_expiry_short = config.CACHE_EXPIRY_SHORT
        self.cache_expiry_long = config.CACHE_EXPIRY_LONG
        
        # ìºì‹œ íŒŒì¼ëª…ë“¤
        self.bm25_cache_file = "bm25_retriever.pkl"
        self.cross_encoder_cache_file = "cross_encoder.pkl"
        
        start_time = time.time()
        
        self._setup_environment()
        self._initialize_utilities()
        self._initialize_core_components()
        self._initialize_retrievers()
        self._setup_llm_and_prompt()
        
        total_time = time.time() - start_time
        self.logger.info(f"=== RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ ({total_time:.2f}ì´ˆ) ===")

    def _setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        self.logger.debug("í™˜ê²½ ì„¤ì • ì‹œì‘")
        
        self.cache_dir.mkdir(exist_ok=True)
        self.logger.debug(f"ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •: {self.cache_dir}")
        
        # API í‚¤ ì„¤ì •
        self.gemini_api_key = config.GEMINI_API_KEY
        
        if not self.gemini_api_key:
            self.logger.error("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        self.logger.info("âœ… API í‚¤ í™•ì¸ ì™„ë£Œ - Gemini LLM + ì„ë² ë”© ì‚¬ìš©")

    def _initialize_utilities(self):
        """ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        self.logger.debug("ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì‹œì‘")
        
        self.cache_manager = CacheManager(
            self.cache_dir, 
            self.cache_expiry_short, 
            self.cache_expiry_long
        )
        self.text_processor = TextProcessor()
        self.search_factory = SearcherFactory()
        
        self.logger.debug("ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_core_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        self.logger.info("ğŸš€ RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        
        # Groundingì„ ìœ„í•œ Google SDK ì´ˆê¸°í™”
        self.logger.info("Google GenAI SDK ì‚¬ìš© - ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì§€ì›")
        try:
            self.genai_client = genai.Client(api_key=self.gemini_api_key)
            self.logger.debug("Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            self.logger.error(f"Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # ê·¸ë¼ìš´ë”© í™œì„±í™” ì—¬ë¶€ ì„¤ì •
        self.logger.info(f"ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”©: {'ON' if self.enable_web_grounding else 'OFF'}")
        
        # ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” (config ì„¤ì •ì— ë”°ë¼ ë™ì  ìƒì„±)
        embedding_type = config.EMBEDDING_TYPE
        self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {self.embed_model_name} (íƒ€ì…: {embedding_type})")
        
        try:
            self.embedding_fn = config.create_embedding_function()
            self.logger.debug(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ ({embedding_type})")
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        # ë²¡í„° DB ì´ˆê¸°í™”
        try:
            self.vectordb = Chroma(
                persist_directory=self.vector_db_dir,
                embedding_function=self.embedding_fn
            )
            self.logger.info(f"ë²¡í„° DB ì—°ê²° ì„±ê³µ: {self.vector_db_dir}")
        except Exception as e:
            self.logger.error(f"ë²¡í„° DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

        self.logger.info(f"LLM ëª¨ë¸ ì„¤ì •: {self.llm_model_name}")
        
        self.logger.info("âœ… í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_retrievers(self):
        """ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        self.logger.info("ğŸ”„ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • (ê²€ìƒ‰ ê°œìˆ˜ ëŒ€í­ ì¦ê°€)
        self.vector_retriever = self.vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 40, "fetch_k": 120, "lambda_mult": 0.5},
        )
        self.logger.debug("ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ")
        
        # BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ ì‚¬ìš©)
        self.bm25_retriever = self._get_bm25_retriever()
        self.logger.debug("BM25 ê²€ìƒ‰ê¸° ë¡œë“œ ì™„ë£Œ")
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„± - ê¸°ë³¸ ì„¤ì •
        # ë™ì  ê°€ì¤‘ì¹˜ëŠ” rag_searchì—ì„œ ì²˜ë¦¬
        self.rrf_retriever = None  # ë‚˜ì¤‘ì— ë™ì ìœ¼ë¡œ ìƒì„±
        
        # CrossEncoder ëª¨ë¸ë§Œ ë¯¸ë¦¬ ë¡œë“œ
        self.cross_encoder_model = self._get_cross_encoder_model()
        self.logger.debug("CrossEncoder ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # internal_retrieverëŠ” rag_searchì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
        self.internal_retriever = None
        
        elapsed_time = time.time() - start_time
        self.logger.info(f"ğŸ‰ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

    def _setup_llm_and_prompt(self):
        """LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë˜íŒŒ ì „ë¬¸ê°€ ë²„ì „)"""
        self.prompt = PromptTemplate(
            input_variables=["internal_context", "question", "character_info", "conversation_history"],
            template="""
ë‹¹ì‹ ì€ 'rpgpt ë˜íŒŒ'ì˜ "ë˜íŒŒ ìµœê³ ì˜ ë„¤ë¹„ê²Œì´í„°" AI ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë˜ì „ì•¤íŒŒì´í„° ìºë¦­í„° ì„±ì¥ ë° ê²Œì„ í”Œë ˆì´ì— í•„ìš”í•œ ì§ˆë¬¸ì— ëŒ€í•´, ì •í™•í•˜ê³  íš¨ìœ¨ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ìµœìš°ì„  ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

â€» ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ì •ë³´ì™€ ì§€ì¹¨ì— ë”°ë¼ì„œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

[ìºë¦­í„° ì •ë³´]
{character_info}

[ì´ì „ ëŒ€í™” ê¸°ë¡]
{conversation_history}

[ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤]
{internal_context}

[ì •ë³´ í™œìš© ì „ëµ ë° ìš°ì„ ìˆœìœ„]
1.  **ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ({internal_context}) ìš°ì„  í™œìš©:** ìµœì‹  ì •ë³´ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì¶©ëŒ ì‹œ ìµœì‹  ì •ë³´ë§Œ ì±„íƒ (ë²„ì „/ë‚ ì§œ ê¸°ì¤€).
2.  **ì›¹ ê²€ìƒ‰ (Web Grounding) í™œìš©:**
    *   **í™œìš© ì¡°ê±´:** ë‚´ë¶€ ì •ë³´ ë¶€ì¬/ë¶€ì¡±, ìµœì‹  ì—…ë°ì´íŠ¸ í•„ìš” ì‹œ (íŒ¨ì¹˜, ì´ë²¤íŠ¸, ì‹œì„¸ ë“±), ì‚¬ìš©ì ëª…ì‹œì  ìµœì‹  ì •ë³´ ìš”êµ¬ ì‹œ.
    *   **ê²€ìƒ‰ ì§€ì¹¨:**
        *   **ìµœì‹ ì„± í™•ë³´:** ê²€ìƒ‰ ì‹œ "2025ë…„" ì´í›„ ë‚ ì§œ, í˜„ì¬ ì‹œì¦Œëª…, ìµœì‹  íŒ¨ì¹˜ëª…ì„ í‚¤ì›Œë“œì— í¬í•¨ (ì˜ˆ: "2025 [ì§ì—…] ìŠ¤í‚¬íŠ¸ë¦¬", "ìµœì‹  ë˜íŒŒ ì •ë³´").
        *   **ì¶œì²˜ ë° ê²€ì¦:** ê³µì‹ ì›¹ì‚¬ì´íŠ¸, ì£¼ìš” ì»¤ë®¤ë‹ˆí‹°/ê³µëµ ì‚¬ì´íŠ¸, ê²Œì„ ë§¤ì²´ë¥¼ ì°¸ê³ . ê°ê´€ì ì´ê³  ê²€ì¦ëœ ì •ë³´ë¥¼ ì„ íƒí•˜ë©°, ì»¤ë®¤ë‹ˆí‹° ì •ë³´ëŠ” êµì°¨ í™•ì¸ìœ¼ë¡œ ì‹ ë¢°ë„ í™•ë³´.
    *   **ì •ë³´ í†µí•©:** ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë‚´ë¶€ ì •ë³´ì™€ êµì°¨ ê²€ì¦ í›„, ìµœì‹ ì˜ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ë†’ì€ ì •ë³´ë¥¼ ì‚¬ìš©. í•µì‹¬ë§Œ ìš”ì•½ ì „ë‹¬.
3.  **ì •ë³´ ë¶€ì¬ ì‹œ ì²˜ë¦¬:** ë‚´ë¶€/ì›¹ ê²€ìƒ‰ìœ¼ë¡œë„ ì •ë³´ í™•ì¸ ë¶ˆê°€ ì‹œ, "ìš”ì²­í•˜ì‹  ì •ë³´ í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."ë¡œ ì•ˆë‚´.

[ë‹µë³€ ìƒì„± ê·œì¹™]
1.  **í˜ë¥´ì†Œë‚˜ ë° ì–´íˆ¬:** í•­ìƒ 'RPGPT ë˜íŒŒ'ì˜ "ì•„ë¼ë“œ ìµœê³ ì˜ ê³µëµ ë„¤ë¹„ê²Œì´í„°"ë¡œì„œ ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìˆëŠ” ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ë©°, ì¹œì ˆí•˜ê³  ëª…ë£Œí•˜ê²Œ, ì‚¬ìš©ì ì´í•´ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.
2.  **ë‹µë³€ ë²”ìœ„ ë° í˜•ì‹ (ë§¤ìš° ì¤‘ìš”):**
    *   **ì§ˆë¬¸ ì˜ë„ ëª…í™•íˆ íŒŒì•…:** ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ì •í™•íˆ ì´í•´í•˜ê³ , í•´ë‹¹ ì§ˆë¬¸ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
    *   **ë²”ìœ„ ì—„ìˆ˜:** ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ë¬»ì§€ ì•Šì€ ë‚´ìš©ì´ë‚˜ ì—°ê´€ì„±ì´ ë‚®ì€ ë¶€ê°€ ì •ë³´ëŠ” **ì ˆëŒ€ ë¨¼ì € ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.** ì‚¬ìš©ìê°€ ì¶”ê°€ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ê²½ìš°ì—ë§Œ í•´ë‹¹ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    *   **í•µì‹¬ ìœ„ì£¼ ì „ë‹¬:** ë‹µë³€ì€ ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤. í•„ìš”ì‹œ ë‹¨ê³„ë³„/ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆì§€ë§Œ, ì´ ì—­ì‹œ ì§ˆë¬¸ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
    *   **ì˜ˆì‹œ:** ì‚¬ìš©ìê°€ "A ìŠ¤í‚¬ì˜ ë°ë¯¸ì§€"ë¥¼ ë¬¼ì—ˆë‹¤ë©´, A ìŠ¤í‚¬ì˜ ë°ë¯¸ì§€ë§Œ ë‹µí•˜ê³ , B ìŠ¤í‚¬ì´ë‚˜ A ìŠ¤í‚¬ì˜ ì—­ì‚¬, ë‹¤ë¥¸ í™œìš©ë²• ë“±ì€ ë¨¼ì € ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3.  **ì½˜í…ì¸  ê´€ë ¨ ë‹µë³€:**
    *   'ëª…ì„±' ê¸°ì¤€ ë‹µë³€, 'ê¶Œì¥ ëª…ì„±' ìš°ì„ .
    *   'ëª…ì„±'ê³¼ 'ë˜ë‹´ì»·'ì€ ë³„ê°œì„ì„ ì¸ì§€.
    *   ì»¤ë®¤ë‹ˆí‹° 'ë˜ë‹´ì»·' í˜•ì‹(ì˜ˆ: "30ì–µ/400ë§Œ" - ë”œëŸ¬ ë°ë¯¸ì§€/ë²„í¼ ë²„í”„ë ¥)ì„ ì´í•´í•˜ê³  ë§¥ë½ì— ë§ì¶° ì°¸ê³ . ì´ëŠ” ëª…ì„±ê³¼ ë‹¤ë¥¸ ë³´ì¡° ì§€í‘œì´ë©°, ìˆ˜ì¹˜ëŠ” ë§¥ë½ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.
    *   ë‚¨ì/ì—¬ì ì§ì—…, ì „ì§ ë“± ëª…í™•íˆ êµ¬ë¶„.
    *   ì•ˆê°œì‹  ë ˆì´ë“œì™€ ë‚˜ë²¨ ë ˆì´ë“œëŠ” ë³„ê°œì„ì„ ì¸ì§€.
4.  **ì´ë²¤íŠ¸ ì•ˆë‚´ ê¸°ì¤€:** ì¢…ë£Œ ì‹œ "ì¢…ë£Œë¨". ì¢…ë£Œì¼ ë¯¸í™•ì¸ ì‹œ "ì¢…ë£Œì¼ ë¯¸í™•ì¸, ê³µì‹ í™ˆí˜ì´ì§€ í™•ì¸ ìš”ë§". ì§„í–‰ ì¤‘ì´ë©´ ê¸°ê°„/ë³´ìƒ/ì°¸ì—¬ ë°©ë²• ì•ˆë‚´.

[ì œí•œ ì‚¬í•­ ë° ê¸ˆì§€ ì‚¬í•­]
*   ë˜íŒŒ ì™¸ ì§ˆë¬¸ ê±°ë¶€.
*   ì œê³µëœ ì •ë³´ ì™¸ ì‚¬ìš© ë° í™˜ê° ê¸ˆì§€.
*   ì¶”ì¸¡/ë¶ˆí™•ì‹¤ ì •ë³´ ê¸°ë°˜ ë‹µë³€ ê¸ˆì§€.
*   ì£¼ê´€ì  ì˜ê²¬/íŒë‹¨ ë°°ì œ.
*   **ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì— ëŒ€í•´ ì„ ì œì ìœ¼ë¡œ ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ê¸ˆì§€í•©ë‹ˆë‹¤.**

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€ - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ë§Œ ì´ˆì ì„ ë§ì¶° ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ]
"""
        )
        self.logger.debug("âœ… LLM í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ")

    def _get_bm25_retriever(self):
        """BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            docs_for_bm25 = self.search_factory.create_bm25_data_from_vectordb(self.vectordb)
            return self.search_factory.create_bm25_retriever(docs_for_bm25)
        
        return self.cache_manager.load_or_create_cached_item(
            self.bm25_cache_file, creation_func, self.cache_expiry_short, "BM25 Retriever"
        )

    def _get_cross_encoder_model(self):
        """CrossEncoder ëª¨ë¸ ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            return self.search_factory.create_cross_encoder_model(self.cross_encoder_model_hf)
        
        return self.cache_manager.load_or_create_cached_item(
            self.cross_encoder_cache_file, creation_func, self.cache_expiry_long, "CrossEncoder ëª¨ë¸"
        )
    
    def _determine_weights(self, query: str, character_info: Optional[Dict]) -> List[float]:
        """ì¿¼ë¦¬ì™€ ìºë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê²°ì •"""
        query_lower = query.lower()

        # ê¸°ë³¸ê°’: BM25 ìš°ì„  (ì§ì—…ë„, ìºë¦­í„° ì •ë³´ë„ ì´ë¯¸ ê°–ê³  ìˆìŒ)
        weights = [0.3, 0.7]

        # â€œìµœì‹ Â·ì—…ë°ì´íŠ¸â€ ë¥˜ í‚¤ì›Œë¦¬ë©´ ë²¡í„° ê°€ì¤‘ì¹˜ë¡œ ìŠ¤ì™‘
        if any(k in query_lower for k in ["ìµœì‹ ", "ì—…ë°ì´íŠ¸", "í˜„ì¬", "íŒ¨ì¹˜", "ì¢…ê²°"]):
            weights = [0.7, 0.3]
            self.logger.debug("ğŸ”„ ìµœì‹ Â·íŒ¨ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€ â†’ ë²¡í„° ê°€ì¤‘ì¹˜ ì¦ê°€")

        return weights
    
    def _build_conversation_context_for_llm(self, conversation_history: Optional[List[Dict]]) -> str:
        """ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if not conversation_history or len(conversation_history) == 0:
            return "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, message in enumerate(conversation_history, 1):
            role = message.get('role', 'unknown')
            content = message.get('content', '')
            
            if role == 'user':
                context_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸ {i//2 + 1}: {content}")
            elif role == 'assistant':
                context_parts.append(f"ì´ì „ ë‹µë³€ {i//2 + 1}: {content}")
        
        return "\n".join(context_parts) if context_parts else "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

    def rag_search(self, query: str, character_info: Optional[Dict]) -> Dict[str, Any]:
        # ìºì‹œ í™•ì¸
        cached_result = self.cache_manager.get_cached_search_result(query, 'rag_search', character_info)
        if cached_result:
            self.logger.debug("ğŸ”„ ìºì‹œëœ RAG ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        search_start_time = time.time()
        # ìºë¦­í„° ì •ë³´ë¡œ ì¿¼ë¦¬ ê°•í™”
        enhanced_query = self.text_processor.enhance_query_with_character(query, character_info)
        times = {"internal_search": 0.0}
        
        # ë™ì  ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = self._determine_weights(query, character_info)
        self.logger.debug(f"ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜: ë²¡í„°={weights[0]:.2f}, BM25={weights[1]:.2f}")
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ë™ì  ìƒì„±
        self.rrf_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=weights,
        )
        
        # CrossEncoder ì¬ë­í‚¹ ì¶”ê°€
        compressor = CrossEncoderReranker(model=self.cross_encoder_model, top_n=60)
        base_retriever = ContextualCompressionRetriever(
            base_retriever=self.rrf_retriever,
            base_compressor=compressor,
        )
        
        # ë©”íƒ€ë°ì´í„° ì¸ì‹ ê²€ìƒ‰ê¸°ë¡œ ë˜í•‘
        self.internal_retriever = MetadataAwareRetriever(base_retriever)

        def _search_internal():
            start = time.time()
            try:
                self.logger.debug("ğŸ”„ ë‚´ë¶€ RAG ê²€ìƒ‰ ì‹œì‘...")
                docs = self.internal_retriever.get_relevant_documents(enhanced_query)
                times["internal_search"] = time.time() - start
                self.logger.info(f"âœ… ë‚´ë¶€ RAG ê²€ìƒ‰ ì™„ë£Œ: {times['internal_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["internal_search"] = time.time() - start
                self.logger.error(f"âŒ ë‚´ë¶€ RAG ê²€ìƒ‰ ì˜¤ë¥˜ ({times['internal_search']:.2f}ì´ˆ): {e}")
                return []

        internal_docs = _search_internal()
        
        times["internal_search"] = time.time() - search_start_time
        self.logger.debug(f"ğŸ¯ ë‚´ë¶€ ê²€ìƒ‰ ì™„ë£Œ - ì´ {times['internal_search']:.2f}ì´ˆ")

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
        internal_context_str = self.text_processor.format_docs_to_context_string(internal_docs, "ë‚´ë¶€")
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "internal_docs": internal_docs,
            "internal_context_provided_to_llm": internal_context_str,
            "enhanced_query": enhanced_query,
            "search_times": times
        }
        
        # ìºì‹œì— ì €ì¥
        self.cache_manager.save_search_result_to_cache(query, result, 'rag_search', character_info)
        return result

    def get_answer(self, query: str, character_info: Optional[Dict] = None, conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """RAG ë‹µë³€ ìƒì„± (ë©”ì¸ API)"""
        total_start_time = time.time()
        
        self.logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: \"{query}\"")
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ë¡œê·¸ ì¶œë ¥
        if conversation_history and len(conversation_history) > 0:
            self.logger.info(f"ì´ì „ ëŒ€í™” ê¸°ë¡: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
        else:
            self.logger.info("ì´ì „ ëŒ€í™” ê¸°ë¡ ì—†ìŒ")

        # ìºë¦­í„° ì •ë³´ë¥¼ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        char_context_for_llm = self.text_processor.build_character_context_for_llm(character_info)
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        conversation_context_for_llm = self._build_conversation_context_for_llm(conversation_history)
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.rag_search(query, character_info)
        
        # LLM ë‹µë³€ ìƒì„±
        llm_start_time = time.time()
        self.logger.info("ğŸ”„ LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        formatted_prompt = self.prompt.format(
            internal_context=search_results["internal_context_provided_to_llm"],
            question=query,
            character_info=char_context_for_llm,
            conversation_history=conversation_context_for_llm
        )
        
        try:
            from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
            
            # ê·¸ë¼ìš´ë”© ë„êµ¬ ì„¤ì •
            tools = []
            if self.enable_web_grounding:
                google_search_tool = Tool(
                    google_search = GoogleSearch()
                )
                tools.append(google_search_tool)
                self.logger.debug("ğŸ” ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© í™œì„±í™”ë¨")
            else:
                self.logger.debug("ğŸš« ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ë¹„í™œì„±í™”ë¨")
            
            # LLM í˜¸ì¶œ
            response = self.genai_client.models.generate_content(
                model=self.llm_model_name,
                contents=formatted_prompt,
                config=GenerateContentConfig(
                    tools=tools,
                    temperature=0,
                )
            )
            
            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            llm_response = ""
            for part in response.candidates[0].content.parts:
                if part.text:
                    llm_response += part.text
            
            # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° í™•ì¸
            if self.enable_web_grounding and hasattr(response.candidates[0], 'grounding_metadata'):
                grounding = response.candidates[0].grounding_metadata
                if hasattr(grounding, 'search_entry_point') and grounding.search_entry_point:
                    self.logger.info("ğŸŒ ì›¹ ê²€ìƒ‰ ê·¸ë¼ìš´ë”©ì´ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    # ê²€ìƒ‰ëœ ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    if grounding.search_entry_point.rendered_content:
                        self.logger.debug(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {grounding.search_entry_point.rendered_content[:200]}...")
        except Exception as e:
            self.logger.error(f"âŒ LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            self.logger.error(f"ìƒì„¸ ì—ëŸ¬: {str(e)}")
            self.logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            llm_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        llm_elapsed_time = time.time() - llm_start_time
        total_elapsed_time = time.time() - total_start_time
        
        self.logger.info(f"âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ ({llm_elapsed_time:.2f}ì´ˆ)")
        self.logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        
        # ìƒì„±ëœ ë‹µë³€ ì¶œë ¥ (ë””ë²„ê·¸ ë ˆë²¨ì—ì„œ)
        self.logger.debug("\n" + "="*50)
        self.logger.debug("[ë‹µë³€]")
        self.logger.debug("="*50)
        self.logger.debug(llm_response[:200] + "..." if len(llm_response) > 200 else llm_response)
        self.logger.debug("="*50)
        
        # FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í‚¤ë¡œ ë°˜í™˜ê°’ êµ¬ì„±
        return {
            "result": llm_response,
            "internal_docs": search_results["internal_docs"],
            "enhanced_query": search_results["enhanced_query"],
            "execution_times": {
                "total": total_elapsed_time,
                "llm": llm_elapsed_time,
                "search": search_results["search_times"]
            },
            "internal_context": search_results["internal_context_provided_to_llm"],
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_structured_rag_service_instance: Optional[StructuredRAGService] = None

def get_structured_rag_service() -> StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _structured_rag_service_instance
    if _structured_rag_service_instance is None:
        logger = get_logger(__name__)
        logger.info("âœ¨ ìƒˆë¡œìš´ StructuredRAGService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± âœ¨")
        _structured_rag_service_instance = StructuredRAGService()
    return _structured_rag_service_instance

def get_structured_rag_answer(query: str, character_info: Optional[Dict] = None, conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜"""
    service = get_structured_rag_service()
    return service.get_answer(query, character_info, conversation_history)
