"""
ë¶„ë¦¬ëœ RAG ì„œë¹„ìŠ¤ - êµ¬ì¡°í™”ëœ ë²„ì „
"""
from __future__ import annotations
import os
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Optional, Any
from pathlib import Path
from dotenv import load_dotenv

# LLM & ì„ë² ë”©
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma

# Gemini ì„ë² ë”© import
from vectorstore.gemini_embeddings import GeminiEmbeddings

# ê²€ìƒ‰ ê´€ë ¨
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.prompts import PromptTemplate

# Google Gen AI SDK
from google import genai

# ë¶„ë¦¬ëœ ìœ í‹¸ë¦¬í‹°ë“¤
from .cache_utils import CacheManager
from .text_utils import TextProcessor
from .retrievers import MetadataAwareRetriever
from .search_factory import SearcherFactory
from .web_search import WebSearcher

load_dotenv()


class StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    # --- ìƒìˆ˜ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€) ---
    CACHE_DIR_NAME = "cache"
    VECTOR_DB_DIR = "vector_db/chroma"
    EMBED_MODEL_NAME = "text-embedding-004"
    BM25_CACHE_FILE = "bm25_retriever.pkl"
    CROSS_ENCODER_CACHE_FILE = "cross_encoder.pkl"
    LLM_MODEL_NAME = "models/gemini-2.5-flash-preview-05-20"
    CROSS_ENCODER_MODEL_HF = "cross-encoder/ms-marco-MiniLM-L6-v2"

    CACHE_EXPIRY_SHORT = 60 * 60 * 12  # 12ì‹œê°„
    CACHE_EXPIRY_LONG = 60 * 60 * 24   # 24ì‹œê°„

    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self._setup_environment()
        self._initialize_utilities()
        self._initialize_core_components()
        self._initialize_retrievers()
        self._setup_llm_and_prompt()

    def _setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        self.cache_dir = Path(self.CACHE_DIR_NAME)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not self.gemini_api_key:
            raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        print("âœ… Gemini API í‚¤ í™•ì¸ ì™„ë£Œ - LLM ë° ì„ë² ë”© ëª¨ë‘ Gemini ì‚¬ìš©")

    def _initialize_utilities(self):
        """ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ ì´ˆê¸°í™”"""
        self.cache_manager = CacheManager(self.cache_dir, self.CACHE_EXPIRY_SHORT, self.CACHE_EXPIRY_LONG)
        self.text_processor = TextProcessor()
        self.search_factory = SearcherFactory()

    def _initialize_core_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        print("ğŸš€ RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=self.gemini_api_key,
            model=self.LLM_MODEL_NAME,
            temperature=0
        )
        
        self.gemini_client = genai.Client(api_key=self.gemini_api_key)
        self.web_searcher = WebSearcher(self.gemini_client)
        
        # Gemini ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
        self.embed_fn = GeminiEmbeddings(
            model=self.EMBED_MODEL_NAME,
            api_key=self.gemini_api_key,
            task_type="RETRIEVAL_QUERY",  # ì¿¼ë¦¬ ê²€ìƒ‰ìš© ìµœì í™”
            rate_limit_delay=0.05  # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì¸í•´ ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•
        )
        self.vectordb = Chroma(
            persist_directory=self.VECTOR_DB_DIR,
            embedding_function=self.embed_fn
        )
        
        print("âœ… í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_retrievers(self):
        """ê²€ìƒ‰ê¸° ì´ˆê¸°í™”"""
        print("ğŸ”„ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        # ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì •
        self.vector_retriever = self.vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 15, "fetch_k": 30, "lambda_mult": 0.8},
        )
        
        # BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ ì‚¬ìš©)
        self.bm25_retriever = self._get_bm25_retriever()
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„±
        self.rrf_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=[0.5, 0.5],
        )
        
        # CrossEncoder ì¬ë­í‚¹ ì¶”ê°€
        cross_encoder_model = self._get_cross_encoder_model()
        compressor = CrossEncoderReranker(model=cross_encoder_model, top_n=15)
        base_retriever = ContextualCompressionRetriever(
            base_retriever=self.rrf_retriever,
            base_compressor=compressor,
        )
        
        # ë©”íƒ€ë°ì´í„° ì¸ì‹ ê²€ìƒ‰ê¸°ë¡œ ë˜í•‘
        self.internal_retriever = MetadataAwareRetriever(base_retriever)
        
        elapsed_time = time.time() - start_time
        print(f"ğŸ‰ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

    def _setup_llm_and_prompt(self):
        """LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)"""
        self.hybrid_prompt = PromptTemplate(
            input_variables=["internal_context", "web_context", "question", "character_info"],
            template="""
ë‹¹ì‹ ì€ ë˜ì „ì•¤íŒŒì´í„° ì „ë¬¸ ìŠ¤í™ì—… ê°€ì´ë“œ ì±—ë´‡ì…ë‹ˆë‹¤.  
â€» ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ì •ë³´ë§Œ í™œìš©í•´ ë‹µë³€í•˜ì„¸ìš”.

[ìºë¦­í„° ì •ë³´]
{character_info}

[ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤]
{internal_context}

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

[ë‹µë³€ ê·œì¹™]
- ì œê³µëœ ì •ë³´ ì™¸ì˜ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì œê³µëœ ì •ë³´ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- ëŒ€ë‹µì—ëŠ” ë‚´ë¶€ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ì‚¬ìš©í•˜ê³ , ì™¸ë¶€ ë°ì´í„°ë¡œ ê²€í† ë¥¼ ë°›ìœ¼ì„¸ìš”.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë²”ìœ„ë§Œ ë‹¤ë£¨ë©°, ê´€ë ¨ ì—†ëŠ” ì„¤ëª…ì€ ìƒëµí•˜ì„¸ìš”.
- ìˆœì„œë¥¼ ë‚˜ì—´í•˜ë©° ì„¤ëª…í•˜ê³ , ì§§ê³  ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ì„¤ëª…í•˜ì„¸ìš”.
- ë‹µë³€ì—” ê°„ë‹¨í•œ ì¶œì²˜ë¥¼ í•¨ê»˜ ì‘ì„±í•˜ì„¸ìš”.

[ì½˜í…ì¸  ê´€ë ¨]
- ì½˜í…ì¸  ê´€ë ¨ ëŒ€ë‹µì´ ë“¤ì–´ì˜¬ ê²½ìš°ì—”, ëª…ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
- ì½˜í…ì¸ ì—ëŠ” ì…ì¥ ëª…ì„±ê³¼ ê¶Œì¥ ëª…ì„±ì´ ìˆëŠ”ë°, ê¶Œì¥ ëª…ì„± ê¸°ì¤€ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”.

[ì´ë²¤íŠ¸ ì•ˆë‚´ ê¸°ì¤€]
- ì¢…ë£Œëœ ì´ë²¤íŠ¸ â†’ "í•´ë‹¹ ì´ë²¤íŠ¸ëŠ” ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
- ì¢…ë£Œì¼ì´ ì—†ì„ ê²½ìš° â†’ "ì´ë²¤íŠ¸ ì¢…ë£Œì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€ - ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ]
"""
        )
        print("âœ… LLM í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ")

    def _get_bm25_retriever(self):
        """BM25 ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            docs_for_bm25 = self.search_factory.create_bm25_data_from_vectordb(self.vectordb)
            return self.search_factory.create_bm25_retriever(docs_for_bm25)
        
        return self.cache_manager.load_or_create_cached_item(
            self.BM25_CACHE_FILE, creation_func, self.CACHE_EXPIRY_SHORT, "BM25 Retriever"
        )

    def _get_cross_encoder_model(self):
        """CrossEncoder ëª¨ë¸ ìƒì„± (ìºì‹œ í™œìš©)"""
        def creation_func():
            return self.search_factory.create_cross_encoder_model(self.CROSS_ENCODER_MODEL_HF)
        
        return self.cache_manager.load_or_create_cached_item(
            self.CROSS_ENCODER_CACHE_FILE, creation_func, self.CACHE_EXPIRY_LONG, "CrossEncoder ëª¨ë¸"
        )

    def _hybrid_search(self, query: str, character_info: Optional[Dict]) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë‚´ë¶€ + ì›¹)"""
        # ìºì‹œ í™•ì¸
        cached_result = self.cache_manager.get_cached_search_result(query, 'hybrid_search', character_info)
        if cached_result:
            print("ğŸ”„ ìºì‹œëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        search_start_time = time.time()
        enhanced_query = self.text_processor.enhance_query_with_character(query, character_info)
        times = {"internal_search": 0.0, "web_search": 0.0, "total_search": 0.0}

        def _search_internal():
            start = time.time()
            try:
                print("ğŸ”„ ë‚´ë¶€ RAG ê²€ìƒ‰ ì‹œì‘...")
                docs = self.internal_retriever.get_relevant_documents(enhanced_query)
                times["internal_search"] = time.time() - start
                print(f"âœ… ë‚´ë¶€ RAG ê²€ìƒ‰ ì™„ë£Œ: {times['internal_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["internal_search"] = time.time() - start
                print(f"âŒ ë‚´ë¶€ RAG ê²€ìƒ‰ ì˜¤ë¥˜ ({times['internal_search']:.2f}ì´ˆ): {e}")
                return []

        def _search_web():
            start = time.time()
            try:
                print("ğŸ”„ ì›¹ ê²€ìƒ‰ (Gemini) ì‹œì‘...")
                # ì›¹ ê²€ìƒ‰ì€ ìºì‹œë¥¼ ë³„ë„ë¡œ í™•ì¸
                cached_web_result = self.cache_manager.get_cached_search_result(query, 'gemini_search', character_info)
                if cached_web_result:
                    print("ğŸ”„ ìºì‹œëœ Gemini ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
                    docs = cached_web_result
                else:
                    docs = self.web_searcher.search_with_grounding(query, character_info)
                    self.cache_manager.save_search_result_to_cache(query, docs, 'gemini_search', character_info)
                
                times["web_search"] = time.time() - start
                print(f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {times['web_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["web_search"] = time.time() - start
                print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ ({times['web_search']:.2f}ì´ˆ): {e}")
                return []

        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        print("ğŸš€ ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘ (ë‚´ë¶€ RAG + ì›¹)")
        with ThreadPoolExecutor(max_workers=2) as executor:
            internal_future = executor.submit(_search_internal)
            web_future = executor.submit(_search_web)
            internal_docs = internal_future.result()
            web_docs = web_future.result()
        
        times["total_search"] = time.time() - search_start_time
        print(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ - ì´ {times['total_search']:.2f}ì´ˆ")

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
        internal_context_str = self.text_processor.format_docs_to_context_string(internal_docs, "ë‚´ë¶€")
        web_context_str = self.text_processor.format_web_search_docs_to_context_string(web_docs)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "all_docs": internal_docs + web_docs,
            "internal_docs": internal_docs,
            "web_docs": web_docs,
            "internal_context_provided_to_llm": internal_context_str,
            "web_context_provided_to_llm": web_context_str,
            "used_web_search": bool(web_docs),
            "enhanced_query": enhanced_query,
            "search_times": times
        }
        
        # ìºì‹œì— ì €ì¥
        self.cache_manager.save_search_result_to_cache(query, result, 'hybrid_search', character_info)
        return result

    def get_answer(self, query: str, character_info: Optional[Dict] = None) -> Dict[str, Any]:
        """RAG ë‹µë³€ ìƒì„± (ë©”ì¸ API)"""
        total_start_time = time.time()
        
        print(f"\n[INFO] ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: \"{query}\"")
        char_desc_parts = []
        if character_info:
            if class_info := character_info.get('class'):
                char_desc_parts.append(class_info)
            if fame_info := character_info.get('fame'):
                char_desc_parts.append(f"{fame_info}ëª…ì„±")
            if char_desc_parts:
                print(f"[INFO] ìºë¦­í„°: {' '.join(char_desc_parts)}")

        # ìºë¦­í„° ì •ë³´ë¥¼ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        char_context_for_llm = self.text_processor.build_character_context_for_llm(character_info)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self._hybrid_search(query, character_info)
        
        # LLM ë‹µë³€ ìƒì„±
        llm_start_time = time.time()
        print("ğŸ”„ LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        formatted_prompt = self.hybrid_prompt.format(
            internal_context=search_results["internal_context_provided_to_llm"],
            web_context=search_results["web_context_provided_to_llm"],
            question=query,
            character_info=char_context_for_llm
        )
        
        try:
            llm_response = self.llm.invoke(formatted_prompt).content
        except Exception as e:
            print(f"âŒ LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            llm_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        llm_elapsed_time = time.time() - llm_start_time
        total_elapsed_time = time.time() - total_start_time
        
        print(f"âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ ({llm_elapsed_time:.2f}ì´ˆ)")
        print(f"[INFO] ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        
        # FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í‚¤ë¡œ ë°˜í™˜ê°’ êµ¬ì„±
        return {
            "result": llm_response,
            "source_documents": search_results["all_docs"],
            "used_web_search": search_results["used_web_search"],
            "internal_docs": search_results["internal_docs"],
            "web_docs": search_results["web_docs"],
            "enhanced_query": search_results["enhanced_query"],
            "execution_times": {
                "total": total_elapsed_time,
                "llm": llm_elapsed_time,
                "search": search_results["search_times"]
            },
            "internal_context": search_results["internal_context_provided_to_llm"],
            "web_context": search_results["web_context_provided_to_llm"]
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_structured_rag_service_instance: Optional[StructuredRAGService] = None

def get_structured_rag_service() -> StructuredRAGService:
    """êµ¬ì¡°í™”ëœ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _structured_rag_service_instance
    if _structured_rag_service_instance is None:
        print("âœ¨ ìƒˆë¡œìš´ StructuredRAGService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± âœ¨")
        _structured_rag_service_instance = StructuredRAGService()
    return _structured_rag_service_instance

def get_structured_rag_answer(query: str, character_info: Optional[Dict] = None) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜"""
    service = get_structured_rag_service()
    return service.get_answer(query, character_info)
