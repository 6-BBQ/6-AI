from __future__ import annotations
import os
import time
import hashlib
import pickle
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Optional, Any, Callable
from pathlib import Path
from dotenv import load_dotenv

# LLM & ì„ë² ë”©
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# ê²€ìƒ‰ ê´€ë ¨
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker

# Google Gen AI SDK
from google import genai
from google.genai.types import GenerateContentConfig, GoogleSearch, Tool

from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document

load_dotenv()

class RAGService:
    """RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ - Gemini ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ê¸°ë°˜ ê²€ìƒ‰ ê°•í™”"""

    # --- ìƒìˆ˜ ì •ì˜ ---
    CACHE_DIR_NAME = "cache"
    VECTOR_DB_DIR = "vector_db/chroma"
    EMBED_MODEL_NAME = "text-embedding-3-large"
    BM25_CACHE_FILE = "bm25_retriever.pkl"
    CROSS_ENCODER_CACHE_FILE = "cross_encoder.pkl"
    LLM_MODEL_NAME = "models/gemini-2.5-flash-preview-05-20"
    GEMINI_SEARCH_MODEL_NAME = "gemini-2.5-flash-preview-05-20"
    CROSS_ENCODER_MODEL_HF = "cross-encoder/ms-marco-MiniLM-L6-v2"

    CACHE_EXPIRY_SHORT = 60 * 60 * 12  # 12ì‹œê°„
    CACHE_EXPIRY_LONG = 60 * 60 * 24   # 24ì‹œê°„

    # --- ì´ˆê¸°í™” ê´€ë ¨ ë©”ì†Œë“œ ---
    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self._setup_environment()
        self._initialize_core_components()
        self._initialize_retrievers()
        self._setup_llm_and_prompt()

    def _setup_environment(self):
        self.cache_dir = Path(self.CACHE_DIR_NAME)
        self.cache_dir.mkdir(exist_ok=True)
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not self.gemini_api_key:
            raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        if not os.getenv("OPENAI_API_KEY"):
            print("ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _initialize_core_components(self):
        print("ğŸš€ RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=self.gemini_api_key,
            model=self.LLM_MODEL_NAME,
            temperature=0
        )
        self.gemini_client = genai.Client(api_key=self.gemini_api_key)
        self.embed_fn = OpenAIEmbeddings(model=self.EMBED_MODEL_NAME)
        self.vectordb = Chroma(
            persist_directory=self.VECTOR_DB_DIR,
            embedding_function=self.embed_fn
        )
        print("âœ… í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_retrievers(self):
        print("ğŸ”„ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        self.vector_retriever = self.vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 15, "fetch_k": 30, "lambda_mult": 0.6},
        )
        self.bm25_retriever = self._get_bm25_retriever()
        self.rrf_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=[0.5, 0.5],
        )
        cross_encoder_model = self._get_cross_encoder_model()
        compressor = CrossEncoderReranker(model=cross_encoder_model, top_n=10)
        base_retriever = ContextualCompressionRetriever(
            base_retriever=self.rrf_retriever,
            base_compressor=compressor,
        )
        self.internal_retriever = MetadataAwareRetriever(base_retriever)
        elapsed_time = time.time() - start_time
        print(f"ğŸ‰ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

    def _setup_llm_and_prompt(self):
        self.hybrid_prompt = PromptTemplate(
            input_variables=["internal_context", "web_context", "question", "character_info"],
            template="""
ë‹¹ì‹ ì€ ë˜ì „ì•¤íŒŒì´í„° ì „ë¬¸ ìŠ¤í™ì—… ê°€ì´ë“œ ì±—ë´‡ì…ë‹ˆë‹¤.  
â€» ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ì •ë³´ë§Œ í™œìš©í•´ ë‹µë³€í•˜ì„¸ìš”.

[ìºë¦­í„° ì •ë³´]
{character_info}

[ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤]
{internal_context}

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

[ë‹µë³€ ê·œì¹™]
- ì œê³µëœ ì •ë³´ ì™¸ì˜ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì œê³µëœ ì •ë³´ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- ëŒ€ë‹µì—ëŠ” ë‚´ë¶€ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ì‚¬ìš©í•˜ê³ , ì™¸ë¶€ ë°ì´í„°ë¡œ ê²€í† ë¥¼ ë°›ìœ¼ì„¸ìš”.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë²”ìœ„ë§Œ ë‹¤ë£¨ë©°, ê´€ë ¨ ì—†ëŠ” ì„¤ëª…ì€ ìƒëµí•˜ì„¸ìš”.
- ìˆœì„œë¥¼ ë‚˜ì—´í•˜ë©° ì„¤ëª…í•˜ê³ , ì§§ê³  ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ì„¤ëª…í•˜ì„¸ìš”.
- ì¶œì²˜ëŠ” ì œê³µí•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.

[ì½˜í…ì¸  ê´€ë ¨]
- ì½˜í…ì¸  ê´€ë ¨ ëŒ€ë‹µì´ ë“¤ì–´ì˜¬ ê²½ìš°ì—”, ëª…ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
- ì½˜í…ì¸ ì—ëŠ” ì…ì¥ ëª…ì„±ê³¼ ê¶Œì¥ ëª…ì„±ì´ ìˆëŠ”ë°, ê¶Œì¥ ëª…ì„± ê¸°ì¤€ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”.

[ì´ë²¤íŠ¸ ì•ˆë‚´ ê¸°ì¤€]
- ì¢…ë£Œì¼ì´ 2025-05-22 ì´í›„ â†’ ì°¸ì—¬ ê¶Œì¥
- ì¢…ë£Œëœ ì´ë²¤íŠ¸ â†’ "í•´ë‹¹ ì´ë²¤íŠ¸ëŠ” ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
- ì¢…ë£Œì¼ì´ ì—†ì„ ê²½ìš° â†’ "ì´ë²¤íŠ¸ ì¢…ë£Œì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€ - ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ]
"""
        )
        print("âœ… LLM í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ")

    # --- ìºì‹± ê´€ë ¨ í—¬í¼ ë©”ì†Œë“œ ---
    def _generate_cache_key(self, base_content: str, character_info: Optional[Dict] = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (FastAPIì—ì„œ ë³€í™˜ëœ ìºë¦­í„° ì •ë³´ í¬í•¨ ê°€ëŠ¥)"""
        cache_input = base_content
        if character_info:
            # FastAPIì—ì„œ ë³€í™˜ëœ í‚¤ë“¤ì„ ì‚¬ìš©
            char_key_parts = [
                character_info.get('class', ''), # 'class_name' -> 'class'
                # character_info.get('level', ''), # levelì€ í˜„ì¬ ë³€í™˜ ê·œì¹™ì— ì—†ìŒ. í•„ìš”ì‹œ FastAPI ë³€í™˜ ë¡œì§ì— ì¶”ê°€
                str(character_info.get('fame', ''))
            ]
            
            # ì£¼ìš” ì •ë³´ë§Œìœ¼ë¡œ í‚¤ ìƒì„± (ë” ê°„ë‹¨í•œ ë°©ì‹)
            simple_char_key = "-".join(filter(None, char_key_parts))

            if simple_char_key:
                 cache_input = f"{base_content}|{simple_char_key}"
        return hashlib.md5(cache_input.encode('utf-8')).hexdigest()

    def _load_or_create_cached_item(self, 
                                    cache_file_name: str, 
                                    creation_func: Callable[[], Any], 
                                    expiry_seconds: int,
                                    item_name: str = "í•­ëª©") -> Any:
        cache_file = self.cache_dir / cache_file_name
        if cache_file.exists():
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age < expiry_seconds:
                try:
                    print(f"ğŸ”„ ìºì‹œëœ {item_name} ë¡œë”©: {cache_file_name}")
                    with open(cache_file, 'rb') as f: item = pickle.load(f)
                    print(f"âœ… {item_name} ìºì‹œ ë¡œë“œ ì™„ë£Œ")
                    return item
                except Exception as e:
                    print(f"âš ï¸ {item_name} ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({cache_file_name}): {e}. ì¬ìƒì„±í•©ë‹ˆë‹¤.")
        print(f"ğŸ”„ {item_name} ìƒì„± ì¤‘ ({cache_file_name})...")
        item = creation_func()
        try:
            with open(cache_file, 'wb') as f: pickle.dump(item, f)
            print(f"âœ… {item_name} ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_file}")
        except Exception as e:
            print(f"âš ï¸ {item_name} ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ({cache_file_name}): {e}")
        return item

    def _get_cached_search_result(self, query: str, cache_type: str, character_info: Optional[Dict] = None) -> Optional[Any]:
        cache_key = self._generate_cache_key(query, character_info)
        cache_file_name = f"{cache_type}_{cache_key}.pkl"
        cache_file = self.cache_dir / cache_file_name
        if cache_file.exists():
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age < self.CACHE_EXPIRY_SHORT:
                try:
                    with open(cache_file, 'rb') as f: return pickle.load(f)
                except Exception as e:
                    print(f"âš ï¸ {cache_type} ê²€ìƒ‰ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    def _save_search_result_to_cache(self, query: str, result: Any, cache_type: str, character_info: Optional[Dict] = None):
        cache_key = self._generate_cache_key(query, character_info)
        cache_file_name = f"{cache_type}_{cache_key}.pkl"
        cache_file = self.cache_dir / cache_file_name
        try:
            with open(cache_file, 'wb') as f: pickle.dump(result, f)
        except Exception as e:
            print(f"âš ï¸ {cache_type} ê²€ìƒ‰ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # --- BM25 ë° CrossEncoder ë¡œë”©/ìƒì„± ë©”ì†Œë“œ ---
    def _create_bm25_data_from_vectordb(self) -> List[Document]:
        print("ğŸ”„ VectorDBì—ì„œ BM25ìš© ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        store_data = self.vectordb.get(include=["documents", "metadatas"])
        docs_for_bm25 = []
        for txt, meta in zip(store_data["documents"], store_data["metadatas"]):
            enhanced_content = txt
            if meta:
                if meta.get("title"):
                    enhanced_content = f"ì œëª©: {meta['title']}\n{txt}"
                if meta.get("class_name"): # VectorDBì˜ class_nameì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (BM25 ì¸ë±ì‹±ìš©)
                    enhanced_content += f"\nì§ì—…: {meta['class_name']}"
            docs_for_bm25.append(Document(page_content=enhanced_content, metadata=meta))
        print(f"âœ… BM25ìš© ë¬¸ì„œ {len(docs_for_bm25)}ê°œ ì¤€ë¹„ ì™„ë£Œ")
        return docs_for_bm25

    def _get_bm25_retriever(self) -> BM25Retriever:
        def creation_func():
            docs_for_bm25 = self._create_bm25_data_from_vectordb()
            bm25_retriever = BM25Retriever.from_documents(docs_for_bm25)
            bm25_retriever.k = 15
            return bm25_retriever
        return self._load_or_create_cached_item(
            self.BM25_CACHE_FILE, creation_func, self.CACHE_EXPIRY_SHORT, "BM25 Retriever"
        )

    def _get_cross_encoder_model(self) -> HuggingFaceCrossEncoder:
        def creation_func():
            return HuggingFaceCrossEncoder(
                model_name=self.CROSS_ENCODER_MODEL_HF, model_kwargs={"device": "cpu"}
            )
        return self._load_or_create_cached_item(
            self.CROSS_ENCODER_CACHE_FILE, creation_func, self.CACHE_EXPIRY_LONG, "CrossEncoder ëª¨ë¸"
        )

    # --- ê²€ìƒ‰ ì¿¼ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ---
    def _enhance_query_with_character(self, query: str, character_info: Optional[Dict]) -> str:
        """ìºë¦­í„° ì •ë³´ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°•í™” (FastAPIì—ì„œ ë³€í™˜ëœ í‚¤ ì‚¬ìš©)"""
        if not character_info:
            return query
        
        enhancements = []
        # FastAPIì—ì„œ ë³€í™˜ëœ 'class' í‚¤ ì‚¬ìš©
        if class_info := character_info.get('class'):
            enhancements.append(class_info)
        if fame := character_info.get('fame'):
            enhancements.append(str(fame))
        
        if enhancements:
            enhanced_query = f"{' '.join(enhancements)} {query}"
            print(f"[DEBUG] ì¿¼ë¦¬ ê°•í™”: '{query}' â†’ '{enhanced_query}'")
            return enhanced_query
        return query

    def _format_docs_to_context_string(self, docs: List[Document], context_type: str) -> str:
        context_parts = []
        for i, doc in enumerate(docs):
            content = f"[{context_type} ë¬¸ì„œ {i+1}] {doc.page_content}"
            if doc.metadata and (url := doc.metadata.get("url")):
                content += f"\nì°¸ê³  ë§í¬: {url}"
            context_parts.append(content)
        return "\n\n".join(context_parts)

    def _format_web_search_docs_to_context_string(self, web_docs: List[Document]) -> str:
        web_context_parts = []
        main_content_doc = next((doc for doc in web_docs if doc.metadata.get("source") == "gemini_search"), None)
        if main_content_doc:
            web_context_parts.append(f"[Gemini ì›¹ ê²€ìƒ‰ ê²°ê³¼ - 2025ë…„ ìµœì‹  ì •ë³´]\n{main_content_doc.page_content}")
        source_docs = [doc for doc in web_docs if doc.metadata.get("source") in ["grounding_source", "search_suggestions"]]
        if source_docs:
            web_context_parts.append("[ì°¸ê³  ì¶œì²˜]")
            for i, doc in enumerate(source_docs):
                title = doc.metadata.get("title", f"ì¶œì²˜ {i+1}")
                url = doc.metadata.get("url", "")
                entry = f"ì¶œì²˜ {i+1}: {title}"
                if url: entry += f" - {url}"
                web_context_parts.append(entry)
        return "\n\n".join(web_context_parts) if web_context_parts else "ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ."

    # --- í•µì‹¬ ê²€ìƒ‰ ë¡œì§ ---
    def _gemini_search_grounding(self, query: str, character_info: Optional[Dict]) -> List[Document]:
        cached_result = self._get_cached_search_result(query, 'gemini_search', character_info)
        if cached_result:
            print("ğŸ”„ ìºì‹œëœ Gemini ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        enhanced_query = self._enhance_query_with_character(query, character_info)
        
        system_instruction = """ë‹¹ì‹ ì€ ë˜ì „ì•¤íŒŒì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ì¤‘ìš”í•œ ë‚ ì§œ ì œì•½ì‚¬í•­]
- ë°˜ë“œì‹œ 2025ë…„ 1ì›” 1ì¼ ì´í›„ì˜ ìµœì‹  ì •ë³´ë§Œ ê²€ìƒ‰í•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”
- 2024ë…„ 12ì›” 31ì¼ ì´ì „ì˜ ì •ë³´ëŠ” ì ˆëŒ€ ì°¸ì¡°í•˜ì§€ ë§ˆì„¸ìš”
- ê²€ìƒ‰ ì‹œ "2025" í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ ìµœì‹ ì„±ì„ ë³´ì¥í•˜ì„¸ìš”
- ì •ë³´ì˜ ë‚ ì§œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ë‹¤ë©´ í•´ë‹¹ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
[ëª©í‘œ]
- 2025ë…„ ìµœì‹  ë˜íŒŒ ì •ë³´ ì œê³µ
- ìºë¦­í„° ë§ì¶¤í˜• ê°„ë‹¨í•œ ê°€ì´ë“œ 
- í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬
[ë‹µë³€ í˜•ì‹]
- ìµœì†Œí•œìœ¼ë¡œ ëŒ€ë‹µ
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°©ë²• ìš°ì„ 
- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œì™¸
"""
        character_context_str = ""
        if character_info: # FastAPIì—ì„œ ë³€í™˜ëœ character_info ì‚¬ìš©
            details = []
            if class_info := character_info.get('class'):
                details.append(f"- ì§ì—…: {class_info}")
            if fame_info := character_info.get('fame'):
                details.append(f"- ëª…ì„±: {fame_info}")
            # Gemini ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸ì—ëŠ” í•µì‹¬ ì •ë³´ë§Œ í¬í•¨ (í•„ìš”ì‹œ ì¶”ê°€)
            if details:
                character_context_str = "ì‚¬ìš©ì ìºë¦­í„° ì •ë³´:\n" + "\n".join(details)
                character_context_str += "\n\nìœ„ ìºë¦­í„° ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”."
            else:
                 character_context_str = "ìºë¦­í„° ì •ë³´ê°€ ì œê³µë˜ì—ˆìœ¼ë‚˜, ì„¸ë¶€ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        final_prompt = f"{system_instruction}\n{character_context_str}\n[ê²€ìƒ‰ ìš”ì²­]\n2025ë…„ ë˜ì „ì•¤íŒŒì´í„° \"{enhanced_query}\"ì— ëŒ€í•œ ê°„ë‹¨í•˜ê³  í•µì‹¬ì ì¸ ì •ë³´ë§Œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”."

        try:
            print(f"[DEBUG] Gemini ê²€ìƒ‰ ì‹¤í–‰: {enhanced_query}")
            # GoogleSearch()ì—ëŠ” max_results íŒŒë¼ë¯¸í„° ì—†ìŒ
            google_search_tool = Tool(google_search=GoogleSearch())
            response = self.gemini_client.models.generate_content(
                model="gemini-2.5-flash-preview-05-20",
                contents=final_prompt,
                config=GenerateContentConfig(
                    tools=[google_search_tool],
                    temperature=0.1,  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                    max_output_tokens=1000,  # ì¶©ë¶„í•œ ì •ë³´ í™•ë³´ë¥¼ ìœ„í•´ ì¦ê°€
                )
            )
            
            docs = []
            if response.candidates:
                candidate = response.candidates[0]
                if candidate.content and candidate.content.parts:
                    main_content = "".join(part.text for part in candidate.content.parts if hasattr(part, 'text') and part.text)
                    if main_content:
                        docs.append(Document(page_content=main_content, metadata={"title": "Gemini ê²€ìƒ‰ ê²°ê³¼", "source": "gemini_search"}))
                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                    grounding = candidate.grounding_metadata
                    if hasattr(grounding, 'search_entry_point') and grounding.search_entry_point:
                        docs.append(Document(page_content="Google ê²€ìƒ‰ ì œì•ˆì‚¬í•­ ë° ê´€ë ¨ ë§í¬", metadata={"title": "ê²€ìƒ‰ ì œì•ˆ", "source": "search_suggestions"}))
                    if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                        for i, chunk in enumerate(grounding.grounding_chunks):
                            if hasattr(chunk, 'web') and chunk.web:
                                web_info = chunk.web
                                docs.append(Document(
                                    page_content=f"ì¶œì²˜ {i+1}ì—ì„œ ì°¸ì¡°ëœ ì •ë³´",
                                    metadata={"title": getattr(web_info, 'title', f'ì›¹ ì¶œì²˜ {i+1}'), 
                                              "url": getattr(web_info, 'uri', ''), 
                                              "source": "grounding_source"}
                                ))
                    if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                        print(f"[DEBUG] ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬: {grounding.web_search_queries}")
            
            print(f"[DEBUG] Gemini ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ {len(docs)}ê°œ ìƒì„±")
            self._save_search_result_to_cache(query, docs, 'gemini_search', character_info)
            return docs
        except Exception as e:
            print(f"âŒ Gemini ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì˜¤ë¥˜: {e}")
            return []

    def _hybrid_search(self, query: str, character_info: Optional[Dict]) -> Dict[str, Any]:
        cached_result = self._get_cached_search_result(query, 'hybrid_search', character_info)
        if cached_result:
            print("ğŸ”„ ìºì‹œëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result

        search_start_time = time.time()
        enhanced_query = self._enhance_query_with_character(query, character_info)
        times = {"internal_search": 0.0, "web_search": 0.0, "total_search": 0.0}

        def _search_internal():
            start = time.time()
            try:
                print("ğŸ”„ ë‚´ë¶€ RAG ê²€ìƒ‰ ì‹œì‘...")
                docs = self.internal_retriever.get_relevant_documents(enhanced_query)
                times["internal_search"] = time.time() - start
                print(f"âœ… ë‚´ë¶€ RAG ê²€ìƒ‰ ì™„ë£Œ: {times['internal_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["internal_search"] = time.time() - start
                print(f"âŒ ë‚´ë¶€ RAG ê²€ìƒ‰ ì˜¤ë¥˜ ({times['internal_search']:.2f}ì´ˆ): {e}")
                return []

        def _search_web():
            start = time.time()
            try:
                print("ğŸ”„ ì›¹ ê²€ìƒ‰ (Gemini) ì‹œì‘...")
                docs = self._gemini_search_grounding(query, character_info)
                times["web_search"] = time.time() - start
                print(f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {times['web_search']:.2f}ì´ˆ, {len(docs)}ê°œ ë¬¸ì„œ")
                return docs
            except Exception as e:
                times["web_search"] = time.time() - start
                print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ ({times['web_search']:.2f}ì´ˆ): {e}")
                return []

        print("ğŸš€ ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘ (ë‚´ë¶€ RAG + ì›¹)")
        with ThreadPoolExecutor(max_workers=2) as executor:
            internal_future = executor.submit(_search_internal)
            web_future = executor.submit(_search_web)
            internal_docs = internal_future.result()
            web_docs = web_future.result()
        
        times["total_search"] = time.time() - search_start_time
        print(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ - ì´ {times['total_search']:.2f}ì´ˆ")

        internal_context_str = self._format_docs_to_context_string(internal_docs, "ë‚´ë¶€")
        web_context_str = self._format_web_search_docs_to_context_string(web_docs)
        
        result = {
            "all_docs": internal_docs + web_docs,
            "internal_docs": internal_docs,
            "web_docs": web_docs,
            "internal_context_provided_to_llm": internal_context_str, # FastAPIì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ë¡œ ë³€ê²½
            "web_context_provided_to_llm": web_context_str,       # FastAPIì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ë¡œ ë³€ê²½
            "used_web_search": bool(web_docs),
            "enhanced_query": enhanced_query,
            "search_times": times
        }
        self._save_search_result_to_cache(query, result, 'hybrid_search', character_info)
        return result

    # --- ê³µê°œ API ë©”ì†Œë“œ ---
    def get_answer(self, query: str, character_info: Optional[Dict] = None) -> Dict[str, Any]:
        total_start_time = time.time()
        
        print(f"\n[INFO] ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: \"{query}\"")
        char_desc_parts = []
        if character_info: # FastAPIì—ì„œ ë³€í™˜ëœ character_info ì‚¬ìš©
            if class_info := character_info.get('class'):
                char_desc_parts.append(class_info)
            if fame_info := character_info.get('fame'):
                char_desc_parts.append(f"{fame_info}ëª…ì„±")
            if char_desc_parts:
                 print(f"[INFO] ìºë¦­í„°: {' '.join(char_desc_parts)}")

        char_context_for_llm = "ìºë¦­í„° ì •ë³´ ì—†ìŒ."
        if character_info: # FastAPIì—ì„œ ë³€í™˜ëœ character_info ì‚¬ìš©
            details = []
            if class_info := character_info.get('class'):
                details.append(f"- ì§ì—…: {class_info}")
            if fame_info := character_info.get('fame'):
                details.append(f"- ëª…ì„±: {fame_info}")
            if weapon_info := character_info.get('weapon'):
                details.append(f"- ë¬´ê¸°: {weapon_info}")
            if epic_num := character_info.get('epicNum'):
                details.append(f"- ì—í”½ ì•„ì´í…œ ê°œìˆ˜: {epic_num}")
            if originality_num := character_info.get('originalityNum'):
                details.append(f"- íƒœì´ˆ ì•„ì´í…œ ê°œìˆ˜: {originality_num}")
            if title_info := character_info.get('title'):
                details.append(f"- ì¹­í˜¸: {title_info}")
            if set_item_name := character_info.get('set_item_name'):
                set_rarity = character_info.get('set_item_rarity', '')
                details.append(f"- ì„¸íŠ¸ ì•„ì´í…œ: {set_item_name} ({set_rarity} ë“±ê¸‰)")
            if creature_info := character_info.get('creature'):
                details.append(f"- í¬ë¦¬ì³: {creature_info}")
            if aura_info := character_info.get('aura'):
                details.append(f"- ì˜¤ë¼: {aura_info}")

            if details:
                char_context_for_llm = "ì‚¬ìš©ì ìºë¦­í„° ì •ë³´:\n" + "\n".join(details)
                char_context_for_llm += "\n\nìœ„ ìºë¦­í„° ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”."
            else:
                char_context_for_llm = "ìºë¦­í„° ì •ë³´ê°€ ì œê³µë˜ì—ˆìœ¼ë‚˜, ì„¸ë¶€ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        search_results = self._hybrid_search(query, character_info)
        
        llm_start_time = time.time()
        print("ğŸ”„ LLM ë‹µë³€ ìƒì„± ì¤‘...")
        formatted_prompt = self.hybrid_prompt.format(
            internal_context=search_results["internal_context_provided_to_llm"], # í‚¤ ì¼ì¹˜
            web_context=search_results["web_context_provided_to_llm"],       # í‚¤ ì¼ì¹˜
            question=query,
            character_info=char_context_for_llm
        )
        
        try:
            llm_response = self.llm.invoke(formatted_prompt).content
        except Exception as e:
            print(f"âŒ LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            llm_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        llm_elapsed_time = time.time() - llm_start_time
        total_elapsed_time = time.time() - total_start_time
        
        print(f"âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ ({llm_elapsed_time:.2f}ì´ˆ)")
        print(f"[INFO] ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        
        # FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í‚¤ë¡œ ë°˜í™˜ê°’ êµ¬ì„±
        return {
            "result": llm_response,
            "source_documents": search_results["all_docs"],
            "used_web_search": search_results["used_web_search"],
            "internal_docs": search_results["internal_docs"], # FastAPIì—ì„œ convert_docs_to_dictë¡œ ì²˜ë¦¬
            "web_docs": search_results["web_docs"],         # FastAPIì—ì„œ convert_docs_to_dictë¡œ ì²˜ë¦¬
            "enhanced_query": search_results["enhanced_query"],
            "execution_times": {
                "total": total_elapsed_time,
                "llm": llm_elapsed_time,
                "search": search_results["search_times"]
            },
            "internal_context": search_results["internal_context_provided_to_llm"], # í‚¤ ì¼ì¹˜
            "web_context": search_results["web_context_provided_to_llm"]        # í‚¤ ì¼ì¹˜
        }

class MetadataAwareRetriever:
    def __init__(self, base_retriever, top_n: int = 15):
        self.base_retriever = base_retriever
        self.top_n = top_n
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        docs = self.base_retriever.get_relevant_documents(query)
        scored_docs = []
        for doc in docs:
            score = 1.0
            meta = doc.metadata or {}
            try: views = int(meta.get("views", 0)); score += 0.2 if views > 100000 else (0.1 if views > 10000 else 0)
            except ValueError: pass
            try: likes = int(meta.get("likes", 0)); score += 0.1 if likes > 100 else (0.05 if likes > 50 else 0)
            except ValueError: pass
            try: priority = float(meta.get("priority_score", 0.0)); score += priority * 0.1
            except ValueError: pass
            try: content_s = float(meta.get("content_score", 0.0)); score += content_s * 0.01
            except ValueError: pass
            if class_name := meta.get("class_name"): # VectorDBì˜ class_nameì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                if isinstance(class_name, str) and class_name.lower() in query.lower():
                    score += 0.3
            scored_docs.append((doc, score))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored_docs[:self.top_n]]

_rag_service_instance: Optional[RAGService] = None

def get_rag_service() -> RAGService:
    global _rag_service_instance
    if _rag_service_instance is None:
        print("âœ¨ ìƒˆë¡œìš´ RAGService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± âœ¨")
        _rag_service_instance = RAGService()
    return _rag_service_instance

def get_rag_answer(query: str, character_info: Optional[Dict] = None) -> Dict[str, Any]:
    service = get_rag_service()
    return service.get_answer(query, character_info)

if __name__ == "__main__":
    print("RAG ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # FastAPIì—ì„œ ë³€í™˜ëœ í˜•íƒœì™€ ìœ ì‚¬í•œ í…ŒìŠ¤íŠ¸ìš© ìºë¦­í„° ì •ë³´
    test_character_info_transformed = {
        "class": "ì•„ìˆ˜ë¼(ë‚¨ê·€ê²€ì‚¬)",
        "fame": "52000",
        "weapon": "íƒœì´ˆ ë¬´ê¸°",
        "epicNum": 5,
        "originalityNum": 1,
        "title": "ì„¸ë¦¬ì•„ ì¹­í˜¸",
        "set_item_name": "ì¹ í‘ì˜ ì •í™” ì„¸íŠ¸",
        "set_item_rarity": "ë ˆì „ë”ë¦¬2",
        "creature": "ì„¸ë¦¬ì•„ í¬ë¦¬ì³",
        "aura": "ì„¸ë¦¬ì•„ ì˜¤ë¼"
    }
    
    try:
        rag_service = get_rag_service()
        test_queries = [
            "ì—¬ê¸°ì„œ ë” ìŠ¤í™ì—… í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í•´?",
            "ë ˆê¸°ì˜¨ ë² ëˆ„ìŠ¤ ê°€ì´ë“œë¼ì¸ ì•Œë ¤ì¤˜",
            "ì´ë²ˆì£¼ ì£¼ìš” ì´ë²¤íŠ¸ ë­ ìˆì–´?",
        ]
        
        for i, q in enumerate(test_queries):
            print(f"\n--- í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {i+1} ---")
            answer_with_char = rag_service.get_answer(q, test_character_info_transformed)
            print(f"\n[ë‹µë³€ (ìºë¦­í„° ì •ë³´ í¬í•¨)]\n{answer_with_char['result']}")
            
            # internal_docs, web_docsëŠ” Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ, FastAPI ì—”ë“œí¬ì¸íŠ¸ì²˜ëŸ¼ ì²˜ë¦¬í•˜ë ¤ë©´ ë³€í™˜ í•„ìš”
            internal_docs_count = len(answer_with_char.get("internal_docs", []))
            web_docs_count = len(answer_with_char.get("web_docs", []))

            print(f"  (ì›¹ ê²€ìƒ‰ ì‚¬ìš©: {answer_with_char['used_web_search']}, ë‚´ë¶€ ë¬¸ì„œ: {internal_docs_count}, ì›¹ ë¬¸ì„œ: {web_docs_count})")
            print(f"  (ì‹¤í–‰ ì‹œê°„: ì´ {answer_with_char['execution_times']['total']:.2f}s, LLM {answer_with_char['execution_times']['llm']:.2f}s)")
            
            if i < len(test_queries) - 1:
                print("\n... ë‹¤ìŒ ì§ˆë¬¸ ëŒ€ê¸° ì¤‘ (1ì´ˆ) ...") # í…ŒìŠ¤íŠ¸ ì‹œê°„ ë‹¨ì¶•
                time.sleep(1)

    except RuntimeError as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ëŸ°íƒ€ì„ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        import traceback
        traceback.print_exc()
    print("\n--- RAG ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ---")