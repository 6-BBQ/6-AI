"""
RAG ì„œë¹„ìŠ¤ ëª¨ë“ˆ - ìºë¦­í„° ì •ë³´ë¥¼ í™œìš©í•œ ê°œì„ ëœ ê²€ìƒ‰
"""

from __future__ import annotations
import os, time, hashlib, pickle
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Optional
from pathlib import Path
from dotenv import load_dotenv

# LLM & ì„ë² ë”©
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# ê²€ìƒ‰ ê´€ë ¨
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker

# ì›¹ ê²€ìƒ‰
from openai import OpenAI

from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document

load_dotenv()

class RAGService:
    """RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ - ìºë¦­í„° ì •ë³´ ê¸°ë°˜ ê²€ìƒ‰ ê°•í™”"""
    
    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # í™˜ê²½ë³€ìˆ˜
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        self.perplexity_api_key = os.getenv("PERPLEXITY_API_KEY")
        
        if not self.gemini_api_key:
            raise RuntimeError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”!")
        
        # ì´ˆê¸°í™”
        self._initialize_rag_components()
        self._setup_llm_and_prompt()
    
    def _initialize_rag_components(self):
        """RAG ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        # ë²¡í„° DB ì„¤ì •
        chroma_dir = "vector_db/chroma"
        embed_model = "text-embedding-3-large"
        embed_fn = OpenAIEmbeddings(model=embed_model)
        
        print("ğŸ”„ ë²¡í„° DB ë¡œë”©...")
        self.vectordb = Chroma(persist_directory=chroma_dir, embedding_function=embed_fn)
        self.vector_retriever = self.vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 6, "fetch_k": 12, "lambda_mult": 0.8},
        )
        print("âœ… ë²¡í„° DB ë¡œë”© ì™„ë£Œ")
        
        # BM25 ë¡œë“œ
        self.bm25_retriever = self._load_bm25_index()
        
        # Ensemble ì„¤ì •
        self.rrf_retriever = EnsembleRetriever(
            retrievers=[self.vector_retriever, self.bm25_retriever],
            weights=[0.5, 0.5],
        )
        
        # Cross-Encoder ì„¤ì •
        cross_encoder = self._load_cross_encoder()
        compressor = CrossEncoderReranker(
            model=cross_encoder,
            top_n=6
        )
        self.internal_retriever = ContextualCompressionRetriever(
            base_retriever=self.rrf_retriever,
            base_compressor=compressor,
        )
        
        elapsed_time = time.time() - start_time
        print(f"ğŸ‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
    
    def _build_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìºì‹±"""
        print("ğŸ”„ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        store_data = self.vectordb.get(include=["documents", "metadatas"])
        docs_for_bm25 = []
        
        for txt, meta in zip(store_data["documents"], store_data["metadatas"]):
            enhanced_content = txt
            if meta:
                if meta.get("title"):
                    enhanced_content = f"ì œëª©: {meta['title']}\\n{txt}"
                if meta.get("class_name"):
                    enhanced_content += f"\\nì§ì—…: {meta['class_name']}"
            
            docs_for_bm25.append(Document(page_content=enhanced_content, metadata=meta))
        
        bm25_retriever = BM25Retriever.from_documents(docs_for_bm25)
        bm25_retriever.k = 6
        
        # ìºì‹± ì €ì¥
        cache_file = self.cache_dir / "bm25_index.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(bm25_retriever, f)
            print(f"âœ… BM25 ì¸ë±ìŠ¤ ìºì‹œ ì €ì¥: {cache_file}")
        except Exception as e:
            print(f"âš ï¸ BM25 ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return bm25_retriever
    
    def _load_bm25_index(self):
        """ìºì‹œì—ì„œ BM25 ì¸ë±ìŠ¤ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ êµ¬ì¶•"""
        cache_file = self.cache_dir / "bm25_index.pkl"
        cache_expiry = 60 * 60 * 12  # 12ì‹œê°„
        
        if cache_file.exists():
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age < cache_expiry:
                try:
                    print("ğŸ”„ ìºì‹œëœ BM25 ì¸ë±ìŠ¤ ë¡œë”©...")
                    with open(cache_file, 'rb') as f:
                        bm25_retriever = pickle.load(f)
                    print("âœ… BM25 ì¸ë±ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ")
                    return bm25_retriever
                except Exception as e:
                    print(f"âš ï¸ BM25 ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return self._build_bm25_index()
    
    def _load_cross_encoder(self):
        """í¬ë¡œìŠ¤ ì¸ì½”ë” ëª¨ë¸ ë¡œë“œ (ìºì‹œ í™œìš©)"""
        cache_file = self.cache_dir / "cross_encoder.pkl"
        cache_expiry = 60 * 60 * 24  # 24ì‹œê°„
        
        if cache_file.exists():
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age < cache_expiry:
                try:
                    print("ğŸ”„ ìºì‹œëœ Cross-Encoder ë¡œë”©...")
                    with open(cache_file, 'rb') as f:
                        cross_encoder = pickle.load(f)
                    print("âœ… Cross-Encoder ìºì‹œ ë¡œë“œ ì™„ë£Œ")
                    return cross_encoder
                except Exception as e:
                    print(f"âš ï¸ Cross-Encoder ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("ğŸ”„ Cross-Encoder ëª¨ë¸ ë¡œë”©...")
        cross_encoder = HuggingFaceCrossEncoder(
            model_name="cross-encoder/ms-marco-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"}
        )
        
        # ìºì‹± ì €ì¥
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(cross_encoder, f)
            print(f"âœ… Cross-Encoder ìºì‹œ ì €ì¥: {cache_file}")
        except Exception as e:
            print(f"âš ï¸ Cross-Encoder ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return cross_encoder
    
    def _setup_llm_and_prompt(self):
        """LLMê³¼ í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=self.gemini_api_key,
            model="models/gemini-2.5-flash-preview-05-20",
            temperature=0,
        )
        
        self.hybrid_prompt = PromptTemplate(
            input_variables=["internal_context", "web_context", "question", "character_info"],
            template="""
ë‹¹ì‹ ì€ ë˜ì „ì•¤íŒŒì´í„° ì „ë¬¸ ìŠ¤í™ì—… ê°€ì´ë“œ ì±—ë´‡ì…ë‹ˆë‹¤.  
â€» ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ì •ë³´ë§Œ í™œìš©í•´ ë‹µë³€í•˜ì„¸ìš”.

[ìºë¦­í„° ì •ë³´]
{character_info}

[ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤]
{internal_context}

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
{web_context}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë‹µë³€ ê·œì¹™
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ì œê³µëœ ì •ë³´ ì™¸ì˜ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì œê³µëœ ì •ë³´ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
- ë‚´ë¶€ ë°ì´í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ê³ , ì™¸ë¶€ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ëŠ” í˜•íƒœë¡œ ì‚¬ìš©í•˜ì„¸ìš”ìš”.
- ë‚´ì™¸ë¶€ ëª¨ë“  ë°ì´í„°ëŠ” 2025ë…„ ë°ì´í„°ë§Œ í™œìš©í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë²”ìœ„ë§Œ ë‹¤ë£¨ë©°, ê´€ë ¨ ì—†ëŠ” ì„¤ëª…ì€ ìƒëµí•˜ì„¸ìš”.
- ìˆœì„œë¥¼ ë‚˜ì—´í•˜ë©° ì¶œì²˜ì™€ í•¨ê»˜ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.

[ì´ë²¤íŠ¸ ì•ˆë‚´ ê¸°ì¤€]
- ì¢…ë£Œì¼ì´ 2025-05-22 ì´í›„ â†’ ì°¸ì—¬ ê¶Œì¥
- ì¢…ë£Œëœ ì´ë²¤íŠ¸ â†’ "í•´ë‹¹ ì´ë²¤íŠ¸ëŠ” ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
- ì¢…ë£Œì¼ì´ ì—†ì„ ê²½ìš° â†’ "ì´ë²¤íŠ¸ ì¢…ë£Œì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""
        )
    
    def _generate_cache_key(self, query, character_info=None):
        """ìºì‹œ í‚¤ ìƒì„± (ìºë¦­í„° ì •ë³´ í¬í•¨)"""
        cache_content = query
        if character_info:
            # ìºë¦­í„° ì •ë³´ë¥¼ ìºì‹œ í‚¤ì— í¬í•¨
            char_key = f"{character_info.get('class_name', '')}-{character_info.get('level', '')}-{character_info.get('fame', '')}"
            cache_content = f"{query}|{char_key}"
        return hashlib.md5(cache_content.encode('utf-8')).hexdigest()
    
    def _get_from_cache(self, query, cache_type='search', character_info=None):
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        cache_key = self._generate_cache_key(query, character_info)
        cache_file = self.cache_dir / f"{cache_type}_{cache_key}.pkl"
        cache_expiry = 60 * 60 * 12  # 12ì‹œê°„
        
        if cache_file.exists():
            file_age = time.time() - cache_file.stat().st_mtime
            if file_age < cache_expiry:
                try:
                    with open(cache_file, 'rb') as f:
                        return pickle.load(f)
                except Exception:
                    pass
        return None
    
    def _save_to_cache(self, query, result, cache_type='search', character_info=None):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        cache_key = self._generate_cache_key(query, character_info)
        cache_file = self.cache_dir / f"{cache_type}_{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _enhance_query_with_character(self, query: str, character_info: Optional[Dict] = None) -> str:
        """ìºë¦­í„° ì •ë³´ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ê°•í™”"""
        if not character_info:
            return query
        
        enhancements = []
        
        # ì§ì—…ëª… ì¶”ê°€
        if character_info.get('class_name'):
            enhancements.append(character_info['class_name'])
        
        # ëª…ì„±ì¶”ê°€
        if character_info.get('fame'):
           enhancements.append(str(character_info['fame']))
        
        # ê°•í™”ëœ ì¿¼ë¦¬ ìƒì„±
        if enhancements:
            enhanced_query = f"{' '.join(enhancements)} {query}"
            print(f"[DEBUG] ì¿¼ë¦¬ ê°•í™”: '{query}' â†’ '{enhanced_query}'")
            return enhanced_query
        
        return query
    
    def _perplexity_web_search(self, query: str, character_info: Optional[Dict] = None, max_results=3) -> List[Document]:
        """Perplexity ì›¹ ê²€ìƒ‰"""
        cached_result = self._get_from_cache(query, 'web_search', character_info)
        if cached_result:
            print("ğŸ”„ ìºì‹œëœ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result
        
        if not self.perplexity_api_key:
            return []
        
        try:
            client = OpenAI(
                api_key=self.perplexity_api_key,
                base_url="https://api.perplexity.ai"
            )
            
            # ìºë¦­í„° ì •ë³´ë¡œ ê°•í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
            enhanced_query = self._enhance_query_with_character(query, character_info)
            
            messages = [
                {
                    "role": "system",
                    "content": (
                        "You are a helpful assistant that provides information about "
                        "Dungeon & Fighter (DNF) game. Focus on providing the most "
                        "recent and accurate information about character progression, equipment, "
                        "and optimization guides. Be concise and direct."
                    )
                },
                {"role": "user", "content": f"2025 ìµœì‹  ë˜ì „ì•¤íŒŒì´í„° {enhanced_query} ìŠ¤í™ì—… ê°€ì´ë“œ"}
            ]
            
            response = client.chat.completions.create(
                model="sonar",
                messages=messages,
                max_tokens=1000,
                temperature=0
            )
            
            docs = []
            content = response.choices[0].message.content
            docs.append(Document(
                page_content=content,
                metadata={"title": "Perplexity ê²€ìƒ‰ ê²°ê³¼", "url": "", "source": "web_search"}
            ))
            
            self._save_to_cache(query, docs, 'web_search', character_info)
            return docs
            
        except Exception as e:
            print(f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _smart_hybrid_search(self, query, character_info: Optional[Dict] = None):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë‚´ë¶€ + ì›¹)"""
        cached_result = self._get_from_cache(query, 'hybrid_search', character_info)
        if cached_result:
            return cached_result
        
        # ìºë¦­í„° ì •ë³´ë¡œ ì¿¼ë¦¬ ê°•í™”
        enhanced_query = self._enhance_query_with_character(query, character_info)
        
        def get_internal_results():
            try:
                return self.internal_retriever.get_relevant_documents(enhanced_query)
            except Exception as e:
                print(f"[ERROR] ë‚´ë¶€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                return []
        
        def get_web_results():
            try:
                return self._perplexity_web_search(query, character_info)
            except Exception as e:
                print(f"[ERROR] ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                return []
        
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            internal_future = executor.submit(get_internal_results)
            web_future = executor.submit(get_web_results)
            
            internal_docs = internal_future.result()
            web_docs = web_future.result()
        
        print(f"[DEBUG] ê²€ìƒ‰ ê²°ê³¼: ë‚´ë¶€ {len(internal_docs)}ê°œ, ì›¹ {len(web_docs)}ê°œ")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        internal_context_parts = []
        for i, doc in enumerate(internal_docs):
            content = f"[ë‚´ë¶€ ë¬¸ì„œ {i+1}] {doc.page_content}"
            if doc.metadata and doc.metadata.get("url"):
                content += f"\\nì°¸ê³  ë§í¬: {doc.metadata['url']}"
            internal_context_parts.append(content)
        
        internal_context = "\\n\\n".join(internal_context_parts)
        
        web_context = "\\n\\n".join([
            f"[ì›¹ ë¬¸ì„œ {i+1} - 2025ë…„ ìµœì‹  ì •ë³´] {doc.page_content}"
            for i, doc in enumerate(web_docs)
        ]) if web_docs else ""
        
        if not internal_docs and not web_docs:
            internal_context = "[ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ] ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        result = {
            "all_docs": internal_docs + web_docs,
            "internal_docs": internal_docs,
            "web_docs": web_docs,
            "internal_context": internal_context,
            "web_context": web_context,
            "used_web_search": len(web_docs) > 0,
            "enhanced_query": enhanced_query  # ë””ë²„ê¹…ìš©
        }
        
        self._save_to_cache(query, result, 'hybrid_search', character_info)
        return result
    
    def get_answer(self, query: str, character_info: Optional[Dict] = None):
        """RAG ë‹µë³€ ìƒì„±"""
        total_start_time = time.time()
        
        print(f"[INFO] ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {query}")
        if character_info:
            print(f"[INFO] ìºë¦­í„°: {character_info.get('class_name', '')} {character_info.get('level', '')}ë ˆë²¨ {character_info.get('fame', '')}ëª…ì„±")
        
        # ìºë¦­í„° ì •ë³´ í¬ë§·íŒ… (LLM í”„ë¡¬í”„íŠ¸ìš©)
        character_context = ""
        if character_info:
            character_context = f"""
ì‚¬ìš©ì ìºë¦­í„° ì •ë³´:
- ì§ì—…: {character_info.get('class_name', 'ì •ë³´ ì—†ìŒ')}
- ëª…ì„±: {character_info.get('fame', 'ì •ë³´ ì—†ìŒ')}

ìœ„ ìºë¦­í„° ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
"""
        
        # ê²€ìƒ‰ ìˆ˜í–‰ (ìºë¦­í„° ì •ë³´ í™œìš©)
        search_results = self._smart_hybrid_search(query, character_info)
        
        # LLM ë‹µë³€ ìƒì„±
        llm_start_time = time.time()
        formatted_prompt = self.hybrid_prompt.format(
            internal_context=search_results["internal_context"],
            web_context=search_results["web_context"],
            question=query,
            character_info=character_context
        )
        
        response = self.llm.invoke(formatted_prompt).content
        
        llm_elapsed_time = time.time() - llm_start_time
        total_elapsed_time = time.time() - total_start_time
        
        print(f"[INFO] ë‹µë³€ ìƒì„± ì™„ë£Œ: {total_elapsed_time:.2f}ì´ˆ")
        
        return {
            "result": response,
            "source_documents": search_results["all_docs"],
            "used_web_search": search_results["used_web_search"],
            "internal_docs": search_results["internal_docs"],
            "web_docs": search_results["web_docs"],
            "enhanced_query": search_results.get("enhanced_query", query),  # ë””ë²„ê¹…ìš©
            "execution_times": {
                "total": total_elapsed_time,
                "llm": llm_elapsed_time
            }
        }





# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_rag_service = None

def get_rag_service() -> RAGService:
    """RAG ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service

def get_rag_answer(query: str, character_info: Optional[Dict] = None):
    """RAG ë‹µë³€ ìƒì„± (ê°„í¸ í•¨ìˆ˜)"""
    service = get_rag_service()
    return service.get_answer(query, character_info)
