"""
Langchain í˜¸í™˜ Gemini ì„ë² ë”© í´ë˜ìŠ¤
"""
import os
from typing import List, Optional
from langchain.embeddings.base import Embeddings
from google import genai
from google.genai import types
import time
import logging

logger = logging.getLogger(__name__)

class GeminiEmbeddings(Embeddings):
    """Langchain í˜¸í™˜ Gemini ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        model: str = "gemini-embedding-exp-03-07",
        api_key: Optional[str] = None,
        task_type: str = "RETRIEVAL_DOCUMENT",
        rate_limit_delay: float = 1.0
    ):
        """
        Args:
            model: Gemini ì„ë² ë”© ëª¨ë¸ëª…
            api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
            task_type: ì„ë² ë”© íƒœìŠ¤í¬ íƒ€ì… (RAGì˜ ê²½ìš° RETRIEVAL_DOCUMENT/RETRIEVAL_QUERY)
            rate_limit_delay: API ìš”ì²­ ê°„ ì§€ì—°ì‹œê°„ (ì´ˆ)
        """
        self.model = model
        self.task_type = task_type
        self.rate_limit_delay = rate_limit_delay
        
        # API í‚¤ ì„¤ì •
        api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = genai.Client(api_key=api_key)
        
        logger.info(f"ğŸ¤– Gemini ì„ë² ë”© ì´ˆê¸°í™”: {model} (task_type: {task_type})")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ìƒì„± (RETRIEVAL_DOCUMENT íƒœìŠ¤í¬) - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        logger.info(f"ğŸ“„ ë¬¸ì„œ ì„ë² ë”© ë°°ì¹˜ ìƒì„±: {len(texts)}ê°œ")
        
        embeddings = []
        batch_size = 200  # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches}: {len(batch_texts)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
            
            try:
                # ë°°ì¹˜ë¡œ í•œë²ˆì— ì„ë² ë”© ìƒì„±
                result = self.client.models.embed_content(
                    model=self.model,
                    contents=batch_texts,  # ë¦¬ìŠ¤íŠ¸ë¡œ í•œë²ˆì— ì „ë‹¬
                    config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
                )
                
                # ë°°ì¹˜ ê²°ê³¼ì—ì„œ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
                for embedding in result.embeddings:
                    embeddings.append(embedding.values)
                
                logger.info(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ: {len(batch_texts)}ê°œ ì„ë² ë”© ìƒì„±")
                
                # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (API ì•ˆì •ì„±)
                if i + batch_size < len(texts):
                    time.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
                logger.info("â¸ï¸ 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(5.0)
                
                try:
                    # ì¬ì‹œë„
                    result = self.client.models.embed_content(
                        model=self.model,
                        contents=batch_texts,
                        config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
                    )
                    
                    for embedding in result.embeddings:
                        embeddings.append(embedding.values)
                    
                    logger.info(f"âœ… ë°°ì¹˜ {batch_num} ì¬ì‹œë„ ì„±ê³µ")
                    
                except Exception as e2:
                    logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì¬ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
                    # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë²¡í„°ë¡œ ëŒ€ì²´
                    for _ in range(len(batch_texts)):
                        if embeddings:
                            embedding_vector = [0.0] * len(embeddings[0])
                        else:
                            embedding_vector = [0.0] * 768  # ê¸°ë³¸ ì°¨ì›
                        embeddings.append(embedding_vector)
        
        logger.info(f"âœ… ë¬¸ì„œ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(embeddings)}ê°œ")
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """ì¿¼ë¦¬ì˜ ì„ë² ë”© ìƒì„± (RETRIEVAL_QUERY íƒœìŠ¤í¬)"""
        logger.debug(f"ğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±: '{text[:50]}...'")
        
        try:
            # RETRIEVAL_QUERY íƒœìŠ¤í¬ íƒ€ì…ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
            result = self.client.models.embed_content(
                model=self.model,
                contents=text,
                config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY")
            )
            
            embedding_vector = result.embeddings[0].values
            logger.debug(f"âœ… ì¿¼ë¦¬ ì„ë² ë”© ì™„ë£Œ: {len(embedding_vector)}ì°¨ì›")
            return embedding_vector
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì°¨ì›ì˜ ë¹ˆ ë²¡í„° ë°˜í™˜
            return [0.0] * 768
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¹„ë™ê¸° ë¬¸ì„œ ì„ë² ë”© (í˜„ì¬ëŠ” ë™ê¸° ë²„ì „ê³¼ ë™ì¼)"""
        return self.embed_documents(texts)
    
    async def aembed_query(self, text: str) -> List[float]:
        """ë¹„ë™ê¸° ì¿¼ë¦¬ ì„ë² ë”© (í˜„ì¬ëŠ” ë™ê¸° ë²„ì „ê³¼ ë™ì¼)"""
        return self.embed_query(text)
