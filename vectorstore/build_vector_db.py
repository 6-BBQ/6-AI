from __future__ import annotations

import json
import logging
import os
import shutil
import argparse
import hashlib
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime

from dotenv import load_dotenv
import torch

from langchain.docstore.document import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •
load_dotenv()
PROCESSED_PATH = Path("data/processed/processed_docs.jsonl")
CHROMA_DIR = "vector_db/chroma"
VECTORDB_CACHE = Path("vector_db/vectordb_cache.json")  # ë²¡í„°DB ìºì‹œ
BATCH_SIZE = 200
MODEL_NAME = "dragonkue/bge-m3-ko"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
log = logging.getLogger("build_vector_db")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ ì¦ë¶„ ì²˜ë¦¬ ë„êµ¬

def load_vectordb_cache() -> Set[str]:
    """ë²¡í„°DBì— ì´ë¯¸ ì¶”ê°€ëœ ë¬¸ì„œ ID ì§‘í•© ë¡œë“œ"""
    try:
        if VECTORDB_CACHE.exists():
            with VECTORDB_CACHE.open('r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_doc_ids', []))
    except Exception as e:
        log.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return set()

def save_vectordb_cache(processed_ids: Set[str]) -> None:
    """ë²¡í„°DB ìºì‹œ ì €ì¥"""
    try:
        VECTORDB_CACHE.parent.mkdir(parents=True, exist_ok=True)
        cache_data = {
            'processed_doc_ids': list(processed_ids),
            'last_updated': datetime.now().isoformat(),
            'model_name': MODEL_NAME,
            'total_docs': len(processed_ids)
        }
        with VECTORDB_CACHE.open('w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_existing_doc_ids_from_db(vectordb) -> Set[str]:
    """ê¸°ì¡´ ë²¡í„°DBì—ì„œ ë¬¸ì„œ ID ì§‘í•© ì¶”ì¶œ"""
    try:
        # Chromaì—ì„œ ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        collection = vectordb.get()
        existing_ids = set()
        
        if collection and 'metadatas' in collection:
            for metadata in collection['metadatas']:
                if metadata and 'doc_id' in metadata:
                    existing_ids.add(metadata['doc_id'])
        
        log.info(f"ğŸ“Š ê¸°ì¡´ ë²¡í„°DBì—ì„œ {len(existing_ids)}ê°œ ë¬¸ì„œ ID ë°œê²¬")
        return existing_ids
    except Exception as e:
        log.warning(f"ê¸°ì¡´ DB ë¬¸ì„œ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return set()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ JSONL â†’ Document ë¦¬ìŠ¤íŠ¸ ë³€í™˜ í•¨ìˆ˜

def load_docs(path: Path, incremental: bool = False, existing_ids: Set[str] = None) -> List[Document]:
    """ë¬¸ì„œ ë¡œë“œ (ì¦ë¶„ ì²˜ë¦¬ ì§€ì›)"""
    docs = []
    skipped = 0
    
    if existing_ids is None:
        existing_ids = set()
    
    with path.open(encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            try:
                raw: Dict = json.loads(line)
                doc_id = raw.get("id", "")
                
                # ì¦ë¶„ ëª¨ë“œì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œ ê±´ë„ˆë›°ê¸°
                if incremental and doc_id in existing_ids:
                    skipped += 1
                    continue
                
                doc = Document(
                    page_content=raw["content"],
                    metadata={**raw["metadata"], "doc_id": doc_id}
                )
                docs.append(doc)
                
            except json.JSONDecodeError as e:
                log.warning(f"ì¤„ {line_num}ì—ì„œ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
            except Exception as e:
                log.warning(f"ì¤„ {line_num}ì—ì„œ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
    
    if incremental and skipped > 0:
        log.info(f"ğŸ”„ ì¦ë¶„ ëª¨ë“œ: {skipped}ê°œ ë¬¸ì„œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì²˜ë¦¬ë¨)")
    
    return docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ ë©”ì¸ í•¨ìˆ˜

def main(incremental: bool = False, force: bool = False):
    """ë²¡í„° DB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜"""
    
    mode_name = "ì¦ë¶„" if incremental else "ì „ì²´"
    log.info(f"ğŸš€ {mode_name} ëª¨ë“œ - í•œêµ­ì–´ BGE-m3-ko ê¸°ë°˜ ì„ë² ë”© ì‹œì‘")
    
    # ì„ë² ë”© í•¨ìˆ˜ ì •ì˜
    embedding_fn = HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    # ê°•ì œ ëª¨ë“œì¼ ë•Œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    if force and Path(CHROMA_DIR).exists():
        log.info("ğŸ—‘ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ Chroma í´ë” ì‚­ì œ")
        shutil.rmtree(CHROMA_DIR)
        if VECTORDB_CACHE.exists():
            VECTORDB_CACHE.unlink()
            log.info("ğŸ—‘ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ ìºì‹œ íŒŒì¼ ì‚­ì œ")
    
    # ë²¡í„°DB ì´ˆê¸°í™”
    vectordb = Chroma(
        persist_directory=CHROMA_DIR,
        embedding_function=embedding_fn,
    )
    
    # ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ ID ë¡œë“œ
    existing_ids = set()
    if incremental:
        # ìºì‹œì—ì„œ ë¡œë“œ (ë¹ ë¦„)
        cached_ids = load_vectordb_cache()
        
        # ì‹¤ì œ DBì—ì„œ ë¡œë“œ (ì •í™•í•¨)
        db_ids = get_existing_doc_ids_from_db(vectordb)
        
        # ë‘ ê²°ê³¼ë¥¼ í•©ì§‘í•©ìœ¼ë¡œ ì²˜ë¦¬ (ì•ˆì „í•¨)
        existing_ids = cached_ids.union(db_ids)
        log.info(f"ğŸ”„ ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ {len(existing_ids)}ê°œ")
    elif not force and Path(CHROMA_DIR).exists():
        # ì „ì²´ ëª¨ë“œì´ì§€ë§Œ forceê°€ ì•„ë‹Œ ê²½ìš°, ê¸°ì¡´ DB ìœ ì§€í•˜ê³  ì¶”ê°€
        log.info("ğŸ“‹ ì „ì²´ ëª¨ë“œ: ê¸°ì¡´ DBì— ìƒˆ ë¬¸ì„œ ì¶”ê°€")
        existing_ids = get_existing_doc_ids_from_db(vectordb)
    
    # ë¬¸ì„œ ë¡œë“œ
    if not PROCESSED_PATH.exists():
        log.error(f"âŒ ì „ì²˜ë¦¬ëœ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PROCESSED_PATH}")
        return
    
    all_docs = load_docs(PROCESSED_PATH, incremental, existing_ids)
    
    if not all_docs:
        log.info("âœ… ì²˜ë¦¬í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    total = len(all_docs)
    log.info(f"ğŸ“„ {total}ê°œ ìƒˆ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ")
    
    # ë°°ì¹˜ ì„ë² ë”© & ì—…ë¡œë“œ
    log.info("ğŸ”„ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘")
    processed_count = 0
    new_doc_ids = set()
    
    for i in range(0, total, BATCH_SIZE):
        batch = all_docs[i:i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        log.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}: {len(batch)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        try:
            vectordb.add_documents(batch)
            processed_count += len(batch)
            
            # ì²˜ë¦¬ëœ ë¬¸ì„œ ID ê¸°ë¡
            for doc in batch:
                doc_id = doc.metadata.get('doc_id', '')
                if doc_id:
                    new_doc_ids.add(doc_id)
            
            log.info(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ ({processed_count}/{total})")
            
            # ë°°ì¹˜ ê°„ ë”œë ˆì´
            if i + BATCH_SIZE < total:
                import time
                time.sleep(0.5)
                
        except Exception as e:
            log.error(f"âŒ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
            import time
            time.sleep(10.0)
            
            # ì¬ì‹œë„
            try:
                vectordb.add_documents(batch)
                processed_count += len(batch)
                
                for doc in batch:
                    doc_id = doc.metadata.get('doc_id', '')
                    if doc_id:
                        new_doc_ids.add(doc_id)
                        
                log.info(f"âœ… ì¬ì‹œë„ ì„±ê³µ: ë°°ì¹˜ {batch_num} ì™„ë£Œ")
            except Exception as e2:
                log.error(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {e2}")
                continue
    
    # ìºì‹œ ì—…ë°ì´íŠ¸ (ì¦ë¶„ ëª¨ë“œì¼ ë•Œ)
    if incremental or new_doc_ids:
        updated_ids = existing_ids.union(new_doc_ids)
        save_vectordb_cache(updated_ids)
        log.info(f"ğŸ’¾ ìºì‹œ ì—…ë°ì´íŠ¸: ì´ {len(updated_ids)}ê°œ ë¬¸ì„œ ID ì €ì¥")
    
    # ì™„ë£Œ ë©”ì‹œì§€
    log.info(f"ğŸ‰ ë²¡í„° DB ì €ì¥ ì™„ë£Œ â†’ {CHROMA_DIR}")
    log.info(f"ğŸ“ˆ ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ì„œ: {processed_count}ê°œ")
    log.info(f"ğŸ“Š ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(existing_ids) + len(new_doc_ids)}ê°œ")
    log.info(f"ğŸ§  ì‚¬ìš© ëª¨ë¸: {MODEL_NAME}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="ë˜íŒŒ ë²¡í„° DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ (ì¦ë¶„ ì²˜ë¦¬ ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ì „ì²´ ë²¡í„° DB êµ¬ì¶• (ê¸°ë³¸ ëª¨ë“œ)
  python build_vector_db.py
  
  # ì¦ë¶„ ë²¡í„° DB êµ¬ì¶• (ìƒˆë¡œìš´ ë¬¸ì„œë§Œ)
  python build_vector_db.py --incremental
  
  # ê°•ì œ ì „ì²´ ì¬êµ¬ì¶•
  python build_vector_db.py --force
  
  # ì¦ë¶„ + ìƒì„¸ ë¡œê·¸
  python build_vector_db.py --incremental --verbose
        """
    )
    
    parser.add_argument(
        "--incremental", 
        action="store_true", 
        default=True,
        help="ì¦ë¶„ ë²¡í„° DB êµ¬ì¶• (ê¸°ë³¸ê°’)"
    )
    
    parser.add_argument(
        "--full", 
        action="store_true", 
        help="ì „ì²´ ë²¡í„° DB êµ¬ì¶• (ì¦ë¶„ ë¬´ì‹œ)"
    )
    
    parser.add_argument(
        "--force", 
        action="store_true", 
        help="ê¸°ì¡´ ë²¡í„° DB ê°•ì œ ì‚­ì œ í›„ ì „ì²´ ì¬êµ¬ì¶•"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true", 
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=BATCH_SIZE,
        help=f"ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: {BATCH_SIZE})"
    )
    
    args = parser.parse_args()
    
    # ì „ì²´ ëª¨ë“œ ê²€ì‚¬
    if args.full:
        args.incremental = False
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    BATCH_SIZE = args.batch_size
    
    # ëª¨ë“œ ì¶©ëŒ ê²€ì‚¬
    if args.incremental and args.force:
        log.warning("âš ï¸ --incrementalê³¼ --forceë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ --forceê°€ ìš°ì„ ë©ë‹ˆë‹¤")
        args.incremental = False
    elif args.full and args.force:
        # --fullê³¼ --forceëŠ” ë™ì¼í•œ íš¨ê³¼
        pass
    
    # ì‹œì‘ ë©”ì‹œì§€
    mode_emoji = "ğŸ”„" if args.incremental else "ğŸ—‘ï¸" if args.force else "ğŸ“‹"
    mode_name = "ì¦ë¶„" if args.incremental else "ê°•ì œ ì¬êµ¬ì¶•" if args.force else "ì „ì²´"
    
    log.info(f"{mode_emoji} {mode_name} ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
    log.info(f"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
    log.info(f"   - ëª¨ë¸: {MODEL_NAME}")
    log.info(f"   - ì…ë ¥ íŒŒì¼: {PROCESSED_PATH}")
    log.info(f"   - ì¶œë ¥ ë””ë ‰í† ë¦¬: {CHROMA_DIR}")
    
    try:
        main(incremental=args.incremental, force=args.force)
        log.info("âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
    except KeyboardInterrupt:
        log.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        log.error(f"âŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
        raise
