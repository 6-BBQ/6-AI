from __future__ import annotations

import json
import logging
import sys
import shutil
import numpy as np
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))
from config import config
from typing import Dict, List, Set, Tuple
from datetime import datetime

import torch

from langchain.docstore.document import Document
from langchain_chroma import Chroma
# ì„ë² ë”© í•¨ìˆ˜ëŠ” config.create_embedding_function()ì„ ì‚¬ìš©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •
PROCESSED_DOCS_PATH       = Path(config.PROCESSED_DOCS_PATH)
VECTOR_DB_DIR             = config.VECTOR_DB_DIR
VECTORDB_CACHE_PATH       = Path(config.VECTORDB_CACHE_PATH)
JOB_EMBEDDINGS_PATH       = Path(config.JOB_EMBEDDINGS_PATH)
JOB_NAMES_PATH            = Path(config.JOB_NAMES_PATH)
EMBED_MODEL_NAME          = config.EMBED_MODEL_NAME
EMBED_BATCH_SIZE          = config.EMBED_BATCH_SIZE
JOB_SIMILARITY_THRESHOLD  = config.JOB_SIMILARITY_THRESHOLD

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
log = logging.getLogger("build_vector_db")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ ì¦ë¶„ ì²˜ë¦¬ ë„êµ¬

def load_vectordb_cache() -> Set[str]:
    """ë²¡í„°DBì— ì´ë¯¸ ì¶”ê°€ëœ ë¬¸ì„œ ID ì§‘í•© ë¡œë“œ"""
    try:
        if VECTORDB_CACHE_PATH.exists():
            with VECTORDB_CACHE_PATH.open('r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_doc_ids', []))
    except Exception as e:
        log.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return set()

def save_vectordb_cache(processed_ids: Set[str]) -> None:
    """ë²¡í„°DB ìºì‹œ ì €ì¥"""
    try:
        VECTORDB_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
        cache_data = {
            'processed_doc_ids': list(processed_ids),
            'last_updated': datetime.now().isoformat(),
            'model_name': EMBED_MODEL_NAME,
            'total_docs': len(processed_ids)
        }
        with VECTORDB_CACHE_PATH.open('w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def get_existing_doc_ids_from_db(vectordb) -> Set[str]:
    """ê¸°ì¡´ ë²¡í„°DBì—ì„œ ë¬¸ì„œ ID ì§‘í•© ì¶”ì¶œ"""
    try:
        collection = vectordb.get()
        existing_ids = set()
        
        if collection and 'metadatas' in collection:
            for metadata in collection['metadatas']:
                if metadata and 'doc_id' in metadata:
                    existing_ids.add(metadata['doc_id'])
        
        log.info(f"ğŸ“Š ê¸°ì¡´ ë²¡í„°DBì—ì„œ {len(existing_ids)}ê°œ ë¬¸ì„œ ID ë°œê²¬")
        return existing_ids
    except Exception as e:
        log.warning(f"ê¸°ì¡´ DB ë¬¸ì„œ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return set()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ ì§ì—…ë³„ ì„ë² ë”© ê´€ë¦¬ í•¨ìˆ˜

def load_job_names() -> List[str]:
    """ì§ì—…ëª… ëª©ë¡ ë¡œë“œ"""
    try:
        if JOB_NAMES_PATH.exists():
            with JOB_NAMES_PATH.open('r', encoding='utf-8') as f:
                job_names = json.load(f)
                log.info(f"ğŸ“‹ ì§ì—…ëª… {len(job_names)}ê°œ ë¡œë“œ ì™„ë£Œ")
                return job_names
    except Exception as e:
        log.warning(f"ì§ì—…ëª… ë¡œë“œ ì‹¤íŒ¨: {e}")
    return []

def build_job_embeddings(embedding_fn, job_names: List[str]) -> Dict[str, List[float]]:
    """ê° ì§ì—…ëª…ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„° ìƒì„±"""
    log.info(f"ğŸ§  {len(job_names)}ê°œ ì§ì—…ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ì¤‘...")
    
    job_embeddings = {}
    
    for job_name in job_names:
        job_context = f"ë˜ì „ì•¤íŒŒì´í„° {job_name} ì§ì—… ê°€ì´ë“œ ê³µëµ ìŠ¤í‚¬ ì¥ë¹„ ì„¸íŒ…"
        
        try:
            embedding_vector = embedding_fn.embed_query(job_context)
            job_embeddings[job_name] = embedding_vector
        except Exception as e:
            log.warning(f"ì§ì—… '{job_name}' ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            continue
    
    log.info(f"âœ… {len(job_embeddings)}ê°œ ì§ì—… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    return job_embeddings

def save_job_embeddings(job_embeddings: Dict[str, List[float]]) -> None:
    """ì§ì—…ë³„ ì„ë² ë”© ì €ì¥"""
    try:
        JOB_EMBEDDINGS_PATH.parent.mkdir(parents=True, exist_ok=True)
        
        serializable_embeddings = {}
        for job_name, embedding in job_embeddings.items():
            if isinstance(embedding, np.ndarray):
                serializable_embeddings[job_name] = embedding.tolist()
            else:
                serializable_embeddings[job_name] = embedding
        
        save_data = {
            'job_embeddings': serializable_embeddings,
            'model_name': EMBED_MODEL_NAME,
            'threshold': JOB_SIMILARITY_THRESHOLD,
            'created_at': datetime.now().isoformat(),
            'total_jobs': len(serializable_embeddings)
        }
        
        with JOB_EMBEDDINGS_PATH.open('w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
            
        log.info(f"ğŸ’¾ ì§ì—… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {JOB_EMBEDDINGS_PATH}")
        
    except Exception as e:
        log.error(f"ì§ì—… ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")

def load_job_embeddings() -> Tuple[Dict[str, np.ndarray], str]:
    """ì €ì¥ëœ ì§ì—…ë³„ ì„ë² ë”© ë¡œë“œ"""
    try:
        if JOB_EMBEDDINGS_PATH.exists():
            with JOB_EMBEDDINGS_PATH.open('r', encoding='utf-8') as f:
                data = json.load(f)
                
            job_embeddings = {}
            for job_name, embedding_list in data['job_embeddings'].items():
                job_embeddings[job_name] = np.array(embedding_list)
                
            model_name = data.get('model_name', EMBED_MODEL_NAME)
            log.info(f"ğŸ“‚ ì €ì¥ëœ ì§ì—… ì„ë² ë”© ë¡œë“œ: {len(job_embeddings)}ê°œ ì§ì—…")
            return job_embeddings, model_name
            
    except Exception as e:
        log.warning(f"ì§ì—… ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return {}, EMBED_MODEL_NAME

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
    except Exception:
        return 0.0

def classify_document_job(doc_content: str, embedding_fn, job_embeddings: Dict[str, np.ndarray]) -> Tuple[str, float]:
    """ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ì—… ë¶„ë¥˜"""
    try:
        doc_embedding = np.array(embedding_fn.embed_query(doc_content))
        
        best_job = None
        best_score = 0.0
        
        for job_name, job_embedding in job_embeddings.items():
            similarity = cosine_similarity(doc_embedding, job_embedding)
            
            if similarity > best_score:
                best_score = similarity
                best_job = job_name
        
        if best_score >= JOB_SIMILARITY_THRESHOLD:
            return best_job, best_score
        else:
            return None, best_score
            
    except Exception as e:
        log.warning(f"ë¬¸ì„œ ì§ì—… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        return None, 0.0

def classify_existing_documents():
    """ê¸°ì¡´ ë¬¸ì„œë“¤ì— ëŒ€í•´ ì§ì—… ë¶„ë¥˜ ìˆ˜í–‰ (í›„ì²˜ë¦¬)"""
    log.info("ğŸ¯ ê¸°ì¡´ ë¬¸ì„œë“¤ì— ëŒ€í•œ ì§ì—… ë¶„ë¥˜ ì‹œì‘")
    
    job_embeddings, model_name = load_job_embeddings()
    if not job_embeddings:
        log.error("âŒ ì§ì—… ì„ë² ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„°DBë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
        return
    
    # configì—ì„œ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
    embedding_fn = config.create_embedding_function()
    
    if not PROCESSED_DOCS_PATH.exists():
        log.error(f"âŒ ì „ì²˜ë¦¬ëœ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PROCESSED_DOCS_PATH}")
        return
    
    classified_docs = []
    total_classified = 0
    total_processed = 0
    
    with PROCESSED_DOCS_PATH.open(encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            try:
                raw: Dict = json.loads(line)
                total_processed += 1
                
                if raw["metadata"].get("class_name"):
                    classified_docs.append(raw)
                    continue
                
                title = raw["metadata"].get("title", "")
                content = raw.get("content", "")
                combined_text = f"{title} {content}".strip()
                
                if not combined_text:
                    classified_docs.append(raw)
                    continue
                
                classified_job, similarity_score = classify_document_job(
                    combined_text, embedding_fn, job_embeddings
                )
                
                if classified_job:
                    raw["metadata"]["class_name"] = classified_job
                    raw["metadata"]["job_similarity_score"] = round(similarity_score, 3)
                    total_classified += 1
                
                classified_docs.append(raw)
                
                if total_processed % 1000 == 0:
                    log.info(f"ğŸ”„ ì§„í–‰ìƒí™©: {total_processed}ê°œ ì²˜ë¦¬ ì¤‘, {total_classified}ê°œ ë¶„ë¥˜ ì™„ë£Œ")
                
            except json.JSONDecodeError as e:
                log.warning(f"ì¤„ {line_num}ì—ì„œ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
            except Exception as e:
                log.warning(f"ì¤„ {line_num}ì—ì„œ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
    
    if classified_docs:
        backup_path = PROCESSED_DOCS_PATH.with_suffix('.backup.jsonl')
        if PROCESSED_DOCS_PATH.exists():
            shutil.copy2(PROCESSED_DOCS_PATH, backup_path)
            log.info(f"ğŸ’¾ ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_path}")
        
        with PROCESSED_DOCS_PATH.open('w', encoding='utf-8') as f:
            for doc in classified_docs:
                f.write(json.dumps(doc, ensure_ascii=False) + '\\n')
        
        log.info(f"âœ… ì§ì—… ë¶„ë¥˜ ì™„ë£Œ!")
        log.info(f"   - ì „ì²´ ë¬¸ì„œ: {total_processed}ê°œ")
        log.info(f"   - ë¶„ë¥˜ëœ ë¬¸ì„œ: {total_classified}ê°œ")
        log.info(f"   - ë¶„ë¥˜ìœ¨: {(total_classified/total_processed*100):.1f}%")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ ë©”ì¸ í•¨ìˆ˜

def load_docs(path: Path, existing_ids: Set[str] = None) -> List[Document]:
    """ë¬¸ì„œ ë¡œë“œ (ì¦ë¶„ ì²˜ë¦¬)"""
    docs = []
    skipped = 0
    
    if existing_ids is None:
        existing_ids = set()
    
    with path.open(encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            try:
                raw: Dict = json.loads(line)
                doc_id = raw.get("id", "")
                
                if doc_id in existing_ids:
                    skipped += 1
                    continue
                
                title = raw["metadata"].get("title", "")
                page_content = f"[TITLE] {title}\\n{raw['content']}"

                doc = Document(
                    page_content=page_content,
                    metadata={**raw["metadata"], "doc_id": doc_id}
                )
                docs.append(doc)
                
            except json.JSONDecodeError as e:
                log.warning(f"ì¤„ {line_num}ì—ì„œ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
            except Exception as e:
                log.warning(f"ì¤„ {line_num}ì—ì„œ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
    
    if skipped > 0:
        log.info(f"ğŸ”„ ì¦ë¶„ ëª¨ë“œ: {skipped}ê°œ ë¬¸ì„œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì²˜ë¦¬ë¨)")
    
    return docs

def main():
    """ë²¡í„° DB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜ (ì¦ë¶„ ëª¨ë“œ)"""
    log.info(f"ğŸš€ ì¦ë¶„ ëª¨ë“œ - {config.EMBEDDING_TYPE} {EMBED_MODEL_NAME} ê¸°ë°˜ ì„ë² ë”© ì‹œì‘")
    
    # configì—ì„œ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
    embedding_fn = config.create_embedding_function()
    
    # ì§ì—…ë³„ ì„ë² ë”© êµ¬ì¶• ë° ì €ì¥
    job_names = load_job_names()
    if job_names:
        existing_job_embeddings, existing_model = load_job_embeddings()
        
        if (not existing_job_embeddings or 
            existing_model != EMBED_MODEL_NAME or
            set(job_names) != set(existing_job_embeddings.keys())):
            
            log.info("ğŸ”„ ì§ì—…ë³„ ì„ë² ë”© ìƒˆë¡œ êµ¬ì¶• ì¤‘...")
            new_job_embeddings = build_job_embeddings(embedding_fn, job_names)
            if new_job_embeddings:
                save_job_embeddings(new_job_embeddings)
        else:
            log.info("ğŸ“‚ ê¸°ì¡´ ì§ì—… ì„ë² ë”© ì‚¬ìš©")
    else:
        log.warning("âš ï¸ ì§ì—…ëª… ëª©ë¡ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ì§ì—… ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤")
    
    # ë²¡í„°DB ì´ˆê¸°í™”
    vectordb = Chroma(
        persist_directory=VECTOR_DB_DIR,
        embedding_function=embedding_fn,
    )
    
    # ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ ID ë¡œë“œ (ì¦ë¶„ ì²˜ë¦¬)
    cached_ids = load_vectordb_cache()
    db_ids = get_existing_doc_ids_from_db(vectordb)
    existing_ids = cached_ids.union(db_ids)
    log.info(f"ğŸ”„ ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ {len(existing_ids)}ê°œ")
    
    # ë¬¸ì„œ ë¡œë“œ
    if not PROCESSED_DOCS_PATH.exists():
        log.error(f"âŒ ì „ì²˜ë¦¬ëœ ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PROCESSED_DOCS_PATH}")
        return
    
    all_docs = load_docs(PROCESSED_DOCS_PATH, existing_ids)
    
    if not all_docs:
        log.info("âœ… ì²˜ë¦¬í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    total = len(all_docs)
    log.info(f"ğŸ“„ {total}ê°œ ìƒˆ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ")
    
    # ë°°ì¹˜ ì„ë² ë”© & ì—…ë¡œë“œ
    log.info("ğŸ”„ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘")
    processed_count = 0
    new_doc_ids = set()
    
    for i in range(0, total, EMBED_BATCH_SIZE):
        batch = all_docs[i:i + EMBED_BATCH_SIZE]
        batch_num = i // EMBED_BATCH_SIZE + 1
        log.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}: {len(batch)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        
        try:
            vectordb.add_documents(batch)
            processed_count += len(batch)
            
            for doc in batch:
                doc_id = doc.metadata.get('doc_id', '')
                if doc_id:
                    new_doc_ids.add(doc_id)
            
            log.info(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ ({processed_count}/{total})")
            
            if i + EMBED_BATCH_SIZE < total:
                import time
                time.sleep(0.5)
                
        except Exception as e:
            log.error(f"âŒ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
            continue
    
    # ìºì‹œ ì—…ë°ì´íŠ¸
    if new_doc_ids:
        updated_ids = existing_ids.union(new_doc_ids)
        save_vectordb_cache(updated_ids)
        log.info(f"ğŸ’¾ ìºì‹œ ì—…ë°ì´íŠ¸: ì´ {len(updated_ids)}ê°œ ë¬¸ì„œ ID ì €ì¥")
    
    # ì™„ë£Œ ë©”ì‹œì§€
    log.info(f"ğŸ‰ ë²¡í„° DB ì €ì¥ ì™„ë£Œ â†’ {VECTOR_DB_DIR}")
    log.info(f"ğŸ“ˆ ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ì„œ: {processed_count}ê°œ")
    log.info(f"ğŸ“Š ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(existing_ids) + len(new_doc_ids)}ê°œ")
    log.info(f"ğŸ§  ì‚¬ìš© ëª¨ë¸: {EMBED_MODEL_NAME}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--classify-jobs":
        log.info("ğŸ¯ ì§ì—… ë¶„ë¥˜ ëª¨ë“œ")
        try:
            classify_existing_documents()
            log.info("âœ… ì§ì—… ë¶„ë¥˜ ì™„ë£Œ!")
        except Exception as e:
            log.error(f"âŒ ì§ì—… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
    else:
        log.info("ğŸ”„ ì¦ë¶„ ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
        try:
            main()
            log.info("âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
        except Exception as e:
            log.error(f"âŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {e}")