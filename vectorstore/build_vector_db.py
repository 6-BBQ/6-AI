from __future__ import annotations

import json
import logging
import os
from pathlib import Path
from typing import Dict, List
from dotenv import load_dotenv

from langchain.docstore.document import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ì„¤ì •
PROCESSED_PATH = Path("data/processed_docs.jsonl")
CHROMA_DIR = "vector_db/chroma"        # persist ë””ë ‰í„°ë¦¬
BATCH_SIZE = 200                       # GPU/CPU ìƒí™©ì— ë§ê²Œ ì¡°ì ˆ
EMBEDDING_MODEL = "text-embedding-3-large"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
log = logging.getLogger("build_vector_db")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ ì„ë² ë”© & DB ì´ˆê¸°í™”
if "OPENAI_API_KEY" not in os.environ:
    raise RuntimeError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì •í•˜ì„¸ìš”!")

embedding_fn = OpenAIEmbeddings(model=EMBEDDING_MODEL)

# ë¨¼ì € ê¸°ì¡´ í´ë” ì œê±°
if Path(CHROMA_DIR).exists():
    log.info("ğŸ§¹ ê¸°ì¡´ Chroma í´ë” ì‚­ì œ í›„ ì¬ìƒì„±")
    import shutil
    shutil.rmtree(CHROMA_DIR)

# ì´í›„ ìƒˆë¡œ ìƒì„±
vectordb = Chroma(
    persist_directory=CHROMA_DIR,
    embedding_function=embedding_fn,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ JSONL â†’ Document ë¦¬ìŠ¤íŠ¸
def load_docs(path: Path) -> List[Document]:
    docs: List[Document] = []
    with path.open(encoding="utf-8") as f:
        for line in f:
            raw: Dict = json.loads(line)
            docs.append(
                Document(
                    page_content=raw["content"],
                    metadata={
                        **raw["metadata"],
                        "doc_id": raw["id"],
                    },
                )
            )
    return docs


all_docs = load_docs(PROCESSED_PATH)
total = len(all_docs)
log.info("ğŸ“„ %dê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ â†’ ì„ë² ë”©/DB ì €ì¥", total)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ ë°°ì¹˜ ì„ë² ë”© & ì—…ë¡œë“œ
for i in range(0, total, BATCH_SIZE):
    batch = all_docs[i : i + BATCH_SIZE]
    vectordb.add_documents(batch)
    log.info("âœ… %d / %d ì—…ë¡œë“œ", min(i + BATCH_SIZE, total), total)

vectordb.persist()
log.info("ğŸ‰ Vector DB ì €ì¥ ì™„ë£Œ â†’ %s", CHROMA_DIR)
